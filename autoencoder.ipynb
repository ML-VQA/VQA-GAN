{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 2,
=======
   "execution_count": 1,
>>>>>>> superresolution
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, multiply, GlobalAveragePooling2D, MaxPooling2D\n",
    "from keras.layers import BatchNormalization, Activation, Embedding, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU, PReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from scipy.misc import imread\n",
    "from scipy.misc import imresize\n",
    "from scipy.misc import imsave\n",
    "import glob\n",
    "import random\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import applications\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import to_categorical\n",
    "#from keras.applications.resnet50 import preprocess_input\n",
    "\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "#from keras.applications.vgg19 import preprocess_input as preprocess_vgg\n",
    "from keras.applications.inception_v3 import preprocess_input as inception_preprocess\n",
    "\n",
    "#from keras.applications.inception_resnet_v2 import preprocess_input\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import keras\n",
    "import copy\n",
    "import cv2\n",
<<<<<<< HEAD
    "\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "\n",
    "import numpy as np \n",
    "import os\n",
    "import numpy as np\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras import backend as keras"
=======
    "import os"
>>>>>>> ganvanilla
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
=======
   "execution_count": 2,
>>>>>>> superresolution
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../data/'\n",
    "savepath = '../data/'\n",
    "images = glob.glob(path+'new_data/*.npy')"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 95,
=======
   "execution_count": 4,
>>>>>>> ganvanilla
=======
   "execution_count": 3,
>>>>>>> superresolution
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_sizes = np.loadtxt(path+'tile_sizes.txt', dtype='int')\n",
    "images_sampled = {}\n",
    "for tile in tile_sizes:\n",
    "    if tile[2] > 200000:\n",
    "        for i in range(30):\n",
    "            images_sampled.setdefault(tile[0]*30+i, []).append(tile[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1615\n",
      "1039\n",
      "992\n",
      "995\n",
      "1042\n",
      "1087\n",
      "1242\n",
      "1038\n",
      "1203\n",
      "1329\n",
      "xtrain shape: (10, 384, 384, 3)\n",
      "ytrain shape: (10, 384, 384, 3)\n"
     ]
    }
   ],
   "source": [
    "def myGenerator(batch_size):\n",
    "    while True:\n",
    "        #index_list = random.sample(range(1, totalImages), batch_size)\n",
    "        index_list = random.sample(images_sampled.keys(), batch_size)\n",
    "        alldata_x = []\n",
    "        alldata_y = []\n",
    "        for i in index_list:\n",
    "            print (i)\n",
    "            frame = path+'sources/new_data/frame'+str(i)+'.npy'\n",
    "            frame = np.load(frame)\n",
<<<<<<< HEAD
    "            tile_index = np.random.randint(0, 199)\n",
    "            #print(i, tile_index, frame.shape)\n",
<<<<<<< HEAD
    "            alldata_x.append(tile_index*totalImages+i)\n",
=======
    "            #tile_index = np.random.randint(0, 199)\n",
    "            #print(i, tile_index, frame.shape, images_sampled[i])\n",
    "            #alldata_x.append(tile_index*totalImages+i)\n",
>>>>>>> ganvanilla
=======
    "            \n",
    "            temp  = imresize(frame[tile_index], (48, 48))\n",
    "            temp  = imresize(temp, (192, 192))\n",
    "            \n",
    "            alldata_x.append(temp)\n",
>>>>>>> superresolution
    "            alldata_y.append(frame[tile_index])\n",
    "        \n",
    "        alldata_x = np.array(alldata_x)\n",
    "        alldata_y = np.array(alldata_y)\n",
<<<<<<< HEAD
    "        #alldata_y = alldata_y/255.0\n",
    "        alldata_y = (alldata_y.astype(np.float32) - 127.5) / 127.5\n",
    "        yield alldata_y, alldata_y\n",
<<<<<<< HEAD
=======
    "        \n",
    "        alldata_y = (alldata_y.astype(np.float32) - 127.5) / 127.5\n",
    "        alldata_x = alldata_x.astype(np.float32)/255.0\n",
    "        \n",
    "        yield alldata_x, alldata_y\n",
    "\n",
>>>>>>> superresolution
    "# x = myGenerator(10)\n",
    "# xtrain, ytrain = next(x)\n",
    "# print('xtrain shape:',xtrain.shape)\n",
    "# print('ytrain shape:',ytrain.shape)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myGenerator_new(batch_size):\n",
    "    while True:\n",
    "        index_list = random.sample(range(1, totalImages), batch_size)\n",
    "        alldata_x = []\n",
    "        alldata_y = []\n",
    "        cls = []\n",
    "        for i in index_list:\n",
    "            frame = images[i]\n",
    "            frame = np.load(frame)\n",
    "            tdx = np.random.randint(0, 199)\n",
    "            choices = None\n",
    "            classes = {}\n",
    "            if tdx < 20:\n",
    "                if tdx == 0:\n",
    "                    choices = [1, 20, 21]\n",
    "                    classes = {1:5, 20:7, 21:8}\n",
    "                elif tdx == 19:\n",
    "                    choices = [18, 38, 39]\n",
    "                    classes = {18:3, 38:6, 39:7}\n",
    "                else:\n",
    "                    choices = [tdx-1, tdx+1, tdx+19, tdx+20, tdx+21]\n",
    "                    classes = {tdx-1:3, tdx+1:5, tdx+19:6, tdx+20:7, tdx+21:8}\n",
    "            elif tdx >= 179:\n",
    "                if tdx == 199:\n",
    "                    choices = [178, 179, 198]\n",
    "                    classes = {178:0, 179:1, 198:3}\n",
    "                elif tdx == 179:\n",
    "                    choices = [158, 159, 160, 178, 180, 198, 199]\n",
    "                    classes = {158:0, 159:1, 160:2, 178:3, 180:5, 198:6, 199:7}\n",
    "                elif tdx == 180:\n",
    "                    choices = [160, 161, 181]\n",
    "                    classes = {160:0, 161:1, 181:5}\n",
    "                else:\n",
    "                    choices = [tdx-21, tdx-20, tdx-19, tdx-1, tdx+1]\n",
    "                    classes = {tdx-21:0, tdx-20:1, tdx-19:2, tdx-1:3, tdx+1:5}\n",
    "            else:\n",
    "                choices = [tdx-21, tdx-20, tdx-19, tdx-1, tdx+1, tdx+19, tdx+20, tdx+21]\n",
    "                classes = {tdx-21:0, tdx-20:1, tdx-19:2, tdx-1:3, tdx+1:5, tdx+19:6, tdx+20:7, tdx+21:8}\n",
    "            choice = random.choice(choices)\n",
    "            #print(i, tdx, choice, choices)\n",
    "            cls.append(classes[choice])\n",
    "            #print(i, tdx, choice, cls, choices)\n",
    "            alldata_x.append(frame[tdx])\n",
    "            alldata_y.append(frame[choice])\n",
    "        alldata_x = np.array(alldata_x)\n",
    "        alldata_y = np.array(alldata_y)\n",
    "        alldata_y = (alldata_y.astype(np.float32) - 127.5) / 127.5\n",
    "        yield alldata_x, alldata_y, np.array(cls)\n",
    "#x = myGenerator_new(1000)\n",
    "#xtrain, ytrain, cls = next(x)\n",
    "#print('xtrain shape:',xtrain.shape)\n",
    "#print('ytrain shape:',ytrain.shape)"
=======
    "x = myGenerator(10)\n",
    "xtrain, ytrain = next(x)\n",
    "print('xtrain shape:',xtrain.shape)\n",
    "print('ytrain shape:',ytrain.shape)"
>>>>>>> ganvanilla
=======
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: `imsave` is deprecated!\n",
      "`imsave` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imwrite`` instead.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "imsave(savepath+\"images/_realtest.jpg\", ytrain[4])"
>>>>>>> superresolution
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 32,
=======
   "execution_count": 5,
>>>>>>> ganvanilla
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
>>>>>>> superresolution
   "metadata": {},
   "outputs": [],
   "source": [
    "class CGAN():\n",
    "    def __init__(self):\n",
    "        # Input shape\n",
    "        self.img_rows = 384\n",
    "        self.img_cols = 384\n",
    "        self.channels = 3\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.optimizer = Adam(0.0002, 0.5)\n",
<<<<<<< HEAD
    "        \n",
    "        \n",
    "        self.num_classes     = 9\n",
    "        self.latent_dim      = 200\n",
    "        self.embedding_layer = Embedding(self.num_classes, self.latent_dim)\n",
    "        \n",
=======
    "        self.latent_dim = 300\n",
>>>>>>> ganvanilla
    "        \n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss=['mse'],\n",
<<<<<<< HEAD
    "           optimizer=self.optimizer,\n",
    "           metrics=['accuracy'])\n",
=======
    "            optimizer=self.optimizer,\n",
    "            metrics=['accuracy'])\n",
>>>>>>> ganvanilla
    "        \n",
    "        #print(self.discriminator.summary())\n",
    "        \n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "        \n",
    "        print(self.generator.summary())\n",
    "        \n",
<<<<<<< HEAD
    "        noise     = Input(shape=(192, 192, 3))\n",
    "        out_image = Input(shape=(192, 192, 3))\n",
    "        label     = Input(shape=(1,))\n",
    "        \n",
    "        img       = self.generator([noise, label])\n",
=======
    "        noise = Input(shape=(self.latent_dim, ))\n",
    "        img = self.generator(noise)\n",
>>>>>>> ganvanilla
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The discriminator takes generated image as input and determines validity\n",
    "        # and the label of that image\n",
<<<<<<< HEAD
    "        valid = self.discriminator([img, label])\n",
    "        \n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains generator to fool discriminator\n",
    "        self.combined = Model([noise, label], [img, valid])\n",
    "        self.combined.compile(loss=['mse', 'binary_crossentropy'],\n",
    "           loss_weights=[0.99, 0.01],\n",
    "           optimizer=self.optimizer)\n",
    "\n",
    "    def build_generator(self):\n",
    "        label      = Input(shape=(1,), dtype='int32')\n",
    "        input_size = (192,192,3)\n",
    "        inputs     = Input(input_size)\n",
    "        \n",
    "        \n",
    "        label_embedding  = Flatten()(self.embedding_layer(label))\n",
    "        label_dense      = Dense(81)(label_embedding)\n",
    "        embedding_vector = Reshape((9, 9, 1))(label_dense)\n",
    "        \n",
    "        \n",
    "        conv1 = Conv2D(64, 3, padding = 'same', kernel_initializer = 'he_normal')(inputs)\n",
    "        conv1 = PReLU()(conv1)\n",
    "        conv1 = BatchNormalization(momentum=0.8)(conv1)\n",
    "\n",
    "\n",
    "\n",
    "        conv2 = Conv2D(64, 3, padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
    "        conv2 = PReLU()(conv2)\n",
    "        conv2 = BatchNormalization(momentum=0.8)(conv2)\n",
    "\n",
    "        conv3 = Conv2D(64, 3, padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
    "        conv3 = PReLU()(conv3)\n",
    "        conv3 = BatchNormalization(momentum=0.8)(conv3)\n",
    "\n",
<<<<<<< HEAD
    "        conv3 = Conv2D(1, 3, padding = 'valid', kernel_initializer = 'he_normal')(conv3)\n",
    "        conv3 = LeakyReLU(alpha=0.2)(conv3)\n",
    "        conv3 = BatchNormalization(momentum=0.8)(conv3)\n",
    "\n",
    "\n",
    "        conv3_pool = Conv2D(1, 3, subsample=(2, 2), kernel_initializer = 'he_normal')(conv3)\n",
    "        conv3_pool = LeakyReLU(alpha=0.2)(conv3_pool)\n",
    "        conv3_pool = BatchNormalization(momentum=0.8)(conv3_pool)\n",
    "        \n",
    "        # Multiplying with the conditional vector\n",
    "        conv3_pool =  multiply([conv3_pool, embedding_vector])\n",
    "        \n",
    "        \n",
    "        # Decoder\n",
    "        conv3_unpool = UpSampling2D(size = (2,2))(conv3_pool)\n",
    "        conv3_unpool = ZeroPadding2D(padding=(1, 1))(conv3_unpool)\n",
    "\n",
    "        conv3_unpool = Conv2D(1, 3, padding = 'same', kernel_initializer = 'he_normal')(conv3_unpool)\n",
    "        conv3_unpool = LeakyReLU(alpha=0.2)(conv3_unpool)\n",
    "        conv3_unpool = BatchNormalization(momentum=0.8)(conv3_unpool)\n",
    "        \n",
    "        up1    = UpSampling2D(size = (2,2))(conv3_unpool)\n",
    "        #up1    = UpSampling2D(size = (2,2))(conv3)\n",
    "        up1    = ZeroPadding2D(padding=(3, 3))(up1)\n",
    "\n",
    "\n",
    "        conv4 = Conv2D(256, 3, padding = 'same', kernel_initializer = 'he_normal')(up1)\n",
    "        conv4 = LeakyReLU(alpha=0.2)(conv4)\n",
=======
    "        concat1 = add([conv1, conv3])\n",
    "\n",
    "\n",
    "\n",
    "        conv4 = Conv2D(64, 3, padding = 'same', kernel_initializer = 'he_normal')(concat1)\n",
    "        conv4 = PReLU()(conv4)\n",
>>>>>>> superresolution
    "        conv4 = BatchNormalization(momentum=0.8)(conv4)\n",
    "\n",
    "        conv5 = Conv2D(64, 3, padding = 'same', kernel_initializer = 'he_normal')(conv4)\n",
    "        conv5 = PReLU()(conv5)\n",
    "        conv5 = BatchNormalization(momentum=0.8)(conv5)\n",
    "\n",
    "        concat2 = add([conv5, concat1])\n",
    "\n",
    "\n",
    "\n",
    "        conv6 = Conv2D(64, 3, padding = 'same', kernel_initializer = 'he_normal')(concat2)\n",
    "        conv6 = PReLU()(conv6)\n",
    "        conv6 = BatchNormalization(momentum=0.8)(conv6)\n",
    "\n",
    "        conv7 = Conv2D(64, 3, padding = 'same', kernel_initializer = 'he_normal')(conv6)\n",
    "        conv7 = PReLU()(conv7)\n",
    "        conv7 = BatchNormalization(momentum=0.8)(conv7)\n",
    "\n",
    "        concat3 = add([conv7, concat2])\n",
    "\n",
    "\n",
    "\n",
    "        conv8 = Conv2D(64, 3, padding = 'same', kernel_initializer = 'he_normal')(concat3)\n",
    "        conv8 = PReLU()(conv8)\n",
    "        conv8 = BatchNormalization(momentum=0.8)(conv8)\n",
    "\n",
    "        conv9 = Conv2D(64, 3, padding = 'same', kernel_initializer = 'he_normal')(conv8)\n",
    "        conv9 = PReLU()(conv9)\n",
    "        conv9 = BatchNormalization(momentum=0.8)(conv9)\n",
    "        \n",
    "\n",
    "        conv10 = Conv2D(64, 3, padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "        conv10 = PReLU()(conv10)\n",
    "        conv10 = BatchNormalization(momentum=0.8)(conv10)\n",
    "\n",
    "        concat4 = add([conv10, conv1])\n",
    "        \n",
    "        conv11 = Conv2D(128, 3, padding = 'same', kernel_initializer = 'he_normal')(concat4)\n",
    "        conv11 = PReLU()(conv11)\n",
    "        conv11 = BatchNormalization(momentum=0.8)(conv11)\n",
    "        \n",
    "        conv12 = Conv2D(128, 3, padding = 'same', kernel_initializer = 'he_normal')(conv11)\n",
    "        conv12 = PReLU()(conv12)\n",
    "        conv12 = BatchNormalization(momentum=0.8)(conv12)\n",
    "\n",
    "\n",
<<<<<<< HEAD
    "        model  = Model([inputs, label], conv11)\n",
=======
    "        valid = self.discriminator(img)\n",
=======
    "        out = Conv2D(3, 3, padding = 'same', kernel_initializer = 'he_normal')(conv12)\n",
    "        out = PReLU()(out)\n",
    "        out = BatchNormalization(momentum=0.8)(out)\n",
    "\n",
    "\n",
    "        model = Model(input = inputs, output = out)\n",
    "        return model\n",
    "    \n",
    "    def build_autoencoder(self):\n",
    "        self.generator.compile(loss=['mse'],optimizer=self.optimizer)\n",
    "    \n",
    "    def train_generator_autoencoder(self, epochs, batch_size=128, sample_interval=10):\n",
    "        for epoch in range(epochs):\n",
    "            X_train, y_train = next(myGenerator(batch_size))\n",
    "            g_loss = self.generator.train_on_batch(X_train, y_train)\n",
    "            print (\"Epoch \", epoch, \" G loss \", g_loss)\n",
    "            if epoch % sample_interval == 0:\n",
    "                self.sample_images(epoch)\n",
    "                self.generator.save_weights(savepath+'weights/generator_weights_'+str(epoch)+'.h5')\n",
    "            \n",
    "    def build_discriminator(self):\n",
    "        img   = Input(shape=(192, 192, 3))\n",
>>>>>>> superresolution
    "        \n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains generator to fool discriminator\n",
    "        self.combined = Model(noise, valid)\n",
    "        self.combined.compile(loss=['mse'],\n",
    "            optimizer=self.optimizer)\n",
    "\n",
    "    def build_generator(self):\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(64 * 48 * 48, input_dim=300))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Reshape((48, 48, 64)))\n",
    "\n",
    "        model.add(Conv2D(512, kernel_size=3, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "        model.add(Conv2D(512, kernel_size=3, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(256, kernel_size=3, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        \n",
    "        model.add(Conv2D(256, kernel_size=3, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        \n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(256, kernel_size=3, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "        model.add(Conv2D(128, kernel_size=3, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(64, kernel_size=3, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "        model.add(Conv2D(64, kernel_size=3, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
<<<<<<< HEAD
    "        model.add(Conv2D(32, kernel_size=3, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Activation(\"relu\"))\n",
    "    \n",
    "        model.add(Conv2D(32, kernel_size=3, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        \n",
    "        model.add(Conv2D(3, kernel_size=3, padding=\"same\"))\n",
    "        model.add(Activation(\"tanh\"))\n",
    "\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        img   = model(noise)\n",
>>>>>>> ganvanilla
    "        return model\n",
    "    \n",
    "    def build_autoencoder(self):\n",
    "        self.generator.compile(loss=['mse'], optimizer=self.optimizer)\n",
    "    \n",
    "    def train_generator_autoencoder(self, epochs, batch_size=128, sample_interval=10):\n",
    "        for epoch in range(epochs):\n",
    "            X_train, y_train = next(myGenerator(batch_size))\n",
    "            g_loss = self.generator.train_on_batch(X_train, X_train)\n",
    "            print (\"Epoch \", epoch, \" G loss \", g_loss)\n",
    "            if epoch % sample_interval == 0:\n",
    "                self.sample_images(epoch)\n",
    "                self.generator.save_weights(savepath+'weights/generator_weights_'+str(epoch)+'.h5')\n",
    "            \n",
    "    def build_discriminator(self):\n",
<<<<<<< HEAD
=======
    "        img   = Input(shape=(384, 384, 3))\n",
>>>>>>> ganvanilla
    "        model = Sequential()\n",
    "        model.add(Conv2D(64, (3, 3), input_shape=(384, 384, 3)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(64,  (3, 3),  strides=(2, 2)))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Conv2D(64, (3, 3)))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Conv2D(64, (6, 6),  strides=(2, 2)))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Conv2D(64, (3, 3),  strides=(2, 2)))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Conv2D(64, (3, 3),  strides=(2, 2)))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Flatten())\n",
<<<<<<< HEAD
    "\n",
    "        model.add(Dense(200))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "        img   = Input(shape=(192, 192, 3))\n",
    "        label = Input(shape=(1,), dtype='int32')\n",
    "        embedding_layer = Embedding(self.num_classes, self.latent_dim)\n",
    "\n",
    "        label_embedding  = Flatten()(self.embedding_layer(label))\n",
    "\n",
    "        dense200    = model(img)\n",
    "        conditioned = multiply([dense200, label_embedding])\n",
    "\n",
    "        temp     = Dense(32)(conditioned)\n",
    "        tempa    = LeakyReLU(alpha=0.2)(temp)\n",
    "        validity = Dense(1, activation='sigmoid')(tempa)\n",
    "\n",
    "        model3 = Model([img, label], validity)\n",
    "        \n",
=======
    "        model.add(Dense(100))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(32))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(1))\n",
    "        \n",
    "        output    = model(img)\n",
    "        model3    = Model(img, output)\n",
>>>>>>> ganvanilla
    "        return model3\n",
    "\n",
    "    def train(self, epochs, batch_size=128, sample_interval=50):\n",
    "        random.seed(10)\n",
    "        \n",
    "        # Load the dataset\n",
    "        for epoch in range(epochs):\n",
    "            X_train, Y_train, cls = next(myGenerator_new(batch_size))\n",
    "            \n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "            \n",
    "            # Adversarial ground truths\n",
    "            valid = np.ones((batch_size, 1))\n",
    "            fake  = np.zeros((batch_size, 1))\n",
    "            \n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "            \n",
    "            # Generate a half batch of new images\n",
<<<<<<< HEAD
    "            gen_imgs = self.generator.predict([X_train, cls])\n",
=======
    "            gen_imgs = self.generator.predict(noise)\n",
>>>>>>> ganvanilla
    "\n",
    "            # Train the discriminator\n",
    "            d_loss_real = self.discriminator.train_on_batch([Y_train, cls], valid)\n",
    "            d_loss_fake = self.discriminator.train_on_batch([gen_imgs, cls], fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "            d_loss_real= np.array(d_loss_real)\n",
    "            d_loss_fake= np.array(d_loss_fake)\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            # Train the generator\n",
<<<<<<< HEAD
    "            g_loss = self.combined.train_on_batch([X_train, cls], [Y_train, valid])\n",
    "            \n",
    "            #print(d_loss.dtype)\n",
    "            #print(d_loss_real.dtype)\n",
    "            #print(d_loss_fake.dtype)\n",
    "            #print(g_loss.dtype)\n",
    "\n",
    "            # Plot the progress\n",
    "            #print (\"%d [D loss: %f, mean_acc: %.2f%% real_acc: %.2f%% fake_acc: %.2f%%] [G loss: %f, mse: %f]\" % (epoch, d_loss[0], 100*d_loss[1], 100*d_loss_real[1], 100*d_loss_fake[1], g_loss[0], g_loss[1]))\n",
=======
    "            g_loss = self.combined.train_on_batch(noise, valid)\n",
    "\n",
    "            # Plot the progress\n",
    "            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
>>>>>>> ganvanilla
    "\n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % sample_interval == 0:\n",
    "                self.sample_images_new(epoch)\n",
    "                self.generator.save_weights(savepath+'weights/generator_weights_'+str(epoch)+'.h5')\n",
    "                self.discriminator.save_weights(savepath+'weights/discriminator_weights_'+str(epoch)+'.h5')\n",
    "    \n",
    "    \n",
    "    def sample_images(self, epoch):\n",
    "        r, c             = 1, 10\n",
    "        noise            = np.random.normal(0, 1, (5, self.latent_dim))\n",
    "        gen_imgs         = self.generator.predict(noise)\n",
=======
    "    def sample_images(self, epoch):\n",
    "        r, c             = 1, 10\n",
    "        X_train, y_train = next(myGenerator(10))\n",
    "        gen_imgs         = self.generator.predict(X_train)\n",
>>>>>>> superresolution
    "        \n",
    "        # Rescale images 0 - 1\n",
    "        temp     = (0.5 * gen_imgs + 0.5)*255\n",
    "        gen_imgs = temp.astype(int)\n",
<<<<<<< HEAD
    "        \n",
<<<<<<< HEAD
=======
    "        y_train = (0.5 * y_train + 0.5)*255\n",
    "        y_train = y_train.astype(int)\n",
    "        X_train = X_train*255\n",
    "        X_train = X_train.astype(int)\n",
    "        \n",
>>>>>>> superresolution
    "        combined = np.array([gen_imgs[0], gen_imgs[1], gen_imgs[2], gen_imgs[3], gen_imgs[4], gen_imgs[5], gen_imgs[6], gen_imgs[7], gen_imgs[8], gen_imgs[9]])\n",
    "        combined = np.hstack(combined.reshape(10, 192,192, 3))\n",
    "        imsave(savepath+\"images/\"+str(epoch)+\".png\", combined)\n",
    "        \n",
<<<<<<< HEAD
    "        combined = np.array([X_train[0], X_train[1], X_train[2], X_train[3], X_train[4], X_train[5], X_train[6], X_train[7], X_train[8], X_train[9]])\n",
    "        combined = np.hstack(combined.reshape(10, 192,192, 3))\n",
    "        imsave(savepath+\"images/\"+str(epoch)+\"_real.png\", combined)\n",
    "    \n",
    "    \n",
    "    def sample_images_new(self, epoch):\n",
    "        r, c                  = 1, 5\n",
    "        X_train, Y_train, cls = next(myGenerator_new(5))\n",
    "        gen_imgs              = self.generator.predict([X_train, cls])\n",
    "        \n",
    "        # Rescale images 0 - 1\n",
    "        temp     = (0.5 * gen_imgs + 0.5)*255\n",
    "        gen_imgs = temp.astype(int)\n",
    "        \n",
    "        combined = np.array([gen_imgs[0], gen_imgs[1], gen_imgs[2], gen_imgs[3], gen_imgs[4]])\n",
    "        combined = np.hstack(combined.reshape(5, 192,192, 3))\n",
    "        imsave(savepath+\"images/\"+str(epoch)+\".png\", combined)\n",
    "        \n",
    "        combined = np.array([Y_train[0], Y_train[1], Y_train[2], Y_train[3], Y_train[4]])\n",
    "        combined = np.hstack(combined.reshape(5, 192,192, 3))\n",
    "        imsave(savepath+\"images/\"+str(epoch)+\"_real.png\", combined)"
=======
    "        combined = np.array([gen_imgs[0], gen_imgs[1], gen_imgs[2], gen_imgs[3], gen_imgs[4]])\n",
    "        combined = np.hstack(combined.reshape(5, 384,384, 3))\n",
    "        imsave(savepath+\"images/\"+str(epoch)+\".png\", combined)\n",
    "        \n",
    "#         combined = np.array([Y_train[0], Y_train[1], Y_train[2], Y_train[3], Y_train[4]])\n",
    "#         combined = np.hstack(combined.reshape(5, 384,384, 3))\n",
    "#         imsave(savepath+\"images/\"+str(epoch)+\"_real.png\", combined)"
>>>>>>> ganvanilla
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#cgan = CGAN()\n",
<<<<<<< HEAD
    "#cgan.build_autoencoder()\n",
    "#cgan.train_generator_autoencoder(1000000, 8, 100)"
=======
    "        combined = np.array([y_train[0], y_train[1], y_train[2], y_train[3], y_train[4], y_train[5], y_train[6], y_train[7], y_train[8], y_train[9]])\n",
    "        combined = np.hstack(combined.reshape(10, 192,192, 3))\n",
    "        imsave(savepath+\"images/\"+str(epoch)+\"_real.png\", combined)\n",
    "        \n",
    "        combined = np.array([X_train[0], X_train[1], X_train[2], X_train[3], X_train[4], X_train[5], X_train[6], X_train[7], X_train[8], X_train[9]])\n",
    "        combined = np.hstack(combined.reshape(10, 192,192, 3))\n",
    "        imsave(savepath+\"images/\"+str(epoch)+\"_lowres.png\", combined)"
>>>>>>> superresolution
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 33,
=======
   "execution_count": 7,
>>>>>>> superresolution
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:68: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, 3, strides=(2, 2), kernel_initializer=\"he_normal\")`\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:83: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, 3, strides=(2, 2), kernel_initializer=\"he_normal\")`\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:97: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, 3, strides=(2, 2), kernel_initializer=\"he_normal\")`\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:112: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(1, 3, strides=(2, 2), kernel_initializer=\"he_normal\")`\n"
=======
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:116: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"ba..., inputs=Tensor(\"in...)`\n"
>>>>>>> superresolution
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
<<<<<<< HEAD
      "input_76 (InputLayer)           (None, 192, 192, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_330 (Conv2D)             (None, 192, 192, 64) 1792        input_76[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_344 (LeakyReLU)     (None, 192, 192, 64) 0           conv2d_330[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_305 (BatchN (None, 192, 192, 64) 256         leaky_re_lu_344[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_331 (Conv2D)             (None, 190, 190, 64) 36928       batch_normalization_305[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_345 (LeakyReLU)     (None, 190, 190, 64) 0           conv2d_331[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_306 (BatchN (None, 190, 190, 64) 256         leaky_re_lu_345[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_332 (Conv2D)             (None, 94, 94, 64)   36928       batch_normalization_306[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_346 (LeakyReLU)     (None, 94, 94, 64)   0           conv2d_332[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_307 (BatchN (None, 94, 94, 64)   256         leaky_re_lu_346[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_333 (Conv2D)             (None, 94, 94, 128)  73856       batch_normalization_307[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_347 (LeakyReLU)     (None, 94, 94, 128)  0           conv2d_333[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_308 (BatchN (None, 94, 94, 128)  512         leaky_re_lu_347[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_334 (Conv2D)             (None, 94, 94, 128)  147584      batch_normalization_308[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_348 (LeakyReLU)     (None, 94, 94, 128)  0           conv2d_334[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_309 (BatchN (None, 94, 94, 128)  512         leaky_re_lu_348[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_335 (Conv2D)             (None, 46, 46, 128)  147584      batch_normalization_309[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_349 (LeakyReLU)     (None, 46, 46, 128)  0           conv2d_335[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_310 (BatchN (None, 46, 46, 128)  512         leaky_re_lu_349[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_336 (Conv2D)             (None, 46, 46, 256)  295168      batch_normalization_310[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_350 (LeakyReLU)     (None, 46, 46, 256)  0           conv2d_336[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_311 (BatchN (None, 46, 46, 256)  1024        leaky_re_lu_350[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_337 (Conv2D)             (None, 46, 46, 256)  590080      batch_normalization_311[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_351 (LeakyReLU)     (None, 46, 46, 256)  0           conv2d_337[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_312 (BatchN (None, 46, 46, 256)  1024        leaky_re_lu_351[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_338 (Conv2D)             (None, 22, 22, 256)  590080      batch_normalization_312[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_352 (LeakyReLU)     (None, 22, 22, 256)  0           conv2d_338[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_313 (BatchN (None, 22, 22, 256)  1024        leaky_re_lu_352[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_339 (Conv2D)             (None, 22, 22, 256)  590080      batch_normalization_313[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_353 (LeakyReLU)     (None, 22, 22, 256)  0           conv2d_339[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_314 (BatchN (None, 22, 22, 256)  1024        leaky_re_lu_353[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_340 (Conv2D)             (None, 20, 20, 1)    2305        batch_normalization_314[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_354 (LeakyReLU)     (None, 20, 20, 1)    0           conv2d_340[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "input_75 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_315 (BatchN (None, 20, 20, 1)    4           leaky_re_lu_354[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_25 (Embedding)        (None, 1, 200)       1800        input_75[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_341 (Conv2D)             (None, 9, 9, 1)      10          batch_normalization_315[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_39 (Flatten)            (None, 200)          0           embedding_25[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_355 (LeakyReLU)     (None, 9, 9, 1)      0           conv2d_341[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_52 (Dense)                (None, 81)           16281       flatten_39[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_316 (BatchN (None, 9, 9, 1)      4           leaky_re_lu_355[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "reshape_13 (Reshape)            (None, 9, 9, 1)      0           dense_52[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "multiply_26 (Multiply)          (None, 9, 9, 1)      0           batch_normalization_316[0][0]    \n",
      "                                                                 reshape_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_49 (UpSampling2D) (None, 18, 18, 1)    0           multiply_26[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_61 (ZeroPadding2 (None, 20, 20, 1)    0           up_sampling2d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_342 (Conv2D)             (None, 20, 20, 1)    10          zero_padding2d_61[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_356 (LeakyReLU)     (None, 20, 20, 1)    0           conv2d_342[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_317 (BatchN (None, 20, 20, 1)    4           leaky_re_lu_356[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_50 (UpSampling2D) (None, 40, 40, 1)    0           batch_normalization_317[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_62 (ZeroPadding2 (None, 46, 46, 1)    0           up_sampling2d_50[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_343 (Conv2D)             (None, 46, 46, 256)  2560        zero_padding2d_62[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_357 (LeakyReLU)     (None, 46, 46, 256)  0           conv2d_343[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_318 (BatchN (None, 46, 46, 256)  1024        leaky_re_lu_357[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_37 (Concatenate)    (None, 46, 46, 512)  0           batch_normalization_318[0][0]    \n",
      "                                                                 batch_normalization_312[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_344 (Conv2D)             (None, 46, 46, 256)  1179904     concatenate_37[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_358 (LeakyReLU)     (None, 46, 46, 256)  0           conv2d_344[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_319 (BatchN (None, 46, 46, 256)  1024        leaky_re_lu_358[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_345 (Conv2D)             (None, 46, 46, 256)  590080      batch_normalization_319[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_359 (LeakyReLU)     (None, 46, 46, 256)  0           conv2d_345[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_320 (BatchN (None, 46, 46, 256)  1024        leaky_re_lu_359[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_51 (UpSampling2D) (None, 92, 92, 256)  0           batch_normalization_320[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_63 (ZeroPadding2 (None, 94, 94, 256)  0           up_sampling2d_51[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_346 (Conv2D)             (None, 94, 94, 128)  295040      zero_padding2d_63[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_360 (LeakyReLU)     (None, 94, 94, 128)  0           conv2d_346[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_321 (BatchN (None, 94, 94, 128)  512         leaky_re_lu_360[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_38 (Concatenate)    (None, 94, 94, 256)  0           batch_normalization_309[0][0]    \n",
      "                                                                 batch_normalization_321[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_347 (Conv2D)             (None, 94, 94, 128)  295040      concatenate_38[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_361 (LeakyReLU)     (None, 94, 94, 128)  0           conv2d_347[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_322 (BatchN (None, 94, 94, 128)  512         leaky_re_lu_361[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_348 (Conv2D)             (None, 94, 94, 128)  147584      batch_normalization_322[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_362 (LeakyReLU)     (None, 94, 94, 128)  0           conv2d_348[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_323 (BatchN (None, 94, 94, 128)  512         leaky_re_lu_362[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_52 (UpSampling2D) (None, 188, 188, 128 0           batch_normalization_323[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_64 (ZeroPadding2 (None, 190, 190, 128 0           up_sampling2d_52[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_39 (Concatenate)    (None, 190, 190, 192 0           batch_normalization_306[0][0]    \n",
      "                                                                 zero_padding2d_64[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_65 (ZeroPadding2 (None, 192, 192, 192 0           concatenate_39[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_349 (Conv2D)             (None, 192, 192, 64) 110656      zero_padding2d_65[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_363 (LeakyReLU)     (None, 192, 192, 64) 0           conv2d_349[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_324 (BatchN (None, 192, 192, 64) 256         leaky_re_lu_363[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_350 (Conv2D)             (None, 192, 192, 64) 36928       batch_normalization_324[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_364 (LeakyReLU)     (None, 192, 192, 64) 0           conv2d_350[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_325 (BatchN (None, 192, 192, 64) 256         leaky_re_lu_364[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_351 (Conv2D)             (None, 192, 192, 3)  195         batch_normalization_325[0][0]    \n",
      "==================================================================================================\n",
      "Total params: 5,200,005\n",
      "Trainable params: 5,194,239\n",
      "Non-trainable params: 5,766\n",
=======
      "input_3 (InputLayer)            (None, 192, 192, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 192, 192, 64) 1792        input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_14 (PReLU)              (None, 192, 192, 64) 2359296     conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 192, 192, 64) 256         p_re_lu_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 192, 192, 64) 36928       batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_15 (PReLU)              (None, 192, 192, 64) 2359296     conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 192, 192, 64) 256         p_re_lu_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 192, 192, 64) 36928       batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_16 (PReLU)              (None, 192, 192, 64) 2359296     conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 192, 192, 64) 256         p_re_lu_16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 192, 192, 64) 0           batch_normalization_14[0][0]     \n",
      "                                                                 batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 192, 192, 64) 36928       add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_17 (PReLU)              (None, 192, 192, 64) 2359296     conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 192, 192, 64) 256         p_re_lu_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 192, 192, 64) 36928       batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_18 (PReLU)              (None, 192, 192, 64) 2359296     conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 192, 192, 64) 256         p_re_lu_18[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 192, 192, 64) 0           batch_normalization_18[0][0]     \n",
      "                                                                 add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 192, 192, 64) 36928       add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_19 (PReLU)              (None, 192, 192, 64) 2359296     conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 192, 192, 64) 256         p_re_lu_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 192, 192, 64) 36928       batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_20 (PReLU)              (None, 192, 192, 64) 2359296     conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 192, 192, 64) 256         p_re_lu_20[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 192, 192, 64) 0           batch_normalization_20[0][0]     \n",
      "                                                                 add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 192, 192, 64) 36928       add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_21 (PReLU)              (None, 192, 192, 64) 2359296     conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 192, 192, 64) 256         p_re_lu_21[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 192, 192, 64) 36928       batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_22 (PReLU)              (None, 192, 192, 64) 2359296     conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 192, 192, 64) 256         p_re_lu_22[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 192, 192, 64) 36928       batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_23 (PReLU)              (None, 192, 192, 64) 2359296     conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 192, 192, 64) 256         p_re_lu_23[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 192, 192, 64) 0           batch_normalization_23[0][0]     \n",
      "                                                                 batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 192, 192, 128 73856       add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_24 (PReLU)              (None, 192, 192, 128 4718592     conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 192, 192, 128 512         p_re_lu_24[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 192, 192, 128 147584      batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_25 (PReLU)              (None, 192, 192, 128 4718592     conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 192, 192, 128 512         p_re_lu_25[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 192, 192, 3)  3459        batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_26 (PReLU)              (None, 192, 192, 3)  110592      conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 192, 192, 3)  12          p_re_lu_26[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 33,703,375\n",
      "Trainable params: 33,701,577\n",
      "Non-trainable params: 1,798\n",
>>>>>>> superresolution
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:314: DeprecationWarning: `imsave` is deprecated!\n",
      "`imsave` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imwrite`` instead.\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:318: DeprecationWarning: `imsave` is deprecated!\n",
=======
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:12: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "  if sys.path[0] == '':\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:13: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0  G loss  1.3060274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:222: DeprecationWarning: `imsave` is deprecated!\n",
      "`imsave` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imwrite`` instead.\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:226: DeprecationWarning: `imsave` is deprecated!\n",
      "`imsave` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imwrite`` instead.\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:230: DeprecationWarning: `imsave` is deprecated!\n",
>>>>>>> superresolution
      "`imsave` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imwrite`` instead.\n"
     ]
    },
    {
<<<<<<< HEAD
=======
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1  G loss  0.8862408\n",
      "Epoch  2  G loss  0.6474298\n",
      "Epoch  3  G loss  0.905849\n",
      "Epoch  4  G loss  0.93571144\n",
      "Epoch  5  G loss  0.4205962\n",
      "Epoch  6  G loss  0.70168424\n",
      "Epoch  7  G loss  0.60607517\n",
      "Epoch  8  G loss  0.45711547\n",
      "Epoch  9  G loss  0.7244445\n",
      "Epoch  10  G loss  0.40805203\n",
      "Epoch  11  G loss  0.27016854\n",
      "Epoch  12  G loss  0.325513\n",
      "Epoch  13  G loss  0.75835353\n",
      "Epoch  14  G loss  0.31618375\n",
      "Epoch  15  G loss  0.40103805\n",
      "Epoch  16  G loss  0.41615054\n",
      "Epoch  17  G loss  0.29694852\n",
      "Epoch  18  G loss  0.41950583\n",
      "Epoch  19  G loss  0.33925888\n",
      "Epoch  20  G loss  0.27988845\n",
      "Epoch  21  G loss  0.3429494\n",
      "Epoch  22  G loss  0.29946047\n",
      "Epoch  23  G loss  0.2868964\n",
      "Epoch  24  G loss  0.2230593\n",
      "Epoch  25  G loss  0.24750474\n",
      "Epoch  26  G loss  0.6436486\n",
      "Epoch  27  G loss  0.3322325\n",
      "Epoch  28  G loss  0.832175\n",
      "Epoch  29  G loss  0.4874094\n",
      "Epoch  30  G loss  0.19578591\n",
      "Epoch  31  G loss  0.3099168\n",
      "Epoch  32  G loss  0.35394365\n",
      "Epoch  33  G loss  0.20912015\n",
      "Epoch  34  G loss  0.39997247\n",
      "Epoch  35  G loss  0.627189\n",
      "Epoch  36  G loss  0.16830327\n",
      "Epoch  37  G loss  0.18605614\n",
      "Epoch  38  G loss  0.23309572\n",
      "Epoch  39  G loss  0.2178087\n",
      "Epoch  40  G loss  0.20053634\n",
      "Epoch  41  G loss  0.299821\n",
      "Epoch  42  G loss  0.15004647\n",
      "Epoch  43  G loss  0.19023451\n",
      "Epoch  44  G loss  0.12356275\n",
      "Epoch  45  G loss  0.19778726\n",
      "Epoch  46  G loss  0.44880453\n",
      "Epoch  47  G loss  0.20581114\n",
      "Epoch  48  G loss  0.46519673\n",
      "Epoch  49  G loss  0.2009699\n",
      "Epoch  50  G loss  0.36768717\n",
      "Epoch  51  G loss  0.17932495\n",
      "Epoch  52  G loss  0.4274847\n",
      "Epoch  53  G loss  0.26728952\n",
      "Epoch  54  G loss  0.21675204\n",
      "Epoch  55  G loss  0.1422202\n",
      "Epoch  56  G loss  0.22699714\n",
      "Epoch  57  G loss  0.23254368\n",
      "Epoch  58  G loss  0.19179353\n",
      "Epoch  59  G loss  0.18674898\n",
      "Epoch  60  G loss  0.17891064\n",
      "Epoch  61  G loss  0.22257924\n",
      "Epoch  62  G loss  0.19407366\n",
      "Epoch  63  G loss  0.19761316\n",
      "Epoch  64  G loss  0.31424594\n",
      "Epoch  65  G loss  1.1407421\n",
      "Epoch  66  G loss  0.31149238\n",
      "Epoch  67  G loss  0.44017112\n",
      "Epoch  68  G loss  0.26058578\n",
      "Epoch  69  G loss  0.14388067\n",
      "Epoch  70  G loss  0.23521543\n",
      "Epoch  71  G loss  0.2602497\n",
      "Epoch  72  G loss  0.31479526\n",
      "Epoch  73  G loss  0.16551462\n",
      "Epoch  74  G loss  0.3155346\n",
      "Epoch  75  G loss  0.27733913\n",
      "Epoch  76  G loss  0.20225522\n",
      "Epoch  77  G loss  0.35170513\n",
      "Epoch  78  G loss  0.6336725\n",
      "Epoch  79  G loss  0.18812695\n",
      "Epoch  80  G loss  0.38443887\n",
      "Epoch  81  G loss  0.21724638\n",
      "Epoch  82  G loss  0.19671923\n",
      "Epoch  83  G loss  0.19056675\n",
      "Epoch  84  G loss  0.21454522\n",
      "Epoch  85  G loss  0.22256666\n",
      "Epoch  86  G loss  0.22192466\n",
      "Epoch  87  G loss  0.2144289\n",
      "Epoch  88  G loss  0.3043844\n",
      "Epoch  89  G loss  0.10731433\n",
      "Epoch  90  G loss  0.23118776\n",
      "Epoch  91  G loss  0.28385937\n",
      "Epoch  92  G loss  0.15230748\n",
      "Epoch  93  G loss  0.28631312\n",
      "Epoch  94  G loss  0.13268125\n",
      "Epoch  95  G loss  0.6259436\n",
      "Epoch  96  G loss  0.20368394\n",
      "Epoch  97  G loss  0.2905327\n",
      "Epoch  98  G loss  0.26219225\n",
      "Epoch  99  G loss  0.34970117\n",
      "Epoch  100  G loss  0.43263918\n",
      "Epoch  101  G loss  0.45946357\n",
      "Epoch  102  G loss  0.22190236\n",
      "Epoch  103  G loss  0.14419189\n",
      "Epoch  104  G loss  0.1593839\n",
      "Epoch  105  G loss  0.11570203\n",
      "Epoch  106  G loss  0.17319393\n",
      "Epoch  107  G loss  0.13008226\n",
      "Epoch  108  G loss  0.16050541\n",
      "Epoch  109  G loss  0.16495049\n",
      "Epoch  110  G loss  0.17985123\n",
      "Epoch  111  G loss  0.21161403\n",
      "Epoch  112  G loss  0.22097664\n",
      "Epoch  113  G loss  0.14696096\n",
      "Epoch  114  G loss  0.20005015\n",
      "Epoch  115  G loss  0.11982331\n",
      "Epoch  116  G loss  0.15836862\n",
      "Epoch  117  G loss  0.19227856\n",
      "Epoch  118  G loss  0.21085212\n",
      "Epoch  119  G loss  0.25483945\n",
      "Epoch  120  G loss  0.15566668\n",
      "Epoch  121  G loss  0.26671806\n",
      "Epoch  122  G loss  0.26307386\n",
      "Epoch  123  G loss  0.14056869\n",
      "Epoch  124  G loss  0.17044151\n",
      "Epoch  125  G loss  0.21019737\n",
      "Epoch  126  G loss  0.25936475\n",
      "Epoch  127  G loss  0.21515077\n",
      "Epoch  128  G loss  0.2551268\n",
      "Epoch  129  G loss  0.4745159\n",
      "Epoch  130  G loss  0.19072352\n",
      "Epoch  131  G loss  0.1586022\n",
      "Epoch  132  G loss  0.50257623\n",
      "Epoch  133  G loss  0.21619174\n",
      "Epoch  134  G loss  0.20766085\n",
      "Epoch  135  G loss  0.22285175\n",
      "Epoch  136  G loss  0.23943107\n",
      "Epoch  137  G loss  0.21153814\n",
      "Epoch  138  G loss  0.18150239\n",
      "Epoch  139  G loss  0.23407477\n",
      "Epoch  140  G loss  0.48604375\n",
      "Epoch  141  G loss  0.14177045\n",
      "Epoch  142  G loss  0.24156453\n",
      "Epoch  143  G loss  0.31032622\n",
      "Epoch  144  G loss  0.14981163\n",
      "Epoch  145  G loss  0.14121191\n",
      "Epoch  146  G loss  0.23321804\n",
      "Epoch  147  G loss  0.20273362\n",
      "Epoch  148  G loss  0.18640879\n",
      "Epoch  149  G loss  0.48410392\n",
      "Epoch  150  G loss  0.1222282\n",
      "Epoch  151  G loss  0.19622785\n",
      "Epoch  152  G loss  0.32825482\n",
      "Epoch  153  G loss  0.14818084\n",
      "Epoch  154  G loss  0.14276141\n",
      "Epoch  155  G loss  0.5799365\n",
      "Epoch  156  G loss  0.29936388\n",
      "Epoch  157  G loss  0.14613865\n",
      "Epoch  158  G loss  0.23476197\n",
      "Epoch  159  G loss  0.16737533\n",
      "Epoch  160  G loss  0.12672906\n",
      "Epoch  161  G loss  0.22347324\n",
      "Epoch  162  G loss  0.22663324\n",
      "Epoch  163  G loss  0.2215537\n",
      "Epoch  164  G loss  0.28758514\n",
      "Epoch  165  G loss  0.32706633\n",
      "Epoch  166  G loss  0.17292356\n",
      "Epoch  167  G loss  0.4975326\n",
      "Epoch  168  G loss  0.3119042\n",
      "Epoch  169  G loss  0.1290783\n",
      "Epoch  170  G loss  0.30935472\n",
      "Epoch  171  G loss  0.2209161\n",
      "Epoch  172  G loss  0.17950201\n",
      "Epoch  173  G loss  0.1657933\n",
      "Epoch  174  G loss  0.112590104\n",
      "Epoch  175  G loss  0.25781274\n",
      "Epoch  176  G loss  0.33921078\n",
      "Epoch  177  G loss  0.14937623\n",
      "Epoch  178  G loss  0.14394066\n",
      "Epoch  179  G loss  0.17339507\n",
      "Epoch  180  G loss  0.13575685\n",
      "Epoch  181  G loss  0.13633256\n",
      "Epoch  182  G loss  0.15653974\n",
      "Epoch  183  G loss  0.14291982\n",
      "Epoch  184  G loss  0.11671265\n",
      "Epoch  185  G loss  0.1199159\n",
      "Epoch  186  G loss  0.3595551\n",
      "Epoch  187  G loss  0.15944144\n",
      "Epoch  188  G loss  0.1548734\n",
      "Epoch  189  G loss  0.22155432\n",
      "Epoch  190  G loss  0.23752695\n",
      "Epoch  191  G loss  0.32415015\n",
      "Epoch  192  G loss  0.12470955\n",
      "Epoch  193  G loss  0.15529968\n",
      "Epoch  194  G loss  0.17208579\n",
      "Epoch  195  G loss  0.093244426\n",
      "Epoch  196  G loss  0.1369613\n",
      "Epoch  197  G loss  0.25847435\n",
      "Epoch  198  G loss  0.16516304\n",
      "Epoch  199  G loss  0.08508418\n",
      "Epoch  200  G loss  0.121797636\n",
      "Epoch  201  G loss  0.31086394\n",
      "Epoch  202  G loss  0.120309785\n",
      "Epoch  203  G loss  0.15041533\n",
      "Epoch  204  G loss  0.12911497\n",
      "Epoch  205  G loss  0.20284183\n",
      "Epoch  206  G loss  0.14057484\n",
      "Epoch  207  G loss  0.110244535\n",
      "Epoch  208  G loss  0.14732996\n",
      "Epoch  209  G loss  0.23447251\n",
      "Epoch  210  G loss  0.25005445\n",
      "Epoch  211  G loss  0.13953972\n",
      "Epoch  212  G loss  0.123270996\n",
      "Epoch  213  G loss  0.16559406\n",
      "Epoch  214  G loss  0.17522421\n",
      "Epoch  215  G loss  0.20766065\n",
      "Epoch  216  G loss  0.104519814\n",
      "Epoch  217  G loss  0.12229209\n",
      "Epoch  218  G loss  0.2494497\n",
      "Epoch  219  G loss  0.17623535\n",
      "Epoch  220  G loss  0.1417779\n",
      "Epoch  221  G loss  0.21930453\n",
      "Epoch  222  G loss  0.1950407\n",
      "Epoch  223  G loss  0.18246937\n",
      "Epoch  224  G loss  0.20042089\n",
      "Epoch  225  G loss  0.17760208\n",
      "Epoch  226  G loss  0.25191557\n",
      "Epoch  227  G loss  0.15188336\n",
      "Epoch  228  G loss  0.18820369\n",
      "Epoch  229  G loss  0.16743293\n",
      "Epoch  230  G loss  0.15912686\n",
      "Epoch  231  G loss  0.72303474\n",
      "Epoch  232  G loss  0.15056531\n",
      "Epoch  233  G loss  0.1410707\n",
      "Epoch  234  G loss  0.15450837\n",
      "Epoch  235  G loss  0.19763297\n",
      "Epoch  236  G loss  0.105575345\n",
      "Epoch  237  G loss  0.17466007\n",
      "Epoch  238  G loss  0.17341961\n",
      "Epoch  239  G loss  0.17332467\n",
      "Epoch  240  G loss  0.13724808\n",
      "Epoch  241  G loss  0.16234386\n",
      "Epoch  242  G loss  0.34849864\n",
      "Epoch  243  G loss  0.30736613\n",
      "Epoch  244  G loss  0.15564544\n",
      "Epoch  245  G loss  0.30987942\n",
      "Epoch  246  G loss  0.15060997\n",
      "Epoch  247  G loss  0.21652067\n",
      "Epoch  248  G loss  0.15747418\n",
      "Epoch  249  G loss  0.14916164\n",
      "Epoch  250  G loss  0.25815067\n",
      "Epoch  251  G loss  0.27934092\n",
      "Epoch  252  G loss  0.22108737\n",
      "Epoch  253  G loss  0.14967671\n",
      "Epoch  254  G loss  0.19518003\n",
      "Epoch  255  G loss  0.11806967\n",
      "Epoch  256  G loss  0.17618264\n",
      "Epoch  257  G loss  0.10306104\n",
      "Epoch  258  G loss  0.14689806\n",
      "Epoch  259  G loss  0.16074084\n",
      "Epoch  260  G loss  0.50975335\n",
      "Epoch  261  G loss  0.19420981\n",
      "Epoch  262  G loss  0.25365376\n",
      "Epoch  263  G loss  0.108016856\n",
      "Epoch  264  G loss  0.14719947\n",
      "Epoch  265  G loss  0.295479\n",
      "Epoch  266  G loss  0.10455757\n",
      "Epoch  267  G loss  0.10292567\n",
      "Epoch  268  G loss  0.5105715\n",
      "Epoch  269  G loss  0.19058663\n",
      "Epoch  270  G loss  0.1489504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  271  G loss  0.124502525\n",
      "Epoch  272  G loss  0.40845728\n",
      "Epoch  273  G loss  0.12347734\n",
      "Epoch  274  G loss  0.1709553\n",
      "Epoch  275  G loss  0.15684769\n",
      "Epoch  276  G loss  0.10483112\n",
      "Epoch  277  G loss  0.15922403\n",
      "Epoch  278  G loss  0.14057186\n",
      "Epoch  279  G loss  0.2369588\n",
      "Epoch  280  G loss  0.31027657\n",
      "Epoch  281  G loss  0.2770005\n",
      "Epoch  282  G loss  0.16039759\n",
      "Epoch  283  G loss  0.13613439\n",
      "Epoch  284  G loss  0.19005275\n",
      "Epoch  285  G loss  0.20366788\n",
      "Epoch  286  G loss  0.10970208\n",
      "Epoch  287  G loss  0.118929125\n",
      "Epoch  288  G loss  0.1475213\n",
      "Epoch  289  G loss  0.18699023\n",
      "Epoch  290  G loss  0.1353387\n",
      "Epoch  291  G loss  0.20308574\n",
      "Epoch  292  G loss  0.12708774\n",
      "Epoch  293  G loss  0.22450605\n",
      "Epoch  294  G loss  0.15617195\n",
      "Epoch  295  G loss  0.108376\n",
      "Epoch  296  G loss  0.1875945\n",
      "Epoch  297  G loss  0.3084266\n",
      "Epoch  298  G loss  0.14707111\n",
      "Epoch  299  G loss  0.16378863\n",
      "Epoch  300  G loss  0.12964801\n",
      "Epoch  301  G loss  0.15188791\n",
      "Epoch  302  G loss  0.13704798\n",
      "Epoch  303  G loss  0.21324116\n",
      "Epoch  304  G loss  0.2777837\n",
      "Epoch  305  G loss  0.14936799\n",
      "Epoch  306  G loss  0.17371468\n",
      "Epoch  307  G loss  0.16091922\n",
      "Epoch  308  G loss  0.19315965\n",
      "Epoch  309  G loss  0.11809784\n",
      "Epoch  310  G loss  0.12098176\n",
      "Epoch  311  G loss  0.16535372\n",
      "Epoch  312  G loss  0.15044954\n",
      "Epoch  313  G loss  0.15982187\n",
      "Epoch  314  G loss  0.33460903\n",
      "Epoch  315  G loss  0.25671786\n",
      "Epoch  316  G loss  0.12786338\n",
      "Epoch  317  G loss  0.11936117\n",
      "Epoch  318  G loss  0.08532407\n",
      "Epoch  319  G loss  0.2167997\n",
      "Epoch  320  G loss  0.2182778\n",
      "Epoch  321  G loss  0.14849108\n",
      "Epoch  322  G loss  0.15179476\n",
      "Epoch  323  G loss  0.10785178\n",
      "Epoch  324  G loss  0.16144985\n",
      "Epoch  325  G loss  0.20988251\n",
      "Epoch  326  G loss  0.12825398\n",
      "Epoch  327  G loss  0.13365914\n",
      "Epoch  328  G loss  0.325292\n",
      "Epoch  329  G loss  0.21856295\n",
      "Epoch  330  G loss  0.1487416\n",
      "Epoch  331  G loss  0.121527016\n",
      "Epoch  332  G loss  0.16727486\n",
      "Epoch  333  G loss  0.24657357\n",
      "Epoch  334  G loss  0.13012433\n",
      "Epoch  335  G loss  0.08751349\n",
      "Epoch  336  G loss  0.18684068\n",
      "Epoch  337  G loss  0.2649899\n",
      "Epoch  338  G loss  0.16708788\n",
      "Epoch  339  G loss  0.18213087\n",
      "Epoch  340  G loss  0.25128746\n",
      "Epoch  341  G loss  0.18770048\n",
      "Epoch  342  G loss  0.13801159\n",
      "Epoch  343  G loss  0.3107493\n",
      "Epoch  344  G loss  0.15422896\n",
      "Epoch  345  G loss  0.1711585\n",
      "Epoch  346  G loss  0.3094213\n",
      "Epoch  347  G loss  0.06519799\n",
      "Epoch  348  G loss  0.12992226\n",
      "Epoch  349  G loss  0.18478964\n",
      "Epoch  350  G loss  0.3131496\n",
      "Epoch  351  G loss  0.40085655\n",
      "Epoch  352  G loss  0.19748825\n",
      "Epoch  353  G loss  0.13637187\n",
      "Epoch  354  G loss  0.15194848\n",
      "Epoch  355  G loss  0.087865725\n",
      "Epoch  356  G loss  0.12682213\n",
      "Epoch  357  G loss  0.3045871\n",
      "Epoch  358  G loss  0.15875396\n",
      "Epoch  359  G loss  0.16644055\n",
      "Epoch  360  G loss  0.15221071\n",
      "Epoch  361  G loss  0.09452611\n",
      "Epoch  362  G loss  0.09360489\n",
      "Epoch  363  G loss  0.2415367\n",
      "Epoch  364  G loss  0.08601618\n",
      "Epoch  365  G loss  0.11798962\n",
      "Epoch  366  G loss  0.108151756\n",
      "Epoch  367  G loss  0.1651421\n",
      "Epoch  368  G loss  0.10040103\n",
      "Epoch  369  G loss  0.16554195\n",
      "Epoch  370  G loss  0.23086257\n",
      "Epoch  371  G loss  0.17760107\n",
      "Epoch  372  G loss  0.11566855\n",
      "Epoch  373  G loss  0.35590565\n",
      "Epoch  374  G loss  0.09402762\n",
      "Epoch  375  G loss  0.48207134\n",
      "Epoch  376  G loss  0.0785289\n",
      "Epoch  377  G loss  0.30162632\n",
      "Epoch  378  G loss  0.1939595\n",
      "Epoch  379  G loss  0.20994903\n",
      "Epoch  380  G loss  0.09979254\n",
      "Epoch  381  G loss  0.16640016\n",
      "Epoch  382  G loss  0.1298989\n",
      "Epoch  383  G loss  0.15756583\n",
      "Epoch  384  G loss  0.3385865\n",
      "Epoch  385  G loss  0.30822122\n",
      "Epoch  386  G loss  0.13649008\n",
      "Epoch  387  G loss  0.16844738\n",
      "Epoch  388  G loss  0.20986483\n",
      "Epoch  389  G loss  0.097704746\n",
      "Epoch  390  G loss  0.25438833\n",
      "Epoch  391  G loss  0.3064878\n",
      "Epoch  392  G loss  0.20526746\n",
      "Epoch  393  G loss  0.14765948\n",
      "Epoch  394  G loss  0.5184276\n",
      "Epoch  395  G loss  0.40090021\n",
      "Epoch  396  G loss  0.105270326\n",
      "Epoch  397  G loss  0.16089833\n",
      "Epoch  398  G loss  0.14592397\n",
      "Epoch  399  G loss  0.21873626\n",
      "Epoch  400  G loss  0.33089703\n",
      "Epoch  401  G loss  0.11264344\n",
      "Epoch  402  G loss  0.155593\n",
      "Epoch  403  G loss  0.17814001\n",
      "Epoch  404  G loss  0.07329202\n",
      "Epoch  405  G loss  0.1872236\n",
      "Epoch  406  G loss  0.16224371\n",
      "Epoch  407  G loss  0.115454555\n",
      "Epoch  408  G loss  0.14065525\n",
      "Epoch  409  G loss  0.17505997\n",
      "Epoch  410  G loss  0.092583686\n",
      "Epoch  411  G loss  0.11421975\n",
      "Epoch  412  G loss  0.38315126\n",
      "Epoch  413  G loss  0.2354975\n",
      "Epoch  414  G loss  0.10774347\n",
      "Epoch  415  G loss  0.13838154\n",
      "Epoch  416  G loss  0.3925703\n",
      "Epoch  417  G loss  0.23186287\n",
      "Epoch  418  G loss  0.20472877\n",
      "Epoch  419  G loss  0.10554248\n",
      "Epoch  420  G loss  0.44610938\n",
      "Epoch  421  G loss  0.1781734\n",
      "Epoch  422  G loss  0.16354689\n",
      "Epoch  423  G loss  0.29426536\n",
      "Epoch  424  G loss  0.10639833\n",
      "Epoch  425  G loss  0.290022\n",
      "Epoch  426  G loss  0.14052647\n",
      "Epoch  427  G loss  0.07709126\n",
      "Epoch  428  G loss  0.17904663\n",
      "Epoch  429  G loss  0.14292479\n",
      "Epoch  430  G loss  0.33397388\n",
      "Epoch  431  G loss  0.7149503\n",
      "Epoch  432  G loss  0.21179228\n",
      "Epoch  433  G loss  0.2798525\n",
      "Epoch  434  G loss  0.1131016\n",
      "Epoch  435  G loss  0.1420849\n",
      "Epoch  436  G loss  0.11030555\n",
      "Epoch  437  G loss  0.083712205\n",
      "Epoch  438  G loss  0.12224828\n",
      "Epoch  439  G loss  0.25394958\n",
      "Epoch  440  G loss  0.27072647\n",
      "Epoch  441  G loss  0.11198042\n",
      "Epoch  442  G loss  0.12972596\n",
      "Epoch  443  G loss  0.15022059\n",
      "Epoch  444  G loss  0.09179031\n",
      "Epoch  445  G loss  0.23868658\n",
      "Epoch  446  G loss  0.089169055\n",
      "Epoch  447  G loss  0.18067923\n",
      "Epoch  448  G loss  0.09017055\n",
      "Epoch  449  G loss  0.13142794\n",
      "Epoch  450  G loss  0.34111547\n",
      "Epoch  451  G loss  0.11504698\n",
      "Epoch  452  G loss  0.17625159\n",
      "Epoch  453  G loss  0.11743242\n",
      "Epoch  454  G loss  0.21278048\n",
      "Epoch  455  G loss  0.11384173\n",
      "Epoch  456  G loss  0.22001657\n",
      "Epoch  457  G loss  0.22222127\n",
      "Epoch  458  G loss  0.21798646\n",
      "Epoch  459  G loss  0.14933807\n",
      "Epoch  460  G loss  0.11311078\n",
      "Epoch  461  G loss  0.15099968\n",
      "Epoch  462  G loss  0.16587523\n",
      "Epoch  463  G loss  0.111074895\n",
      "Epoch  464  G loss  0.13078348\n",
      "Epoch  465  G loss  0.12317471\n",
      "Epoch  466  G loss  0.24721456\n",
      "Epoch  467  G loss  0.07432275\n",
      "Epoch  468  G loss  0.1111181\n",
      "Epoch  469  G loss  0.166181\n",
      "Epoch  470  G loss  0.07386653\n",
      "Epoch  471  G loss  0.16158916\n",
      "Epoch  472  G loss  0.099094406\n",
      "Epoch  473  G loss  0.16248749\n",
      "Epoch  474  G loss  0.118728004\n",
      "Epoch  475  G loss  0.15668592\n",
      "Epoch  476  G loss  0.8321192\n",
      "Epoch  477  G loss  0.43642592\n",
      "Epoch  478  G loss  0.16386677\n",
      "Epoch  479  G loss  0.18326044\n",
      "Epoch  480  G loss  0.16180323\n",
      "Epoch  481  G loss  0.11907308\n",
      "Epoch  482  G loss  0.12006134\n",
      "Epoch  483  G loss  0.10486269\n",
      "Epoch  484  G loss  0.28710568\n",
      "Epoch  485  G loss  0.30547482\n",
      "Epoch  486  G loss  0.20646363\n",
      "Epoch  487  G loss  0.102330014\n",
      "Epoch  488  G loss  0.17549875\n",
      "Epoch  489  G loss  0.16907874\n",
      "Epoch  490  G loss  0.13167222\n",
      "Epoch  491  G loss  0.09610419\n",
      "Epoch  492  G loss  0.45188817\n",
      "Epoch  493  G loss  0.14655419\n",
      "Epoch  494  G loss  0.3010273\n",
      "Epoch  495  G loss  0.23597349\n",
      "Epoch  496  G loss  0.11239755\n",
      "Epoch  497  G loss  0.14904208\n",
      "Epoch  498  G loss  0.1698404\n",
      "Epoch  499  G loss  0.13291532\n",
      "Epoch  500  G loss  0.116902776\n",
      "Epoch  501  G loss  0.16152826\n",
      "Epoch  502  G loss  0.14705577\n",
      "Epoch  503  G loss  0.09180403\n",
      "Epoch  504  G loss  0.09267822\n",
      "Epoch  505  G loss  0.10219072\n",
      "Epoch  506  G loss  0.21374232\n",
      "Epoch  507  G loss  0.19998643\n",
      "Epoch  508  G loss  0.060825884\n",
      "Epoch  509  G loss  0.08758765\n",
      "Epoch  510  G loss  0.102817975\n",
      "Epoch  511  G loss  0.256175\n",
      "Epoch  512  G loss  0.102308124\n",
      "Epoch  513  G loss  0.123956546\n",
      "Epoch  514  G loss  0.17878959\n",
      "Epoch  515  G loss  0.12252704\n",
      "Epoch  516  G loss  0.0797973\n",
      "Epoch  517  G loss  0.13055488\n",
      "Epoch  518  G loss  0.24551068\n",
      "Epoch  519  G loss  0.06960474\n",
      "Epoch  520  G loss  0.13829394\n",
      "Epoch  521  G loss  0.09167699\n",
      "Epoch  522  G loss  0.16448116\n",
      "Epoch  523  G loss  0.28993237\n",
      "Epoch  524  G loss  0.15870427\n",
      "Epoch  525  G loss  0.08643582\n",
      "Epoch  526  G loss  0.280706\n",
      "Epoch  527  G loss  0.059900455\n",
      "Epoch  528  G loss  0.20443112\n",
      "Epoch  529  G loss  0.14449525\n",
      "Epoch  530  G loss  0.15326007\n",
      "Epoch  531  G loss  0.0964018\n",
      "Epoch  532  G loss  0.18086746\n",
      "Epoch  533  G loss  0.11470255\n",
      "Epoch  534  G loss  0.14176628\n",
      "Epoch  535  G loss  0.13067701\n",
      "Epoch  536  G loss  0.095952734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  537  G loss  0.15147257\n",
      "Epoch  538  G loss  0.09899628\n",
      "Epoch  539  G loss  0.07447948\n",
      "Epoch  540  G loss  0.100146726\n",
      "Epoch  541  G loss  0.1332173\n",
      "Epoch  542  G loss  0.07383353\n",
      "Epoch  543  G loss  0.07620466\n",
      "Epoch  544  G loss  0.09800191\n",
      "Epoch  545  G loss  0.12715173\n",
      "Epoch  546  G loss  0.108219475\n",
      "Epoch  547  G loss  0.11543948\n",
      "Epoch  548  G loss  0.07054533\n",
      "Epoch  549  G loss  0.22428298\n",
      "Epoch  550  G loss  0.05066558\n",
      "Epoch  551  G loss  0.11074953\n",
      "Epoch  552  G loss  0.22270176\n",
      "Epoch  553  G loss  0.13853996\n",
      "Epoch  554  G loss  0.08698146\n",
      "Epoch  555  G loss  0.17253473\n",
      "Epoch  556  G loss  0.11895706\n",
      "Epoch  557  G loss  0.08669489\n",
      "Epoch  558  G loss  0.08483681\n",
      "Epoch  559  G loss  0.0798487\n",
      "Epoch  560  G loss  0.38886094\n",
      "Epoch  561  G loss  0.39002857\n",
      "Epoch  562  G loss  0.0966554\n",
      "Epoch  563  G loss  0.13995314\n",
      "Epoch  564  G loss  0.10099776\n",
      "Epoch  565  G loss  0.11173349\n",
      "Epoch  566  G loss  0.08576919\n",
      "Epoch  567  G loss  0.14428523\n",
      "Epoch  568  G loss  0.14983891\n",
      "Epoch  569  G loss  0.07384701\n",
      "Epoch  570  G loss  0.06874318\n",
      "Epoch  571  G loss  0.16453427\n",
      "Epoch  572  G loss  0.12310854\n",
      "Epoch  573  G loss  0.05766529\n",
      "Epoch  574  G loss  0.15909292\n",
      "Epoch  575  G loss  0.0709026\n",
      "Epoch  576  G loss  0.13607551\n",
      "Epoch  577  G loss  0.14442985\n",
      "Epoch  578  G loss  0.13799573\n",
      "Epoch  579  G loss  0.2992834\n",
      "Epoch  580  G loss  0.106431596\n",
      "Epoch  581  G loss  0.08285367\n",
      "Epoch  582  G loss  0.12170914\n",
      "Epoch  583  G loss  0.12901781\n",
      "Epoch  584  G loss  0.31039488\n",
      "Epoch  585  G loss  0.060473118\n",
      "Epoch  586  G loss  0.041596763\n",
      "Epoch  587  G loss  0.09343088\n",
      "Epoch  588  G loss  0.09556043\n",
      "Epoch  589  G loss  0.1054923\n",
      "Epoch  590  G loss  0.22607541\n",
      "Epoch  591  G loss  0.16666123\n",
      "Epoch  592  G loss  0.09739157\n",
      "Epoch  593  G loss  0.0805625\n",
      "Epoch  594  G loss  0.09903549\n",
      "Epoch  595  G loss  0.06994496\n",
      "Epoch  596  G loss  0.37757105\n",
      "Epoch  597  G loss  0.26918653\n",
      "Epoch  598  G loss  0.12997493\n",
      "Epoch  599  G loss  0.060674105\n",
      "Epoch  600  G loss  0.11005468\n",
      "Epoch  601  G loss  0.16947085\n",
      "Epoch  602  G loss  0.111711994\n",
      "Epoch  603  G loss  0.14759123\n",
      "Epoch  604  G loss  0.08864558\n",
      "Epoch  605  G loss  0.07297145\n",
      "Epoch  606  G loss  0.13232148\n",
      "Epoch  607  G loss  0.13337442\n",
      "Epoch  608  G loss  0.13500509\n",
      "Epoch  609  G loss  0.11794802\n",
      "Epoch  610  G loss  0.13588496\n",
      "Epoch  611  G loss  0.18961391\n",
      "Epoch  612  G loss  0.11907166\n",
      "Epoch  613  G loss  0.13702767\n",
      "Epoch  614  G loss  0.057683125\n",
      "Epoch  615  G loss  0.13500993\n",
      "Epoch  616  G loss  0.11549425\n",
      "Epoch  617  G loss  0.08117741\n",
      "Epoch  618  G loss  0.030782666\n",
      "Epoch  619  G loss  0.15571776\n",
      "Epoch  620  G loss  0.22030796\n",
      "Epoch  621  G loss  0.31009695\n",
      "Epoch  622  G loss  0.21330628\n",
      "Epoch  623  G loss  0.10130072\n",
      "Epoch  624  G loss  0.18771785\n",
      "Epoch  625  G loss  0.07948659\n",
      "Epoch  626  G loss  0.06872407\n",
      "Epoch  627  G loss  0.12469761\n",
      "Epoch  628  G loss  0.13751432\n",
      "Epoch  629  G loss  0.10088169\n",
      "Epoch  630  G loss  0.12303682\n",
      "Epoch  631  G loss  0.16265562\n",
      "Epoch  632  G loss  0.18669334\n",
      "Epoch  633  G loss  0.08469863\n",
      "Epoch  634  G loss  0.3145867\n",
      "Epoch  635  G loss  0.17659664\n",
      "Epoch  636  G loss  0.039968893\n",
      "Epoch  637  G loss  0.13532308\n",
      "Epoch  638  G loss  0.17900896\n",
      "Epoch  639  G loss  0.13760312\n",
      "Epoch  640  G loss  0.33263493\n",
      "Epoch  641  G loss  0.12268007\n",
      "Epoch  642  G loss  0.16179357\n",
      "Epoch  643  G loss  0.16608334\n",
      "Epoch  644  G loss  0.08502224\n",
      "Epoch  645  G loss  0.24487704\n",
      "Epoch  646  G loss  0.10084303\n",
      "Epoch  647  G loss  0.17302439\n",
      "Epoch  648  G loss  0.10835963\n",
      "Epoch  649  G loss  0.07315558\n",
      "Epoch  650  G loss  0.08894786\n",
      "Epoch  651  G loss  0.07114963\n",
      "Epoch  652  G loss  0.11388908\n",
      "Epoch  653  G loss  0.09249661\n",
      "Epoch  654  G loss  0.18754324\n",
      "Epoch  655  G loss  0.19949204\n",
      "Epoch  656  G loss  0.25044802\n",
      "Epoch  657  G loss  0.20327881\n",
      "Epoch  658  G loss  0.60690236\n",
      "Epoch  659  G loss  0.09592449\n",
      "Epoch  660  G loss  0.4036662\n",
      "Epoch  661  G loss  0.06962635\n",
      "Epoch  662  G loss  0.10946361\n",
      "Epoch  663  G loss  0.09415522\n",
      "Epoch  664  G loss  0.07031809\n",
      "Epoch  665  G loss  0.1466665\n",
      "Epoch  666  G loss  0.053731434\n",
      "Epoch  667  G loss  0.12122837\n",
      "Epoch  668  G loss  0.08425455\n",
      "Epoch  669  G loss  0.14774013\n",
      "Epoch  670  G loss  0.24762675\n",
      "Epoch  671  G loss  0.13598838\n",
      "Epoch  672  G loss  0.12743336\n",
      "Epoch  673  G loss  0.18641254\n",
      "Epoch  674  G loss  0.101582184\n",
      "Epoch  675  G loss  0.15681356\n",
      "Epoch  676  G loss  0.16370746\n",
      "Epoch  677  G loss  0.1094693\n",
      "Epoch  678  G loss  0.09088037\n",
      "Epoch  679  G loss  0.22507553\n",
      "Epoch  680  G loss  0.09064815\n",
      "Epoch  681  G loss  0.24532975\n",
      "Epoch  682  G loss  0.087228596\n",
      "Epoch  683  G loss  0.26939046\n",
      "Epoch  684  G loss  0.054301716\n",
      "Epoch  685  G loss  0.17524952\n",
      "Epoch  686  G loss  0.12896885\n",
      "Epoch  687  G loss  0.32730508\n",
      "Epoch  688  G loss  0.15084673\n",
      "Epoch  689  G loss  0.46335316\n",
      "Epoch  690  G loss  0.10465844\n",
      "Epoch  691  G loss  0.13165827\n",
      "Epoch  692  G loss  0.13539323\n",
      "Epoch  693  G loss  0.16080657\n",
      "Epoch  694  G loss  0.27581602\n",
      "Epoch  695  G loss  0.24838601\n",
      "Epoch  696  G loss  0.38557947\n",
      "Epoch  697  G loss  0.12833935\n",
      "Epoch  698  G loss  0.056013178\n",
      "Epoch  699  G loss  0.07738763\n",
      "Epoch  700  G loss  0.07484131\n",
      "Epoch  701  G loss  0.21025461\n",
      "Epoch  702  G loss  0.2963329\n",
      "Epoch  703  G loss  0.18086055\n",
      "Epoch  704  G loss  0.06560977\n",
      "Epoch  705  G loss  0.31154954\n",
      "Epoch  706  G loss  0.22250721\n",
      "Epoch  707  G loss  0.06149465\n",
      "Epoch  708  G loss  0.09120789\n",
      "Epoch  709  G loss  0.11843438\n",
      "Epoch  710  G loss  0.31765342\n",
      "Epoch  711  G loss  0.116829485\n",
      "Epoch  712  G loss  0.04136093\n",
      "Epoch  713  G loss  0.119342074\n",
      "Epoch  714  G loss  0.11766798\n",
      "Epoch  715  G loss  0.18173304\n",
      "Epoch  716  G loss  0.1972575\n",
      "Epoch  717  G loss  0.1658638\n",
      "Epoch  718  G loss  0.11071721\n",
      "Epoch  719  G loss  0.0964719\n",
      "Epoch  720  G loss  0.025867334\n",
      "Epoch  721  G loss  0.07096179\n",
      "Epoch  722  G loss  0.41976583\n",
      "Epoch  723  G loss  0.06627065\n",
      "Epoch  724  G loss  0.21678299\n",
      "Epoch  725  G loss  0.09325832\n",
      "Epoch  726  G loss  0.21860887\n",
      "Epoch  727  G loss  0.12642051\n",
      "Epoch  728  G loss  0.38861534\n",
      "Epoch  729  G loss  0.04259348\n",
      "Epoch  730  G loss  0.3520803\n",
      "Epoch  731  G loss  0.12718415\n",
      "Epoch  732  G loss  0.073753625\n",
      "Epoch  733  G loss  0.21233329\n",
      "Epoch  734  G loss  0.14715526\n",
      "Epoch  735  G loss  0.08734883\n",
      "Epoch  736  G loss  0.14568034\n",
      "Epoch  737  G loss  0.07473816\n",
      "Epoch  738  G loss  0.06635828\n",
      "Epoch  739  G loss  0.16233394\n",
      "Epoch  740  G loss  0.26508334\n",
      "Epoch  741  G loss  0.074854806\n",
      "Epoch  742  G loss  0.16875044\n",
      "Epoch  743  G loss  0.039956465\n",
      "Epoch  744  G loss  0.09474994\n",
      "Epoch  745  G loss  0.16808224\n",
      "Epoch  746  G loss  0.12209767\n",
      "Epoch  747  G loss  0.14217994\n",
      "Epoch  748  G loss  0.12284294\n",
      "Epoch  749  G loss  0.0595733\n",
      "Epoch  750  G loss  0.5353463\n",
      "Epoch  751  G loss  0.074662715\n",
      "Epoch  752  G loss  0.16118976\n",
      "Epoch  753  G loss  0.12486364\n",
      "Epoch  754  G loss  0.11180961\n",
      "Epoch  755  G loss  0.11117995\n",
      "Epoch  756  G loss  0.27372575\n",
      "Epoch  757  G loss  0.2956651\n",
      "Epoch  758  G loss  0.27028722\n",
      "Epoch  759  G loss  0.62772465\n",
      "Epoch  760  G loss  0.14421669\n",
      "Epoch  761  G loss  0.05690539\n",
      "Epoch  762  G loss  0.20028241\n",
      "Epoch  763  G loss  0.07900282\n",
      "Epoch  764  G loss  0.16189224\n",
      "Epoch  765  G loss  0.13152233\n",
      "Epoch  766  G loss  0.04370518\n",
      "Epoch  767  G loss  0.05024686\n",
      "Epoch  768  G loss  0.10264942\n",
      "Epoch  769  G loss  0.25825217\n",
      "Epoch  770  G loss  0.0521475\n",
      "Epoch  771  G loss  0.106432654\n",
      "Epoch  772  G loss  0.044731654\n",
      "Epoch  773  G loss  0.29010576\n",
      "Epoch  774  G loss  0.09557807\n",
      "Epoch  775  G loss  0.026816776\n",
      "Epoch  776  G loss  0.07594615\n",
      "Epoch  777  G loss  0.2046244\n",
      "Epoch  778  G loss  0.03845881\n",
      "Epoch  779  G loss  0.15989915\n",
      "Epoch  780  G loss  0.12459567\n",
      "Epoch  781  G loss  0.082139954\n",
      "Epoch  782  G loss  0.07631664\n",
      "Epoch  783  G loss  0.0675139\n",
      "Epoch  784  G loss  0.052667715\n",
      "Epoch  785  G loss  0.1005481\n",
      "Epoch  786  G loss  0.36948568\n",
      "Epoch  787  G loss  0.20209059\n",
      "Epoch  788  G loss  0.068466045\n",
      "Epoch  789  G loss  0.051485978\n",
      "Epoch  790  G loss  0.09707353\n",
      "Epoch  791  G loss  0.044235602\n",
      "Epoch  792  G loss  0.23073266\n",
      "Epoch  793  G loss  0.123347744\n",
      "Epoch  794  G loss  0.06796323\n",
      "Epoch  795  G loss  0.36883438\n",
      "Epoch  796  G loss  0.09067367\n",
      "Epoch  797  G loss  0.11012091\n",
      "Epoch  798  G loss  0.15215428\n",
      "Epoch  799  G loss  0.103977814\n",
      "Epoch  800  G loss  0.037581824\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  801  G loss  0.054688513\n",
      "Epoch  802  G loss  0.16920893\n",
      "Epoch  803  G loss  0.08260837\n",
      "Epoch  804  G loss  0.09591024\n",
      "Epoch  805  G loss  0.08673957\n",
      "Epoch  806  G loss  0.21324246\n",
      "Epoch  807  G loss  0.081644386\n",
      "Epoch  808  G loss  0.09710407\n",
      "Epoch  809  G loss  0.09546992\n",
      "Epoch  810  G loss  0.072928\n",
      "Epoch  811  G loss  0.18737242\n",
      "Epoch  812  G loss  0.056738578\n",
      "Epoch  813  G loss  0.13420579\n",
      "Epoch  814  G loss  0.1499995\n",
      "Epoch  815  G loss  0.06198655\n",
      "Epoch  816  G loss  0.15540986\n",
      "Epoch  817  G loss  0.15016016\n",
      "Epoch  818  G loss  0.15595475\n",
      "Epoch  819  G loss  0.2340992\n",
      "Epoch  820  G loss  0.16606267\n",
      "Epoch  821  G loss  0.0883504\n",
      "Epoch  822  G loss  0.06867757\n",
      "Epoch  823  G loss  0.08062289\n",
      "Epoch  824  G loss  0.14329635\n",
      "Epoch  825  G loss  0.264871\n",
      "Epoch  826  G loss  0.06824957\n",
      "Epoch  827  G loss  0.079392664\n",
      "Epoch  828  G loss  0.09749536\n",
      "Epoch  829  G loss  0.93270755\n",
      "Epoch  830  G loss  0.18387936\n",
      "Epoch  831  G loss  0.109633386\n",
      "Epoch  832  G loss  0.09496552\n",
      "Epoch  833  G loss  0.16890916\n",
      "Epoch  834  G loss  0.16301353\n",
      "Epoch  835  G loss  0.069204204\n",
      "Epoch  836  G loss  0.06256178\n",
      "Epoch  837  G loss  0.08208081\n",
      "Epoch  838  G loss  0.23359892\n",
      "Epoch  839  G loss  0.110891104\n",
      "Epoch  840  G loss  0.073476136\n",
      "Epoch  841  G loss  0.10218032\n",
      "Epoch  842  G loss  0.17180732\n",
      "Epoch  843  G loss  0.15904787\n",
      "Epoch  844  G loss  0.1117302\n",
      "Epoch  845  G loss  0.21026322\n",
      "Epoch  846  G loss  0.17737684\n",
      "Epoch  847  G loss  0.2983933\n",
      "Epoch  848  G loss  0.062509805\n",
      "Epoch  849  G loss  0.11458133\n",
      "Epoch  850  G loss  0.16839148\n",
      "Epoch  851  G loss  0.08186394\n",
      "Epoch  852  G loss  0.23401904\n",
      "Epoch  853  G loss  0.0975701\n",
      "Epoch  854  G loss  0.11093923\n",
      "Epoch  855  G loss  0.15075904\n",
      "Epoch  856  G loss  0.07157436\n",
      "Epoch  857  G loss  0.067820035\n",
      "Epoch  858  G loss  0.20140308\n",
      "Epoch  859  G loss  0.11492039\n",
      "Epoch  860  G loss  0.067916125\n",
      "Epoch  861  G loss  0.20880926\n",
      "Epoch  862  G loss  0.10441572\n",
      "Epoch  863  G loss  0.28013402\n",
      "Epoch  864  G loss  0.07831784\n",
      "Epoch  865  G loss  0.039659873\n",
      "Epoch  866  G loss  0.3201567\n",
      "Epoch  867  G loss  0.22054338\n",
      "Epoch  868  G loss  0.2698308\n",
      "Epoch  869  G loss  0.17262435\n",
      "Epoch  870  G loss  0.041830212\n",
      "Epoch  871  G loss  0.51293004\n",
      "Epoch  872  G loss  0.109723076\n",
      "Epoch  873  G loss  0.05345597\n",
      "Epoch  874  G loss  0.38653007\n",
      "Epoch  875  G loss  0.0941778\n",
      "Epoch  876  G loss  0.07195109\n",
      "Epoch  877  G loss  0.10622004\n",
      "Epoch  878  G loss  0.11780959\n",
      "Epoch  879  G loss  0.19396849\n",
      "Epoch  880  G loss  0.13392374\n",
      "Epoch  881  G loss  0.050853647\n",
      "Epoch  882  G loss  0.036397923\n",
      "Epoch  883  G loss  0.03222078\n",
      "Epoch  884  G loss  0.118802965\n",
      "Epoch  885  G loss  0.31276453\n",
      "Epoch  886  G loss  0.062256202\n",
      "Epoch  887  G loss  0.06535138\n",
      "Epoch  888  G loss  0.21910983\n",
      "Epoch  889  G loss  0.086103275\n",
      "Epoch  890  G loss  0.084932216\n",
      "Epoch  891  G loss  0.1702989\n",
      "Epoch  892  G loss  0.21801607\n",
      "Epoch  893  G loss  0.11762228\n",
      "Epoch  894  G loss  0.15527686\n",
      "Epoch  895  G loss  0.063073784\n",
      "Epoch  896  G loss  0.068046205\n",
      "Epoch  897  G loss  0.077323735\n",
      "Epoch  898  G loss  0.24948351\n",
      "Epoch  899  G loss  0.043264546\n",
      "Epoch  900  G loss  0.14545698\n",
      "Epoch  901  G loss  0.14217559\n",
      "Epoch  902  G loss  0.27586353\n",
      "Epoch  903  G loss  0.089532815\n",
      "Epoch  904  G loss  0.09533078\n",
      "Epoch  905  G loss  0.27202243\n",
      "Epoch  906  G loss  0.109370664\n",
      "Epoch  907  G loss  0.17354426\n",
      "Epoch  908  G loss  0.08646317\n",
      "Epoch  909  G loss  0.055378206\n",
      "Epoch  910  G loss  0.12749542\n",
      "Epoch  911  G loss  0.09683311\n",
      "Epoch  912  G loss  0.07398562\n",
      "Epoch  913  G loss  0.15750095\n",
      "Epoch  914  G loss  0.05950036\n",
      "Epoch  915  G loss  0.0725701\n",
      "Epoch  916  G loss  0.058736425\n",
      "Epoch  917  G loss  0.32473025\n",
      "Epoch  918  G loss  0.08977811\n",
      "Epoch  919  G loss  0.06641451\n",
      "Epoch  920  G loss  0.06853269\n",
      "Epoch  921  G loss  0.06548901\n",
      "Epoch  922  G loss  0.15221474\n",
      "Epoch  923  G loss  0.066440955\n",
      "Epoch  924  G loss  0.15813905\n",
      "Epoch  925  G loss  0.20994414\n",
      "Epoch  926  G loss  0.09206594\n",
      "Epoch  927  G loss  0.108717054\n",
      "Epoch  928  G loss  0.059153985\n",
      "Epoch  929  G loss  0.17837113\n",
      "Epoch  930  G loss  0.08826804\n",
      "Epoch  931  G loss  0.0567714\n",
      "Epoch  932  G loss  0.07990347\n",
      "Epoch  933  G loss  0.057264466\n",
      "Epoch  934  G loss  0.32327455\n",
      "Epoch  935  G loss  0.10290699\n",
      "Epoch  936  G loss  0.0327336\n",
      "Epoch  937  G loss  0.14066525\n",
      "Epoch  938  G loss  0.06209606\n",
      "Epoch  939  G loss  0.11647613\n",
      "Epoch  940  G loss  0.096789114\n",
      "Epoch  941  G loss  0.10643465\n",
      "Epoch  942  G loss  0.10474242\n",
      "Epoch  943  G loss  0.12246067\n",
      "Epoch  944  G loss  0.095619634\n",
      "Epoch  945  G loss  0.08645682\n",
      "Epoch  946  G loss  0.06283181\n",
      "Epoch  947  G loss  0.07490841\n",
      "Epoch  948  G loss  0.09479272\n",
      "Epoch  949  G loss  0.04598099\n",
      "Epoch  950  G loss  0.095218375\n",
      "Epoch  951  G loss  0.20540713\n",
      "Epoch  952  G loss  0.31660074\n",
      "Epoch  953  G loss  0.13030148\n",
      "Epoch  954  G loss  0.11134383\n",
      "Epoch  955  G loss  0.11550002\n",
      "Epoch  956  G loss  0.1815571\n",
      "Epoch  957  G loss  0.13556916\n",
      "Epoch  958  G loss  0.10112997\n",
      "Epoch  959  G loss  0.0728368\n",
      "Epoch  960  G loss  0.039295644\n",
      "Epoch  961  G loss  0.17438972\n",
      "Epoch  962  G loss  0.13721113\n",
      "Epoch  963  G loss  0.22444405\n",
      "Epoch  964  G loss  0.063909\n",
      "Epoch  965  G loss  0.21162957\n",
      "Epoch  966  G loss  0.08594029\n",
      "Epoch  967  G loss  0.10521427\n",
      "Epoch  968  G loss  0.3403862\n",
      "Epoch  969  G loss  0.09895127\n",
      "Epoch  970  G loss  0.046405617\n",
      "Epoch  971  G loss  0.08745815\n",
      "Epoch  972  G loss  0.10405637\n",
      "Epoch  973  G loss  0.08730524\n",
      "Epoch  974  G loss  0.086958915\n",
      "Epoch  975  G loss  0.06090163\n",
      "Epoch  976  G loss  0.15203819\n",
      "Epoch  977  G loss  0.28576076\n",
      "Epoch  978  G loss  0.096953765\n",
      "Epoch  979  G loss  0.13241541\n",
      "Epoch  980  G loss  0.15949844\n",
      "Epoch  981  G loss  0.23052055\n",
      "Epoch  982  G loss  0.086319104\n",
      "Epoch  983  G loss  0.16312608\n",
      "Epoch  984  G loss  0.08408131\n",
      "Epoch  985  G loss  0.1863056\n",
      "Epoch  986  G loss  0.11374917\n",
      "Epoch  987  G loss  0.14434114\n",
      "Epoch  988  G loss  0.123011485\n",
      "Epoch  989  G loss  0.32552573\n",
      "Epoch  990  G loss  0.041941103\n",
      "Epoch  991  G loss  0.09871807\n",
      "Epoch  992  G loss  0.1408042\n",
      "Epoch  993  G loss  0.08207512\n",
      "Epoch  994  G loss  0.09290262\n",
      "Epoch  995  G loss  0.072832294\n",
      "Epoch  996  G loss  0.16602762\n",
      "Epoch  997  G loss  0.04381333\n",
      "Epoch  998  G loss  0.11294831\n",
      "Epoch  999  G loss  0.11474036\n",
      "Epoch  1000  G loss  0.083905615\n",
      "Epoch  1001  G loss  0.058041405\n",
      "Epoch  1002  G loss  0.21064705\n",
      "Epoch  1003  G loss  0.12687281\n",
      "Epoch  1004  G loss  0.31998667\n",
      "Epoch  1005  G loss  0.06784921\n",
      "Epoch  1006  G loss  0.0997213\n",
      "Epoch  1007  G loss  0.073278576\n",
      "Epoch  1008  G loss  0.10590998\n",
      "Epoch  1009  G loss  0.039748654\n",
      "Epoch  1010  G loss  0.099337876\n",
      "Epoch  1011  G loss  0.049930774\n",
      "Epoch  1012  G loss  0.063571215\n",
      "Epoch  1013  G loss  0.075345024\n",
      "Epoch  1014  G loss  0.11892706\n",
      "Epoch  1015  G loss  0.054516286\n",
      "Epoch  1016  G loss  0.06719746\n",
      "Epoch  1017  G loss  0.020689227\n",
      "Epoch  1018  G loss  0.05741273\n",
      "Epoch  1019  G loss  0.054069407\n",
      "Epoch  1020  G loss  0.09011658\n",
      "Epoch  1021  G loss  0.09117769\n",
      "Epoch  1022  G loss  0.06286152\n",
      "Epoch  1023  G loss  0.07175794\n",
      "Epoch  1024  G loss  0.07861922\n",
      "Epoch  1025  G loss  0.04019805\n",
      "Epoch  1026  G loss  0.041183762\n",
      "Epoch  1027  G loss  0.048895128\n",
      "Epoch  1028  G loss  0.054695\n",
      "Epoch  1029  G loss  0.048416324\n",
      "Epoch  1030  G loss  0.08946976\n",
      "Epoch  1031  G loss  0.0860938\n",
      "Epoch  1032  G loss  0.07255671\n",
      "Epoch  1033  G loss  0.08226177\n",
      "Epoch  1034  G loss  0.19364692\n",
      "Epoch  1035  G loss  0.05862072\n",
      "Epoch  1036  G loss  0.04914183\n",
      "Epoch  1037  G loss  0.059228715\n",
      "Epoch  1038  G loss  0.48346904\n",
      "Epoch  1039  G loss  0.045401886\n",
      "Epoch  1040  G loss  0.025654722\n",
      "Epoch  1041  G loss  0.30643135\n",
      "Epoch  1042  G loss  0.28923896\n",
      "Epoch  1043  G loss  0.055458523\n",
      "Epoch  1044  G loss  0.04499644\n",
      "Epoch  1045  G loss  0.08276563\n",
      "Epoch  1046  G loss  0.13106927\n",
      "Epoch  1047  G loss  0.17115185\n",
      "Epoch  1048  G loss  0.08095688\n",
      "Epoch  1049  G loss  0.055580094\n",
      "Epoch  1050  G loss  0.09943443\n",
      "Epoch  1051  G loss  0.21943581\n",
      "Epoch  1052  G loss  0.029253246\n",
      "Epoch  1053  G loss  0.13631265\n",
      "Epoch  1054  G loss  0.22839929\n",
      "Epoch  1055  G loss  0.044062987\n",
      "Epoch  1056  G loss  0.11502461\n",
      "Epoch  1057  G loss  0.18111221\n",
      "Epoch  1058  G loss  0.03724318\n",
      "Epoch  1059  G loss  0.1369864\n",
      "Epoch  1060  G loss  0.108039774\n",
      "Epoch  1061  G loss  0.056960396\n",
      "Epoch  1062  G loss  0.07635835\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1063  G loss  0.108980246\n",
      "Epoch  1064  G loss  0.089758575\n",
      "Epoch  1065  G loss  0.10808859\n",
      "Epoch  1066  G loss  0.07441295\n",
      "Epoch  1067  G loss  0.43138617\n",
      "Epoch  1068  G loss  0.061149407\n",
      "Epoch  1069  G loss  0.076557815\n",
      "Epoch  1070  G loss  0.10157071\n",
      "Epoch  1071  G loss  0.053854577\n",
      "Epoch  1072  G loss  0.068733945\n",
      "Epoch  1073  G loss  0.05016552\n",
      "Epoch  1074  G loss  0.04544834\n",
      "Epoch  1075  G loss  0.050149698\n",
      "Epoch  1076  G loss  0.266233\n",
      "Epoch  1077  G loss  0.065121196\n",
      "Epoch  1078  G loss  0.06439823\n",
      "Epoch  1079  G loss  0.06374131\n",
      "Epoch  1080  G loss  0.038652606\n",
      "Epoch  1081  G loss  0.114224724\n",
      "Epoch  1082  G loss  0.035403773\n",
      "Epoch  1083  G loss  0.05754718\n",
      "Epoch  1084  G loss  0.092751086\n",
      "Epoch  1085  G loss  0.10311683\n",
      "Epoch  1086  G loss  0.049366936\n",
      "Epoch  1087  G loss  0.14661394\n",
      "Epoch  1088  G loss  0.14224333\n",
      "Epoch  1089  G loss  0.17729266\n",
      "Epoch  1090  G loss  0.08617975\n",
      "Epoch  1091  G loss  0.172162\n",
      "Epoch  1092  G loss  0.12835592\n",
      "Epoch  1093  G loss  0.10488841\n",
      "Epoch  1094  G loss  0.0746482\n",
      "Epoch  1095  G loss  0.06282687\n",
      "Epoch  1096  G loss  0.033277072\n",
      "Epoch  1097  G loss  0.06184155\n",
      "Epoch  1098  G loss  0.27566755\n",
      "Epoch  1099  G loss  0.054207366\n",
      "Epoch  1100  G loss  0.039040014\n",
      "Epoch  1101  G loss  0.024730626\n",
      "Epoch  1102  G loss  0.13767207\n",
      "Epoch  1103  G loss  0.048157893\n",
      "Epoch  1104  G loss  0.015590217\n",
      "Epoch  1105  G loss  0.23208582\n",
      "Epoch  1106  G loss  0.019267656\n",
      "Epoch  1107  G loss  0.11187082\n",
      "Epoch  1108  G loss  0.13925833\n",
      "Epoch  1109  G loss  0.06637926\n",
      "Epoch  1110  G loss  0.069781475\n",
      "Epoch  1111  G loss  0.13370311\n",
      "Epoch  1112  G loss  0.08287005\n",
      "Epoch  1113  G loss  0.22270438\n",
      "Epoch  1114  G loss  0.086958796\n",
      "Epoch  1115  G loss  0.069370896\n",
      "Epoch  1116  G loss  0.042533323\n",
      "Epoch  1117  G loss  0.23083168\n",
      "Epoch  1118  G loss  0.15235552\n",
      "Epoch  1119  G loss  0.046932265\n",
      "Epoch  1120  G loss  0.0554242\n",
      "Epoch  1121  G loss  0.04864898\n",
      "Epoch  1122  G loss  0.056911908\n",
      "Epoch  1123  G loss  0.03861872\n",
      "Epoch  1124  G loss  0.076745115\n",
      "Epoch  1125  G loss  0.04215944\n",
      "Epoch  1126  G loss  0.122672535\n",
      "Epoch  1127  G loss  0.047914937\n",
      "Epoch  1128  G loss  0.041910157\n",
      "Epoch  1129  G loss  0.040220894\n",
      "Epoch  1130  G loss  0.042257912\n",
      "Epoch  1131  G loss  0.052540906\n",
      "Epoch  1132  G loss  0.10799639\n",
      "Epoch  1133  G loss  0.052443534\n",
      "Epoch  1134  G loss  0.08415975\n",
      "Epoch  1135  G loss  0.1566802\n",
      "Epoch  1136  G loss  0.06452155\n",
      "Epoch  1137  G loss  0.040995166\n",
      "Epoch  1138  G loss  0.065527126\n",
      "Epoch  1139  G loss  0.15531439\n",
      "Epoch  1140  G loss  0.043544024\n",
      "Epoch  1141  G loss  0.05505593\n",
      "Epoch  1142  G loss  0.06560319\n",
      "Epoch  1143  G loss  0.06409457\n",
      "Epoch  1144  G loss  0.088508666\n",
      "Epoch  1145  G loss  0.16190475\n",
      "Epoch  1146  G loss  0.22573413\n",
      "Epoch  1147  G loss  0.059318244\n",
      "Epoch  1148  G loss  0.07208457\n",
      "Epoch  1149  G loss  0.20195712\n",
      "Epoch  1150  G loss  0.0602323\n",
      "Epoch  1151  G loss  0.15143666\n",
      "Epoch  1152  G loss  0.17878531\n",
      "Epoch  1153  G loss  0.048126105\n",
      "Epoch  1154  G loss  0.069376364\n",
      "Epoch  1155  G loss  0.08336086\n",
      "Epoch  1156  G loss  0.0435762\n",
      "Epoch  1157  G loss  0.056340925\n",
      "Epoch  1158  G loss  0.1453143\n",
      "Epoch  1159  G loss  0.069232136\n",
      "Epoch  1160  G loss  0.16169614\n",
      "Epoch  1161  G loss  0.0760651\n",
      "Epoch  1162  G loss  0.17466232\n",
      "Epoch  1163  G loss  0.03554701\n",
      "Epoch  1164  G loss  0.05548318\n",
      "Epoch  1165  G loss  0.057342373\n",
      "Epoch  1166  G loss  0.13526618\n",
      "Epoch  1167  G loss  0.048914034\n",
      "Epoch  1168  G loss  0.12394749\n",
      "Epoch  1169  G loss  0.08723609\n",
      "Epoch  1170  G loss  0.13387096\n",
      "Epoch  1171  G loss  0.03634758\n",
      "Epoch  1172  G loss  0.0977723\n",
      "Epoch  1173  G loss  0.25357816\n",
      "Epoch  1174  G loss  0.06588579\n",
      "Epoch  1175  G loss  0.061659828\n",
      "Epoch  1176  G loss  0.06544034\n",
      "Epoch  1177  G loss  0.114291504\n",
      "Epoch  1178  G loss  0.08249694\n",
      "Epoch  1179  G loss  0.06440772\n",
      "Epoch  1180  G loss  0.03691415\n",
      "Epoch  1181  G loss  0.02207087\n",
      "Epoch  1182  G loss  0.07384373\n",
      "Epoch  1183  G loss  0.09567832\n",
      "Epoch  1184  G loss  0.09544925\n",
      "Epoch  1185  G loss  0.27810016\n",
      "Epoch  1186  G loss  0.018951593\n",
      "Epoch  1187  G loss  0.078861244\n",
      "Epoch  1188  G loss  0.09723006\n",
      "Epoch  1189  G loss  0.06848911\n",
      "Epoch  1190  G loss  0.04271736\n",
      "Epoch  1191  G loss  0.514783\n",
      "Epoch  1192  G loss  0.058960143\n",
      "Epoch  1193  G loss  0.053042926\n",
      "Epoch  1194  G loss  0.2445837\n",
      "Epoch  1195  G loss  0.041841112\n",
      "Epoch  1196  G loss  0.092101485\n",
      "Epoch  1197  G loss  0.08398777\n",
      "Epoch  1198  G loss  0.075147465\n",
      "Epoch  1199  G loss  0.07974583\n",
      "Epoch  1200  G loss  0.14649092\n",
      "Epoch  1201  G loss  0.031423174\n",
      "Epoch  1202  G loss  0.1519477\n",
      "Epoch  1203  G loss  0.054859035\n",
      "Epoch  1204  G loss  0.044110004\n",
      "Epoch  1205  G loss  0.101757795\n",
      "Epoch  1206  G loss  0.13520518\n",
      "Epoch  1207  G loss  0.06330508\n",
      "Epoch  1208  G loss  0.38819206\n",
      "Epoch  1209  G loss  0.027770188\n",
      "Epoch  1210  G loss  0.07529252\n",
      "Epoch  1211  G loss  0.06385338\n",
      "Epoch  1212  G loss  0.0943429\n",
      "Epoch  1213  G loss  0.048134938\n",
      "Epoch  1214  G loss  0.035210542\n",
      "Epoch  1215  G loss  0.22604667\n",
      "Epoch  1216  G loss  0.08781156\n",
      "Epoch  1217  G loss  0.295667\n",
      "Epoch  1218  G loss  0.10710655\n",
      "Epoch  1219  G loss  0.05029302\n",
      "Epoch  1220  G loss  0.10106443\n",
      "Epoch  1221  G loss  0.27740517\n",
      "Epoch  1222  G loss  0.027772894\n",
      "Epoch  1223  G loss  0.074090846\n",
      "Epoch  1224  G loss  0.28096098\n",
      "Epoch  1225  G loss  0.028694896\n",
      "Epoch  1226  G loss  0.32235625\n",
      "Epoch  1227  G loss  0.07750882\n",
      "Epoch  1228  G loss  0.031625293\n",
      "Epoch  1229  G loss  0.21458098\n",
      "Epoch  1230  G loss  0.12081017\n",
      "Epoch  1231  G loss  0.27405277\n",
      "Epoch  1232  G loss  0.05973342\n",
      "Epoch  1233  G loss  0.06222121\n",
      "Epoch  1234  G loss  0.045074068\n",
      "Epoch  1235  G loss  0.06048896\n",
      "Epoch  1236  G loss  0.07961139\n",
      "Epoch  1237  G loss  0.095746815\n",
      "Epoch  1238  G loss  0.35247093\n",
      "Epoch  1239  G loss  0.051545136\n",
      "Epoch  1240  G loss  0.05759182\n",
      "Epoch  1241  G loss  0.044805795\n",
      "Epoch  1242  G loss  0.23964213\n",
      "Epoch  1243  G loss  0.043986045\n",
      "Epoch  1244  G loss  0.07661176\n",
      "Epoch  1245  G loss  0.13798371\n",
      "Epoch  1246  G loss  0.095068455\n",
      "Epoch  1247  G loss  0.070082955\n",
      "Epoch  1248  G loss  0.19214788\n",
      "Epoch  1249  G loss  0.0530305\n",
      "Epoch  1250  G loss  0.13535087\n",
      "Epoch  1251  G loss  0.04404804\n",
      "Epoch  1252  G loss  0.02618381\n",
      "Epoch  1253  G loss  0.2706672\n",
      "Epoch  1254  G loss  0.021806296\n",
      "Epoch  1255  G loss  0.047031235\n",
      "Epoch  1256  G loss  0.31239974\n",
      "Epoch  1257  G loss  0.038643807\n",
      "Epoch  1258  G loss  0.12471581\n",
      "Epoch  1259  G loss  0.117954105\n",
      "Epoch  1260  G loss  0.03320516\n",
      "Epoch  1261  G loss  0.027713642\n",
      "Epoch  1262  G loss  0.22298133\n",
      "Epoch  1263  G loss  0.056253947\n",
      "Epoch  1264  G loss  0.06295839\n",
      "Epoch  1265  G loss  0.051907215\n",
      "Epoch  1266  G loss  0.021366715\n",
      "Epoch  1267  G loss  0.070429996\n",
      "Epoch  1268  G loss  0.06672762\n",
      "Epoch  1269  G loss  0.03892998\n",
      "Epoch  1270  G loss  0.19510815\n",
      "Epoch  1271  G loss  0.0401036\n",
      "Epoch  1272  G loss  0.08516051\n",
      "Epoch  1273  G loss  0.08883788\n",
      "Epoch  1274  G loss  0.0611285\n",
      "Epoch  1275  G loss  0.22427526\n",
      "Epoch  1276  G loss  0.052147973\n",
      "Epoch  1277  G loss  0.054666996\n",
      "Epoch  1278  G loss  0.06943054\n",
      "Epoch  1279  G loss  0.032075923\n",
      "Epoch  1280  G loss  0.13351539\n",
      "Epoch  1281  G loss  0.055653483\n",
      "Epoch  1282  G loss  0.06503114\n",
      "Epoch  1283  G loss  0.043329656\n",
      "Epoch  1284  G loss  0.077584915\n",
      "Epoch  1285  G loss  0.12179366\n",
      "Epoch  1286  G loss  0.05127\n",
      "Epoch  1287  G loss  0.011367448\n",
      "Epoch  1288  G loss  0.051484436\n",
      "Epoch  1289  G loss  0.15478563\n",
      "Epoch  1290  G loss  0.10544321\n",
      "Epoch  1291  G loss  0.06152854\n",
      "Epoch  1292  G loss  0.066851035\n",
      "Epoch  1293  G loss  0.039332274\n",
      "Epoch  1294  G loss  0.02125163\n",
      "Epoch  1295  G loss  0.31304026\n",
      "Epoch  1296  G loss  0.038840313\n",
      "Epoch  1297  G loss  0.28403524\n",
      "Epoch  1298  G loss  0.0683213\n",
      "Epoch  1299  G loss  0.032223675\n",
      "Epoch  1300  G loss  0.067925885\n",
      "Epoch  1301  G loss  0.027953072\n",
      "Epoch  1302  G loss  0.03074942\n",
      "Epoch  1303  G loss  0.18350732\n",
      "Epoch  1304  G loss  0.067733444\n",
      "Epoch  1305  G loss  0.045029193\n",
      "Epoch  1306  G loss  0.061442006\n",
      "Epoch  1307  G loss  0.07443574\n",
      "Epoch  1308  G loss  0.036355376\n",
      "Epoch  1309  G loss  0.045752764\n",
      "Epoch  1310  G loss  0.051161624\n",
      "Epoch  1311  G loss  0.23361076\n",
      "Epoch  1312  G loss  0.073869504\n",
      "Epoch  1313  G loss  0.045262616\n",
      "Epoch  1314  G loss  0.14843448\n",
      "Epoch  1315  G loss  0.21742254\n",
      "Epoch  1316  G loss  0.04445689\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1317  G loss  0.055957347\n",
      "Epoch  1318  G loss  0.077911496\n",
      "Epoch  1319  G loss  0.02994386\n",
      "Epoch  1320  G loss  0.25871038\n",
      "Epoch  1321  G loss  0.08221792\n",
      "Epoch  1322  G loss  0.0574808\n",
      "Epoch  1323  G loss  0.03713215\n",
      "Epoch  1324  G loss  0.07846257\n",
      "Epoch  1325  G loss  0.21562001\n",
      "Epoch  1326  G loss  0.06098815\n",
      "Epoch  1327  G loss  0.20022777\n",
      "Epoch  1328  G loss  0.061215896\n",
      "Epoch  1329  G loss  0.10734475\n",
      "Epoch  1330  G loss  0.11003347\n",
      "Epoch  1331  G loss  0.056514867\n",
      "Epoch  1332  G loss  0.28544462\n",
      "Epoch  1333  G loss  0.07684043\n",
      "Epoch  1334  G loss  0.041220255\n",
      "Epoch  1335  G loss  0.08728586\n",
      "Epoch  1336  G loss  0.053325266\n",
      "Epoch  1337  G loss  0.12992147\n",
      "Epoch  1338  G loss  0.055288803\n",
      "Epoch  1339  G loss  0.36858118\n",
      "Epoch  1340  G loss  0.05779269\n",
      "Epoch  1341  G loss  0.040508524\n",
      "Epoch  1342  G loss  0.07921156\n",
      "Epoch  1343  G loss  0.10846042\n",
      "Epoch  1344  G loss  0.0135933375\n",
      "Epoch  1345  G loss  0.03985235\n",
      "Epoch  1346  G loss  0.15467033\n",
      "Epoch  1347  G loss  0.09383262\n",
      "Epoch  1348  G loss  0.09581797\n",
      "Epoch  1349  G loss  0.08320679\n",
      "Epoch  1350  G loss  0.06591892\n",
      "Epoch  1351  G loss  0.09245898\n",
      "Epoch  1352  G loss  0.12539347\n",
      "Epoch  1353  G loss  0.13979025\n",
      "Epoch  1354  G loss  0.1007516\n",
      "Epoch  1355  G loss  0.052251328\n",
      "Epoch  1356  G loss  0.19747198\n",
      "Epoch  1357  G loss  0.0316194\n",
      "Epoch  1358  G loss  0.14130577\n",
      "Epoch  1359  G loss  0.15484707\n",
      "Epoch  1360  G loss  0.23312923\n",
      "Epoch  1361  G loss  0.07786962\n",
      "Epoch  1362  G loss  0.23458531\n",
      "Epoch  1363  G loss  0.052434213\n",
      "Epoch  1364  G loss  0.08643904\n",
      "Epoch  1365  G loss  0.049894683\n",
      "Epoch  1366  G loss  0.063573614\n",
      "Epoch  1367  G loss  0.033636656\n",
      "Epoch  1368  G loss  0.025000371\n",
      "Epoch  1369  G loss  0.047864348\n",
      "Epoch  1370  G loss  0.044312987\n",
      "Epoch  1371  G loss  0.022776242\n",
      "Epoch  1372  G loss  0.09455075\n",
      "Epoch  1373  G loss  0.04504498\n",
      "Epoch  1374  G loss  0.09141398\n",
      "Epoch  1375  G loss  0.24851969\n",
      "Epoch  1376  G loss  0.0640261\n",
      "Epoch  1377  G loss  0.03733026\n",
      "Epoch  1378  G loss  0.06215691\n",
      "Epoch  1379  G loss  0.21840487\n",
      "Epoch  1380  G loss  0.37384766\n",
      "Epoch  1381  G loss  0.14487563\n",
      "Epoch  1382  G loss  0.050230693\n",
      "Epoch  1383  G loss  0.12946145\n",
      "Epoch  1384  G loss  0.13309836\n",
      "Epoch  1385  G loss  0.056126606\n",
      "Epoch  1386  G loss  0.057636026\n",
      "Epoch  1387  G loss  0.13248661\n",
      "Epoch  1388  G loss  0.08094076\n",
      "Epoch  1389  G loss  0.0318599\n",
      "Epoch  1390  G loss  0.20817792\n",
      "Epoch  1391  G loss  0.08806293\n",
      "Epoch  1392  G loss  0.09366632\n",
      "Epoch  1393  G loss  0.060448512\n",
      "Epoch  1394  G loss  0.03125015\n",
      "Epoch  1395  G loss  0.071086705\n",
      "Epoch  1396  G loss  0.08653823\n",
      "Epoch  1397  G loss  0.13901436\n",
      "Epoch  1398  G loss  0.06602028\n",
      "Epoch  1399  G loss  0.027373407\n",
      "Epoch  1400  G loss  0.096144915\n",
      "Epoch  1401  G loss  0.10440244\n",
      "Epoch  1402  G loss  0.15346536\n",
      "Epoch  1403  G loss  0.03519868\n",
      "Epoch  1404  G loss  0.22283843\n",
      "Epoch  1405  G loss  0.10177391\n",
      "Epoch  1406  G loss  0.13690755\n",
      "Epoch  1407  G loss  0.08076833\n",
      "Epoch  1408  G loss  0.075002596\n",
      "Epoch  1409  G loss  0.14067829\n",
      "Epoch  1410  G loss  0.028657578\n",
      "Epoch  1411  G loss  0.06106875\n",
      "Epoch  1412  G loss  0.031176658\n",
      "Epoch  1413  G loss  0.031650215\n",
      "Epoch  1414  G loss  0.1638953\n",
      "Epoch  1415  G loss  0.16705567\n",
      "Epoch  1416  G loss  0.08156478\n",
      "Epoch  1417  G loss  0.09110673\n",
      "Epoch  1418  G loss  0.039116684\n",
      "Epoch  1419  G loss  0.020845812\n",
      "Epoch  1420  G loss  0.07831139\n",
      "Epoch  1421  G loss  0.12633303\n",
      "Epoch  1422  G loss  0.21296783\n",
      "Epoch  1423  G loss  0.05694962\n",
      "Epoch  1424  G loss  0.020316377\n",
      "Epoch  1425  G loss  0.14445019\n",
      "Epoch  1426  G loss  0.081915304\n",
      "Epoch  1427  G loss  0.17017654\n",
      "Epoch  1428  G loss  0.027584441\n",
      "Epoch  1429  G loss  0.03828256\n",
      "Epoch  1430  G loss  0.0756335\n",
      "Epoch  1431  G loss  0.06938108\n",
      "Epoch  1432  G loss  0.040898625\n",
      "Epoch  1433  G loss  0.079394415\n",
      "Epoch  1434  G loss  0.17068215\n",
      "Epoch  1435  G loss  0.04931842\n",
      "Epoch  1436  G loss  0.08646492\n",
      "Epoch  1437  G loss  0.16560733\n",
      "Epoch  1438  G loss  0.12849884\n",
      "Epoch  1439  G loss  0.046338167\n",
      "Epoch  1440  G loss  0.1383598\n",
      "Epoch  1441  G loss  0.10990028\n",
      "Epoch  1442  G loss  0.13633335\n",
      "Epoch  1443  G loss  0.11474438\n",
      "Epoch  1444  G loss  0.051962733\n",
      "Epoch  1445  G loss  0.019126926\n",
      "Epoch  1446  G loss  0.06828019\n",
      "Epoch  1447  G loss  0.036954243\n",
      "Epoch  1448  G loss  0.034692135\n",
      "Epoch  1449  G loss  0.045282267\n",
      "Epoch  1450  G loss  0.13963503\n",
      "Epoch  1451  G loss  0.09164273\n",
      "Epoch  1452  G loss  0.040757142\n",
      "Epoch  1453  G loss  0.094487004\n",
      "Epoch  1454  G loss  0.12183978\n",
      "Epoch  1455  G loss  0.021925561\n",
      "Epoch  1456  G loss  0.03032175\n",
      "Epoch  1457  G loss  0.045552764\n",
      "Epoch  1458  G loss  0.03895718\n",
      "Epoch  1459  G loss  0.39424434\n",
      "Epoch  1460  G loss  0.105078526\n",
      "Epoch  1461  G loss  0.111665845\n",
      "Epoch  1462  G loss  0.04403846\n",
      "Epoch  1463  G loss  0.07115594\n",
      "Epoch  1464  G loss  0.036111027\n",
      "Epoch  1465  G loss  0.025672017\n",
      "Epoch  1466  G loss  0.047823247\n",
      "Epoch  1467  G loss  0.21402101\n",
      "Epoch  1468  G loss  0.09274697\n",
      "Epoch  1469  G loss  0.17110102\n",
      "Epoch  1470  G loss  0.04470767\n",
      "Epoch  1471  G loss  0.07300189\n",
      "Epoch  1472  G loss  0.059581943\n",
      "Epoch  1473  G loss  0.05581376\n",
      "Epoch  1474  G loss  0.04425457\n",
      "Epoch  1475  G loss  0.020502105\n",
      "Epoch  1476  G loss  0.07784118\n",
      "Epoch  1477  G loss  0.06955999\n",
      "Epoch  1478  G loss  0.053834256\n",
      "Epoch  1479  G loss  0.0492159\n",
      "Epoch  1480  G loss  0.044053566\n",
      "Epoch  1481  G loss  0.06136979\n",
      "Epoch  1482  G loss  0.19113962\n",
      "Epoch  1483  G loss  0.026109036\n",
      "Epoch  1484  G loss  0.07662442\n",
      "Epoch  1485  G loss  0.06555022\n",
      "Epoch  1486  G loss  0.08296977\n",
      "Epoch  1487  G loss  0.10686675\n",
      "Epoch  1488  G loss  0.11025625\n",
      "Epoch  1489  G loss  0.04657749\n",
      "Epoch  1490  G loss  0.08956292\n",
      "Epoch  1491  G loss  0.035159014\n",
      "Epoch  1492  G loss  0.022329763\n",
      "Epoch  1493  G loss  0.18260765\n",
      "Epoch  1494  G loss  0.042980492\n",
      "Epoch  1495  G loss  0.029524507\n",
      "Epoch  1496  G loss  0.0351824\n",
      "Epoch  1497  G loss  0.12897822\n",
      "Epoch  1498  G loss  0.54389095\n",
      "Epoch  1499  G loss  0.040625386\n",
      "Epoch  1500  G loss  0.17308226\n",
      "Epoch  1501  G loss  0.17175893\n",
      "Epoch  1502  G loss  0.13059436\n",
      "Epoch  1503  G loss  0.02628294\n",
      "Epoch  1504  G loss  0.07093249\n",
      "Epoch  1505  G loss  0.13237329\n",
      "Epoch  1506  G loss  0.03228655\n",
      "Epoch  1507  G loss  0.053527415\n",
      "Epoch  1508  G loss  0.020651117\n",
      "Epoch  1509  G loss  0.036719777\n",
      "Epoch  1510  G loss  0.08357975\n",
      "Epoch  1511  G loss  0.035169315\n",
      "Epoch  1512  G loss  0.093278214\n",
      "Epoch  1513  G loss  0.07720265\n",
      "Epoch  1514  G loss  0.06911694\n",
      "Epoch  1515  G loss  0.047885682\n",
      "Epoch  1516  G loss  0.09921054\n",
      "Epoch  1517  G loss  0.047923587\n",
      "Epoch  1518  G loss  0.19593573\n",
      "Epoch  1519  G loss  0.03833845\n",
      "Epoch  1520  G loss  0.14723083\n",
      "Epoch  1521  G loss  0.12192921\n",
      "Epoch  1522  G loss  0.06466101\n",
      "Epoch  1523  G loss  0.21514292\n",
      "Epoch  1524  G loss  0.10422045\n",
      "Epoch  1525  G loss  0.014298459\n",
      "Epoch  1526  G loss  0.058554683\n",
      "Epoch  1527  G loss  0.06363696\n",
      "Epoch  1528  G loss  0.015505697\n",
      "Epoch  1529  G loss  0.12720384\n",
      "Epoch  1530  G loss  0.40028605\n",
      "Epoch  1531  G loss  0.09620148\n",
      "Epoch  1532  G loss  0.18863358\n",
      "Epoch  1533  G loss  0.101051435\n",
      "Epoch  1534  G loss  0.59247255\n",
      "Epoch  1535  G loss  0.0653805\n",
      "Epoch  1536  G loss  0.09651652\n",
      "Epoch  1537  G loss  0.40003103\n",
      "Epoch  1538  G loss  0.051116537\n",
      "Epoch  1539  G loss  0.057202697\n",
      "Epoch  1540  G loss  0.032364666\n",
      "Epoch  1541  G loss  0.10539151\n",
      "Epoch  1542  G loss  0.0620068\n",
      "Epoch  1543  G loss  0.025697\n",
      "Epoch  1544  G loss  0.021944888\n",
      "Epoch  1545  G loss  0.028241586\n",
      "Epoch  1546  G loss  0.01830841\n",
      "Epoch  1547  G loss  0.015632346\n",
      "Epoch  1548  G loss  0.0362795\n",
      "Epoch  1549  G loss  0.0643156\n",
      "Epoch  1550  G loss  0.13806194\n",
      "Epoch  1551  G loss  0.16621885\n",
      "Epoch  1552  G loss  0.07633111\n",
      "Epoch  1553  G loss  0.051690016\n",
      "Epoch  1554  G loss  0.070971\n",
      "Epoch  1555  G loss  0.08409852\n",
      "Epoch  1556  G loss  0.02468504\n",
      "Epoch  1557  G loss  0.12367328\n",
      "Epoch  1558  G loss  0.0373357\n",
      "Epoch  1559  G loss  0.033804268\n",
      "Epoch  1560  G loss  0.04000096\n",
      "Epoch  1561  G loss  0.35474455\n",
      "Epoch  1562  G loss  0.10228458\n",
      "Epoch  1563  G loss  0.11894488\n",
      "Epoch  1564  G loss  0.13757354\n",
      "Epoch  1565  G loss  0.036707047\n",
      "Epoch  1566  G loss  0.06217308\n",
      "Epoch  1567  G loss  0.10705161\n",
      "Epoch  1568  G loss  0.043056864\n",
      "Epoch  1569  G loss  0.052770983\n",
      "Epoch  1570  G loss  0.06993046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1571  G loss  0.020445533\n",
      "Epoch  1572  G loss  0.054776214\n",
      "Epoch  1573  G loss  0.10214581\n",
      "Epoch  1574  G loss  0.0155213\n",
      "Epoch  1575  G loss  0.16494262\n",
      "Epoch  1576  G loss  0.052593805\n",
      "Epoch  1577  G loss  0.16091993\n",
      "Epoch  1578  G loss  0.0094514545\n",
      "Epoch  1579  G loss  0.027737467\n",
      "Epoch  1580  G loss  0.13764897\n",
      "Epoch  1581  G loss  0.06763446\n",
      "Epoch  1582  G loss  0.033541165\n",
      "Epoch  1583  G loss  0.098101705\n",
      "Epoch  1584  G loss  0.016943648\n",
      "Epoch  1585  G loss  0.06243215\n",
      "Epoch  1586  G loss  0.047079176\n",
      "Epoch  1587  G loss  0.017366188\n",
      "Epoch  1588  G loss  0.03633311\n",
      "Epoch  1589  G loss  0.07095923\n",
      "Epoch  1590  G loss  0.075864956\n",
      "Epoch  1591  G loss  0.16087806\n",
      "Epoch  1592  G loss  0.028423976\n",
      "Epoch  1593  G loss  0.054476157\n",
      "Epoch  1594  G loss  0.044275288\n",
      "Epoch  1595  G loss  0.05404816\n",
      "Epoch  1596  G loss  0.13709839\n",
      "Epoch  1597  G loss  0.07381442\n",
      "Epoch  1598  G loss  0.04877459\n",
      "Epoch  1599  G loss  0.039073482\n",
      "Epoch  1600  G loss  0.04831491\n",
      "Epoch  1601  G loss  0.022544358\n",
      "Epoch  1602  G loss  0.02881613\n",
      "Epoch  1603  G loss  0.11646612\n",
      "Epoch  1604  G loss  0.17923075\n",
      "Epoch  1605  G loss  0.012968132\n",
      "Epoch  1606  G loss  0.19448833\n",
      "Epoch  1607  G loss  0.03086057\n",
      "Epoch  1608  G loss  0.06289256\n",
      "Epoch  1609  G loss  0.03478758\n",
      "Epoch  1610  G loss  0.045852505\n",
      "Epoch  1611  G loss  0.12803635\n",
      "Epoch  1612  G loss  0.049139794\n",
      "Epoch  1613  G loss  0.028458701\n",
      "Epoch  1614  G loss  0.031990618\n",
      "Epoch  1615  G loss  0.22928593\n",
      "Epoch  1616  G loss  0.03529987\n",
      "Epoch  1617  G loss  0.04799792\n",
      "Epoch  1618  G loss  0.17342296\n",
      "Epoch  1619  G loss  0.12375984\n",
      "Epoch  1620  G loss  0.043541074\n",
      "Epoch  1621  G loss  0.059353568\n",
      "Epoch  1622  G loss  0.11628018\n",
      "Epoch  1623  G loss  0.09196426\n",
      "Epoch  1624  G loss  0.07117073\n",
      "Epoch  1625  G loss  0.023733098\n",
      "Epoch  1626  G loss  0.024465278\n",
      "Epoch  1627  G loss  0.04388126\n",
      "Epoch  1628  G loss  0.14371145\n",
      "Epoch  1629  G loss  0.09194549\n",
      "Epoch  1630  G loss  0.06316045\n",
      "Epoch  1631  G loss  0.04419341\n",
      "Epoch  1632  G loss  0.05375791\n",
      "Epoch  1633  G loss  0.055814102\n",
      "Epoch  1634  G loss  0.17554373\n",
      "Epoch  1635  G loss  0.010122644\n",
      "Epoch  1636  G loss  0.04279667\n",
      "Epoch  1637  G loss  0.03965243\n",
      "Epoch  1638  G loss  0.056775916\n",
      "Epoch  1639  G loss  0.028297098\n",
      "Epoch  1640  G loss  0.03752061\n",
      "Epoch  1641  G loss  0.055479936\n",
      "Epoch  1642  G loss  0.04108588\n",
      "Epoch  1643  G loss  0.014113426\n",
      "Epoch  1644  G loss  0.063769475\n",
      "Epoch  1645  G loss  0.010636946\n",
      "Epoch  1646  G loss  0.20600957\n",
      "Epoch  1647  G loss  0.05876192\n",
      "Epoch  1648  G loss  0.06253732\n",
      "Epoch  1649  G loss  0.045241926\n",
      "Epoch  1650  G loss  0.15640227\n",
      "Epoch  1651  G loss  0.031110004\n",
      "Epoch  1652  G loss  0.06541955\n",
      "Epoch  1653  G loss  0.035223246\n",
      "Epoch  1654  G loss  0.12242286\n",
      "Epoch  1655  G loss  0.036356382\n",
      "Epoch  1656  G loss  0.037399445\n",
      "Epoch  1657  G loss  0.10063932\n",
      "Epoch  1658  G loss  0.026468828\n",
      "Epoch  1659  G loss  0.039805237\n",
      "Epoch  1660  G loss  0.022435073\n",
      "Epoch  1661  G loss  0.09528984\n",
      "Epoch  1662  G loss  0.12959932\n",
      "Epoch  1663  G loss  0.02186425\n",
      "Epoch  1664  G loss  0.08609043\n",
      "Epoch  1665  G loss  0.08182759\n",
      "Epoch  1666  G loss  0.036836945\n",
      "Epoch  1667  G loss  0.021398973\n",
      "Epoch  1668  G loss  0.07836047\n",
      "Epoch  1669  G loss  0.013524332\n",
      "Epoch  1670  G loss  0.03194548\n",
      "Epoch  1671  G loss  0.05402185\n",
      "Epoch  1672  G loss  0.0380833\n",
      "Epoch  1673  G loss  0.056997627\n",
      "Epoch  1674  G loss  0.1257619\n",
      "Epoch  1675  G loss  0.24468887\n",
      "Epoch  1676  G loss  0.118679486\n",
      "Epoch  1677  G loss  0.0853184\n",
      "Epoch  1678  G loss  0.12447963\n",
      "Epoch  1679  G loss  0.017363988\n",
      "Epoch  1680  G loss  0.05917302\n",
      "Epoch  1681  G loss  0.06660548\n",
      "Epoch  1682  G loss  0.12500484\n",
      "Epoch  1683  G loss  0.1507691\n",
      "Epoch  1684  G loss  0.040346455\n",
      "Epoch  1685  G loss  0.04338094\n",
      "Epoch  1686  G loss  0.16665156\n",
      "Epoch  1687  G loss  0.023934767\n",
      "Epoch  1688  G loss  0.045279898\n",
      "Epoch  1689  G loss  0.071936525\n",
      "Epoch  1690  G loss  0.18739757\n",
      "Epoch  1691  G loss  0.08105824\n",
      "Epoch  1692  G loss  0.20173657\n",
      "Epoch  1693  G loss  0.08019575\n",
      "Epoch  1694  G loss  0.017012872\n",
      "Epoch  1695  G loss  0.015671846\n",
      "Epoch  1696  G loss  0.10044016\n",
      "Epoch  1697  G loss  0.046771493\n",
      "Epoch  1698  G loss  0.107695565\n",
      "Epoch  1699  G loss  0.19683847\n",
      "Epoch  1700  G loss  0.14002149\n",
      "Epoch  1701  G loss  0.1338943\n",
      "Epoch  1702  G loss  0.041211087\n",
      "Epoch  1703  G loss  0.07961928\n",
      "Epoch  1704  G loss  0.011190573\n",
      "Epoch  1705  G loss  0.014257595\n",
      "Epoch  1706  G loss  0.017781788\n",
      "Epoch  1707  G loss  0.03135528\n",
      "Epoch  1708  G loss  0.0703536\n",
      "Epoch  1709  G loss  0.1480357\n",
      "Epoch  1710  G loss  0.041010827\n",
      "Epoch  1711  G loss  0.042367764\n",
      "Epoch  1712  G loss  0.08819249\n",
      "Epoch  1713  G loss  0.22729765\n",
      "Epoch  1714  G loss  0.05854799\n",
      "Epoch  1715  G loss  0.12865084\n",
      "Epoch  1716  G loss  0.028153755\n",
      "Epoch  1717  G loss  0.027834436\n",
      "Epoch  1718  G loss  0.05606626\n",
      "Epoch  1719  G loss  0.07834382\n",
      "Epoch  1720  G loss  0.027654517\n",
      "Epoch  1721  G loss  0.15992004\n",
      "Epoch  1722  G loss  0.22478673\n",
      "Epoch  1723  G loss  0.03254047\n",
      "Epoch  1724  G loss  0.07100629\n",
      "Epoch  1725  G loss  0.07311063\n",
      "Epoch  1726  G loss  0.01780219\n",
      "Epoch  1727  G loss  0.7749984\n",
      "Epoch  1728  G loss  0.036333714\n",
      "Epoch  1729  G loss  0.06397654\n",
      "Epoch  1730  G loss  0.047894686\n",
      "Epoch  1731  G loss  0.0147807505\n",
      "Epoch  1732  G loss  0.03835353\n",
      "Epoch  1733  G loss  0.022796543\n",
      "Epoch  1734  G loss  0.043138936\n",
      "Epoch  1735  G loss  0.19216153\n",
      "Epoch  1736  G loss  0.057216056\n",
      "Epoch  1737  G loss  0.024661383\n",
      "Epoch  1738  G loss  0.07367377\n",
      "Epoch  1739  G loss  0.19796503\n",
      "Epoch  1740  G loss  0.042160444\n",
      "Epoch  1741  G loss  0.024186388\n",
      "Epoch  1742  G loss  0.04694716\n",
      "Epoch  1743  G loss  0.16852829\n",
      "Epoch  1744  G loss  0.006513105\n",
      "Epoch  1745  G loss  0.15233284\n",
      "Epoch  1746  G loss  0.03297838\n",
      "Epoch  1747  G loss  0.04125086\n",
      "Epoch  1748  G loss  0.07891172\n",
      "Epoch  1749  G loss  0.18361951\n",
      "Epoch  1750  G loss  0.14879677\n",
      "Epoch  1751  G loss  0.02296006\n",
      "Epoch  1752  G loss  0.054441534\n",
      "Epoch  1753  G loss  0.05937118\n",
      "Epoch  1754  G loss  0.033810455\n",
      "Epoch  1755  G loss  0.037135154\n",
      "Epoch  1756  G loss  0.08474833\n",
      "Epoch  1757  G loss  0.027373783\n",
      "Epoch  1758  G loss  0.028971793\n",
      "Epoch  1759  G loss  0.03495145\n",
      "Epoch  1760  G loss  0.025077589\n",
      "Epoch  1761  G loss  0.02060601\n",
      "Epoch  1762  G loss  0.04988359\n",
      "Epoch  1763  G loss  0.024426553\n",
      "Epoch  1764  G loss  0.1367762\n",
      "Epoch  1765  G loss  0.045009047\n",
      "Epoch  1766  G loss  0.055830434\n",
      "Epoch  1767  G loss  0.054486565\n",
      "Epoch  1768  G loss  0.03837917\n",
      "Epoch  1769  G loss  0.028458882\n",
      "Epoch  1770  G loss  0.073393725\n",
      "Epoch  1771  G loss  0.01845819\n",
      "Epoch  1772  G loss  0.017635852\n",
      "Epoch  1773  G loss  0.027960602\n",
      "Epoch  1774  G loss  0.08323182\n",
      "Epoch  1775  G loss  0.00934853\n",
      "Epoch  1776  G loss  0.05700089\n",
      "Epoch  1777  G loss  0.020008028\n",
      "Epoch  1778  G loss  0.026325494\n",
      "Epoch  1779  G loss  0.037957042\n",
      "Epoch  1780  G loss  0.03711769\n",
      "Epoch  1781  G loss  0.044377733\n",
      "Epoch  1782  G loss  0.078543365\n",
      "Epoch  1783  G loss  0.019019514\n",
      "Epoch  1784  G loss  0.174623\n",
      "Epoch  1785  G loss  0.03428388\n",
      "Epoch  1786  G loss  0.037897184\n",
      "Epoch  1787  G loss  0.053610634\n",
      "Epoch  1788  G loss  0.007112369\n",
      "Epoch  1789  G loss  0.055441525\n",
      "Epoch  1790  G loss  0.019935418\n",
      "Epoch  1791  G loss  0.11463475\n",
      "Epoch  1792  G loss  0.07348824\n",
      "Epoch  1793  G loss  0.6515547\n",
      "Epoch  1794  G loss  0.016729964\n",
      "Epoch  1795  G loss  0.07181366\n",
      "Epoch  1796  G loss  0.028233863\n",
      "Epoch  1797  G loss  0.1737442\n",
      "Epoch  1798  G loss  0.051555417\n",
      "Epoch  1799  G loss  0.028614273\n",
      "Epoch  1800  G loss  0.07312447\n",
      "Epoch  1801  G loss  0.02883961\n",
      "Epoch  1802  G loss  0.090900674\n",
      "Epoch  1803  G loss  0.1683324\n",
      "Epoch  1804  G loss  0.039858565\n",
      "Epoch  1805  G loss  0.064687595\n",
      "Epoch  1806  G loss  0.05419212\n",
      "Epoch  1807  G loss  0.18005109\n",
      "Epoch  1808  G loss  0.031931188\n",
      "Epoch  1809  G loss  0.20740885\n",
      "Epoch  1810  G loss  0.036204606\n",
      "Epoch  1811  G loss  0.064389646\n",
      "Epoch  1812  G loss  0.01227843\n",
      "Epoch  1813  G loss  0.02380115\n",
      "Epoch  1814  G loss  0.04283808\n",
      "Epoch  1815  G loss  0.10170014\n",
      "Epoch  1816  G loss  0.080473006\n",
      "Epoch  1817  G loss  0.014553554\n",
      "Epoch  1818  G loss  0.032964718\n",
      "Epoch  1819  G loss  0.019312363\n",
      "Epoch  1820  G loss  0.042380814\n",
      "Epoch  1821  G loss  0.033203725\n",
      "Epoch  1822  G loss  0.11334487\n",
      "Epoch  1823  G loss  0.033002097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1824  G loss  0.019291919\n",
      "Epoch  1825  G loss  0.12242883\n",
      "Epoch  1826  G loss  0.02544776\n",
      "Epoch  1827  G loss  0.033260636\n",
      "Epoch  1828  G loss  0.010139648\n",
      "Epoch  1829  G loss  0.11293365\n",
      "Epoch  1830  G loss  0.12481624\n",
      "Epoch  1831  G loss  0.025288517\n",
      "Epoch  1832  G loss  0.08549103\n",
      "Epoch  1833  G loss  0.07803244\n",
      "Epoch  1834  G loss  0.2704581\n",
      "Epoch  1835  G loss  0.041490607\n",
      "Epoch  1836  G loss  0.029717598\n",
      "Epoch  1837  G loss  0.038169347\n",
      "Epoch  1838  G loss  0.03994465\n",
      "Epoch  1839  G loss  0.028618123\n",
      "Epoch  1840  G loss  0.073201075\n",
      "Epoch  1841  G loss  0.015240958\n",
      "Epoch  1842  G loss  0.053296626\n",
      "Epoch  1843  G loss  0.012363253\n",
      "Epoch  1844  G loss  0.024515424\n",
      "Epoch  1845  G loss  0.019820945\n",
      "Epoch  1846  G loss  0.22010371\n",
      "Epoch  1847  G loss  0.026077218\n",
      "Epoch  1848  G loss  0.03281954\n",
      "Epoch  1849  G loss  0.0718202\n",
      "Epoch  1850  G loss  0.011779018\n",
      "Epoch  1851  G loss  0.06406883\n",
      "Epoch  1852  G loss  0.03361877\n",
      "Epoch  1853  G loss  0.051712498\n",
      "Epoch  1854  G loss  0.058271185\n",
      "Epoch  1855  G loss  0.089800365\n",
      "Epoch  1856  G loss  0.010014619\n",
      "Epoch  1857  G loss  0.0445792\n",
      "Epoch  1858  G loss  0.116659015\n",
      "Epoch  1859  G loss  0.0140332375\n",
      "Epoch  1860  G loss  0.042303562\n",
      "Epoch  1861  G loss  0.027273696\n",
      "Epoch  1862  G loss  0.036288273\n",
      "Epoch  1863  G loss  0.2257925\n",
      "Epoch  1864  G loss  0.061266467\n",
      "Epoch  1865  G loss  0.04029607\n",
      "Epoch  1866  G loss  0.01374119\n",
      "Epoch  1867  G loss  0.046875976\n",
      "Epoch  1868  G loss  0.019943915\n",
      "Epoch  1869  G loss  0.024296517\n",
      "Epoch  1870  G loss  0.10392381\n",
      "Epoch  1871  G loss  0.05265908\n",
      "Epoch  1872  G loss  0.15766892\n",
      "Epoch  1873  G loss  0.020044172\n",
      "Epoch  1874  G loss  0.072133645\n",
      "Epoch  1875  G loss  0.016595012\n",
      "Epoch  1876  G loss  0.13825756\n",
      "Epoch  1877  G loss  0.033575512\n",
      "Epoch  1878  G loss  0.065194264\n",
      "Epoch  1879  G loss  0.09337729\n",
      "Epoch  1880  G loss  0.00975536\n",
      "Epoch  1881  G loss  0.07762301\n",
      "Epoch  1882  G loss  0.080554545\n",
      "Epoch  1883  G loss  0.45296472\n",
      "Epoch  1884  G loss  0.014783345\n",
      "Epoch  1885  G loss  0.07286669\n",
      "Epoch  1886  G loss  0.06796847\n",
      "Epoch  1887  G loss  0.13519135\n",
      "Epoch  1888  G loss  0.038118724\n",
      "Epoch  1889  G loss  0.026738172\n",
      "Epoch  1890  G loss  0.018677041\n",
      "Epoch  1891  G loss  0.06523231\n",
      "Epoch  1892  G loss  0.033184215\n",
      "Epoch  1893  G loss  0.25449237\n",
      "Epoch  1894  G loss  0.12247569\n",
      "Epoch  1895  G loss  0.020585649\n",
      "Epoch  1896  G loss  0.12445441\n",
      "Epoch  1897  G loss  0.042031527\n",
      "Epoch  1898  G loss  0.028221102\n",
      "Epoch  1899  G loss  0.17882761\n",
      "Epoch  1900  G loss  0.0958517\n",
      "Epoch  1901  G loss  0.13838169\n",
      "Epoch  1902  G loss  0.13374084\n",
      "Epoch  1903  G loss  0.027274467\n",
      "Epoch  1904  G loss  0.06321667\n",
      "Epoch  1905  G loss  0.028020103\n",
      "Epoch  1906  G loss  0.009962503\n",
      "Epoch  1907  G loss  0.049504958\n",
      "Epoch  1908  G loss  0.13491544\n",
      "Epoch  1909  G loss  0.02099042\n",
      "Epoch  1910  G loss  0.031059979\n",
      "Epoch  1911  G loss  0.03436882\n",
      "Epoch  1912  G loss  0.5807927\n",
      "Epoch  1913  G loss  0.021329757\n",
      "Epoch  1914  G loss  0.114715815\n",
      "Epoch  1915  G loss  0.027146496\n",
      "Epoch  1916  G loss  0.03200474\n",
      "Epoch  1917  G loss  0.06922045\n",
      "Epoch  1918  G loss  0.18657666\n",
      "Epoch  1919  G loss  0.07218089\n",
      "Epoch  1920  G loss  0.015268467\n",
      "Epoch  1921  G loss  0.123880185\n",
      "Epoch  1922  G loss  0.053084828\n",
      "Epoch  1923  G loss  0.1818426\n",
      "Epoch  1924  G loss  0.055099435\n",
      "Epoch  1925  G loss  0.060621124\n",
      "Epoch  1926  G loss  0.057841115\n",
      "Epoch  1927  G loss  0.021095743\n",
      "Epoch  1928  G loss  0.009764726\n",
      "Epoch  1929  G loss  0.041197382\n",
      "Epoch  1930  G loss  0.042875648\n",
      "Epoch  1931  G loss  0.027519524\n",
      "Epoch  1932  G loss  0.084298655\n",
      "Epoch  1933  G loss  0.020363608\n",
      "Epoch  1934  G loss  0.13223247\n",
      "Epoch  1935  G loss  0.03244545\n",
      "Epoch  1936  G loss  0.021178577\n",
      "Epoch  1937  G loss  0.04926582\n",
      "Epoch  1938  G loss  0.109534785\n",
      "Epoch  1939  G loss  0.024663072\n",
      "Epoch  1940  G loss  0.011809283\n",
      "Epoch  1941  G loss  0.022042245\n",
      "Epoch  1942  G loss  0.0778615\n",
      "Epoch  1943  G loss  0.22729361\n",
      "Epoch  1944  G loss  0.022625234\n",
      "Epoch  1945  G loss  0.056977164\n",
      "Epoch  1946  G loss  0.24188964\n",
      "Epoch  1947  G loss  0.029929584\n",
      "Epoch  1948  G loss  0.020208327\n",
      "Epoch  1949  G loss  0.060986657\n",
      "Epoch  1950  G loss  0.03280682\n",
      "Epoch  1951  G loss  0.044431217\n",
      "Epoch  1952  G loss  0.030254891\n",
      "Epoch  1953  G loss  0.0442701\n",
      "Epoch  1954  G loss  0.02813378\n",
      "Epoch  1955  G loss  0.03243438\n",
      "Epoch  1956  G loss  0.014329814\n",
      "Epoch  1957  G loss  0.012256242\n",
      "Epoch  1958  G loss  0.058351874\n",
      "Epoch  1959  G loss  0.034140795\n",
      "Epoch  1960  G loss  0.04859021\n",
      "Epoch  1961  G loss  0.04248906\n",
      "Epoch  1962  G loss  0.06864817\n",
      "Epoch  1963  G loss  0.06308268\n",
      "Epoch  1964  G loss  0.06518626\n",
      "Epoch  1965  G loss  0.035383247\n",
      "Epoch  1966  G loss  0.018709756\n",
      "Epoch  1967  G loss  0.028502177\n",
      "Epoch  1968  G loss  0.05794889\n",
      "Epoch  1969  G loss  0.038762517\n",
      "Epoch  1970  G loss  0.03612037\n",
      "Epoch  1971  G loss  0.03836128\n",
      "Epoch  1972  G loss  0.025384372\n",
      "Epoch  1973  G loss  0.14818047\n",
      "Epoch  1974  G loss  0.029276932\n",
      "Epoch  1975  G loss  0.008419511\n",
      "Epoch  1976  G loss  0.13827458\n",
      "Epoch  1977  G loss  0.0423641\n",
      "Epoch  1978  G loss  0.11596558\n",
      "Epoch  1979  G loss  0.12618273\n",
      "Epoch  1980  G loss  0.028021306\n",
      "Epoch  1981  G loss  0.012557319\n",
      "Epoch  1982  G loss  0.06741943\n",
      "Epoch  1983  G loss  0.014371678\n",
      "Epoch  1984  G loss  0.07196612\n",
      "Epoch  1985  G loss  0.029082574\n",
      "Epoch  1986  G loss  0.028243273\n",
      "Epoch  1987  G loss  0.01613391\n",
      "Epoch  1988  G loss  0.19812049\n",
      "Epoch  1989  G loss  0.20638712\n",
      "Epoch  1990  G loss  0.048677243\n",
      "Epoch  1991  G loss  0.025854614\n",
      "Epoch  1992  G loss  0.005436342\n",
      "Epoch  1993  G loss  0.05335853\n",
      "Epoch  1994  G loss  0.06971203\n",
      "Epoch  1995  G loss  0.019852098\n",
      "Epoch  1996  G loss  0.061000183\n",
      "Epoch  1997  G loss  0.02266378\n",
      "Epoch  1998  G loss  0.041833\n",
      "Epoch  1999  G loss  0.022923434\n",
      "Epoch  2000  G loss  0.08769077\n",
      "Epoch  2001  G loss  0.062607184\n",
      "Epoch  2002  G loss  0.21564928\n",
      "Epoch  2003  G loss  0.15123317\n",
      "Epoch  2004  G loss  0.2035129\n",
      "Epoch  2005  G loss  0.1200724\n",
      "Epoch  2006  G loss  0.031151589\n",
      "Epoch  2007  G loss  0.0319658\n",
      "Epoch  2008  G loss  0.06456663\n",
      "Epoch  2009  G loss  0.046310753\n",
      "Epoch  2010  G loss  0.037063405\n",
      "Epoch  2011  G loss  0.00835581\n",
      "Epoch  2012  G loss  0.037422605\n",
      "Epoch  2013  G loss  0.046489023\n",
      "Epoch  2014  G loss  0.009769736\n",
      "Epoch  2015  G loss  0.020025283\n",
      "Epoch  2016  G loss  0.034571003\n",
      "Epoch  2017  G loss  0.026076457\n",
      "Epoch  2018  G loss  0.041735187\n",
      "Epoch  2019  G loss  0.14712584\n",
      "Epoch  2020  G loss  0.026779149\n",
      "Epoch  2021  G loss  0.028961739\n",
      "Epoch  2022  G loss  0.05127288\n",
      "Epoch  2023  G loss  0.07208465\n",
      "Epoch  2024  G loss  0.2309971\n",
      "Epoch  2025  G loss  0.033992708\n",
      "Epoch  2026  G loss  0.1335144\n",
      "Epoch  2027  G loss  0.023834243\n",
      "Epoch  2028  G loss  0.016531613\n",
      "Epoch  2029  G loss  0.017948654\n",
      "Epoch  2030  G loss  0.034559146\n",
      "Epoch  2031  G loss  0.015612908\n",
      "Epoch  2032  G loss  0.009329225\n",
      "Epoch  2033  G loss  0.052189395\n",
      "Epoch  2034  G loss  0.04619287\n",
      "Epoch  2035  G loss  0.041094586\n",
      "Epoch  2036  G loss  0.007889947\n",
      "Epoch  2037  G loss  0.055087663\n",
      "Epoch  2038  G loss  0.011068914\n",
      "Epoch  2039  G loss  0.06843695\n",
      "Epoch  2040  G loss  0.060507774\n",
      "Epoch  2041  G loss  0.05659335\n",
      "Epoch  2042  G loss  0.018999122\n",
      "Epoch  2043  G loss  0.062172864\n",
      "Epoch  2044  G loss  0.11673313\n",
      "Epoch  2045  G loss  0.02849099\n",
      "Epoch  2046  G loss  0.13207835\n",
      "Epoch  2047  G loss  0.039268278\n",
      "Epoch  2048  G loss  0.022882452\n",
      "Epoch  2049  G loss  0.08943668\n",
      "Epoch  2050  G loss  0.0625093\n",
      "Epoch  2051  G loss  0.05206247\n",
      "Epoch  2052  G loss  0.022028387\n",
      "Epoch  2053  G loss  0.021244453\n",
      "Epoch  2054  G loss  0.19027327\n",
      "Epoch  2055  G loss  0.20541239\n",
      "Epoch  2056  G loss  0.12988979\n",
      "Epoch  2057  G loss  0.03236346\n",
      "Epoch  2058  G loss  0.034837417\n",
      "Epoch  2059  G loss  0.06271247\n",
      "Epoch  2060  G loss  0.050139524\n",
      "Epoch  2061  G loss  0.03504101\n",
      "Epoch  2062  G loss  0.02528501\n",
      "Epoch  2063  G loss  0.040637385\n",
      "Epoch  2064  G loss  0.037083935\n",
      "Epoch  2065  G loss  0.022910044\n",
      "Epoch  2066  G loss  0.019006541\n",
      "Epoch  2067  G loss  0.16748594\n",
      "Epoch  2068  G loss  0.028183006\n",
      "Epoch  2069  G loss  0.07522263\n",
      "Epoch  2070  G loss  0.008441137\n",
      "Epoch  2071  G loss  0.13053918\n",
      "Epoch  2072  G loss  0.0116461795\n",
      "Epoch  2073  G loss  0.024183918\n",
      "Epoch  2074  G loss  0.058360826\n",
      "Epoch  2075  G loss  0.080004066\n",
      "Epoch  2076  G loss  0.10354048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  2077  G loss  0.018992586\n",
      "Epoch  2078  G loss  0.105931014\n",
      "Epoch  2079  G loss  0.043333698\n",
      "Epoch  2080  G loss  0.03006854\n",
      "Epoch  2081  G loss  0.046302594\n",
      "Epoch  2082  G loss  0.03993486\n",
      "Epoch  2083  G loss  0.110233456\n",
      "Epoch  2084  G loss  0.01717273\n",
      "Epoch  2085  G loss  0.096373044\n",
      "Epoch  2086  G loss  0.034791887\n",
      "Epoch  2087  G loss  0.12727605\n",
      "Epoch  2088  G loss  0.011652069\n",
      "Epoch  2089  G loss  0.027549896\n",
      "Epoch  2090  G loss  0.011380342\n",
      "Epoch  2091  G loss  0.0319606\n",
      "Epoch  2092  G loss  0.019797023\n",
      "Epoch  2093  G loss  0.029986646\n",
      "Epoch  2094  G loss  0.11462358\n",
      "Epoch  2095  G loss  0.012723636\n",
      "Epoch  2096  G loss  0.017398551\n",
      "Epoch  2097  G loss  0.061729636\n",
      "Epoch  2098  G loss  0.032189764\n",
      "Epoch  2099  G loss  0.026225204\n",
      "Epoch  2100  G loss  0.06507139\n",
      "Epoch  2101  G loss  0.13008888\n",
      "Epoch  2102  G loss  0.1195519\n",
      "Epoch  2103  G loss  0.30268627\n",
      "Epoch  2104  G loss  0.034282826\n",
      "Epoch  2105  G loss  0.04855928\n",
      "Epoch  2106  G loss  0.016949937\n",
      "Epoch  2107  G loss  0.13038489\n",
      "Epoch  2108  G loss  0.030543307\n",
      "Epoch  2109  G loss  0.02840459\n",
      "Epoch  2110  G loss  0.0622845\n",
      "Epoch  2111  G loss  0.020903077\n",
      "Epoch  2112  G loss  0.058219254\n",
      "Epoch  2113  G loss  0.017767064\n",
      "Epoch  2114  G loss  0.11724701\n",
      "Epoch  2115  G loss  0.2275471\n",
      "Epoch  2116  G loss  0.00903132\n",
      "Epoch  2117  G loss  0.11400536\n",
      "Epoch  2118  G loss  0.064356804\n",
      "Epoch  2119  G loss  0.054238297\n",
      "Epoch  2120  G loss  0.07687768\n",
      "Epoch  2121  G loss  0.048923302\n",
      "Epoch  2122  G loss  0.112878665\n",
      "Epoch  2123  G loss  0.013165772\n",
      "Epoch  2124  G loss  0.18242575\n",
      "Epoch  2125  G loss  0.01899542\n",
      "Epoch  2126  G loss  0.05166519\n",
      "Epoch  2127  G loss  0.013416104\n",
      "Epoch  2128  G loss  0.02442746\n",
      "Epoch  2129  G loss  0.06922841\n",
      "Epoch  2130  G loss  0.021608595\n",
      "Epoch  2131  G loss  0.03154245\n",
      "Epoch  2132  G loss  0.12617563\n",
      "Epoch  2133  G loss  0.142604\n",
      "Epoch  2134  G loss  0.041930106\n",
      "Epoch  2135  G loss  0.063944414\n",
      "Epoch  2136  G loss  0.016146045\n",
      "Epoch  2137  G loss  0.061364\n",
      "Epoch  2138  G loss  0.031436194\n",
      "Epoch  2139  G loss  0.07571871\n",
      "Epoch  2140  G loss  0.07217899\n",
      "Epoch  2141  G loss  0.021853622\n",
      "Epoch  2142  G loss  0.0212861\n",
      "Epoch  2143  G loss  0.052860856\n",
      "Epoch  2144  G loss  0.019754473\n",
      "Epoch  2145  G loss  0.040269036\n",
      "Epoch  2146  G loss  0.01642499\n",
      "Epoch  2147  G loss  0.041771993\n",
      "Epoch  2148  G loss  0.024184933\n",
      "Epoch  2149  G loss  0.012652554\n",
      "Epoch  2150  G loss  0.04359161\n",
      "Epoch  2151  G loss  0.044788446\n",
      "Epoch  2152  G loss  0.06838827\n",
      "Epoch  2153  G loss  0.028526718\n",
      "Epoch  2154  G loss  0.1391992\n",
      "Epoch  2155  G loss  0.02135836\n",
      "Epoch  2156  G loss  0.12881504\n",
      "Epoch  2157  G loss  0.051616803\n",
      "Epoch  2158  G loss  0.10241481\n",
      "Epoch  2159  G loss  0.012145588\n",
      "Epoch  2160  G loss  0.15023772\n",
      "Epoch  2161  G loss  0.040326837\n",
      "Epoch  2162  G loss  0.049000397\n",
      "Epoch  2163  G loss  0.23397641\n",
      "Epoch  2164  G loss  0.06888111\n",
      "Epoch  2165  G loss  0.08820169\n",
      "Epoch  2166  G loss  0.062507175\n",
      "Epoch  2167  G loss  0.019653466\n",
      "Epoch  2168  G loss  0.05537451\n",
      "Epoch  2169  G loss  0.017247047\n",
      "Epoch  2170  G loss  0.02269805\n",
      "Epoch  2171  G loss  0.18241182\n",
      "Epoch  2172  G loss  0.028537007\n",
      "Epoch  2173  G loss  0.07282376\n",
      "Epoch  2174  G loss  0.20373748\n",
      "Epoch  2175  G loss  0.148317\n",
      "Epoch  2176  G loss  0.01647768\n",
      "Epoch  2177  G loss  0.02314717\n",
      "Epoch  2178  G loss  0.022051768\n",
      "Epoch  2179  G loss  0.025481464\n",
      "Epoch  2180  G loss  0.06755159\n",
      "Epoch  2181  G loss  0.10040633\n",
      "Epoch  2182  G loss  0.05654709\n",
      "Epoch  2183  G loss  0.010898038\n",
      "Epoch  2184  G loss  0.039056174\n",
      "Epoch  2185  G loss  0.045335874\n",
      "Epoch  2186  G loss  0.014675403\n",
      "Epoch  2187  G loss  0.042477053\n",
      "Epoch  2188  G loss  0.059881356\n",
      "Epoch  2189  G loss  0.030780027\n",
      "Epoch  2190  G loss  0.03796845\n",
      "Epoch  2191  G loss  0.017749805\n",
      "Epoch  2192  G loss  0.036599964\n",
      "Epoch  2193  G loss  0.019404408\n",
      "Epoch  2194  G loss  0.05237473\n",
      "Epoch  2195  G loss  0.013942193\n",
      "Epoch  2196  G loss  0.020625968\n",
      "Epoch  2197  G loss  0.026487166\n",
      "Epoch  2198  G loss  0.06233264\n",
      "Epoch  2199  G loss  0.04948342\n",
      "Epoch  2200  G loss  0.1193828\n",
      "Epoch  2201  G loss  0.022619776\n",
      "Epoch  2202  G loss  0.012237538\n",
      "Epoch  2203  G loss  0.052134514\n",
      "Epoch  2204  G loss  0.18692158\n",
      "Epoch  2205  G loss  0.01965216\n",
      "Epoch  2206  G loss  0.044448078\n",
      "Epoch  2207  G loss  0.028387181\n",
      "Epoch  2208  G loss  0.020218153\n",
      "Epoch  2209  G loss  0.028061947\n",
      "Epoch  2210  G loss  0.030917514\n",
      "Epoch  2211  G loss  0.03942678\n",
      "Epoch  2212  G loss  0.25714615\n",
      "Epoch  2213  G loss  0.03246092\n",
      "Epoch  2214  G loss  0.051018514\n",
      "Epoch  2215  G loss  0.08963835\n",
      "Epoch  2216  G loss  0.03842845\n",
      "Epoch  2217  G loss  0.021374758\n",
      "Epoch  2218  G loss  0.099281535\n",
      "Epoch  2219  G loss  0.02504526\n",
      "Epoch  2220  G loss  0.008864147\n",
      "Epoch  2221  G loss  0.05516082\n",
      "Epoch  2222  G loss  0.011189134\n",
      "Epoch  2223  G loss  0.015677566\n",
      "Epoch  2224  G loss  0.14542177\n",
      "Epoch  2225  G loss  0.051799145\n",
      "Epoch  2226  G loss  0.04494503\n",
      "Epoch  2227  G loss  0.14280573\n",
      "Epoch  2228  G loss  0.01584356\n",
      "Epoch  2229  G loss  0.02981516\n",
      "Epoch  2230  G loss  0.08811551\n",
      "Epoch  2231  G loss  0.009366391\n",
      "Epoch  2232  G loss  0.036573075\n",
      "Epoch  2233  G loss  0.019550499\n",
      "Epoch  2234  G loss  0.3043797\n",
      "Epoch  2235  G loss  0.026315158\n",
      "Epoch  2236  G loss  0.023346676\n",
      "Epoch  2237  G loss  0.023881596\n",
      "Epoch  2238  G loss  0.03222609\n",
      "Epoch  2239  G loss  0.061370246\n",
      "Epoch  2240  G loss  0.019456856\n",
      "Epoch  2241  G loss  0.15604721\n",
      "Epoch  2242  G loss  0.029385135\n",
      "Epoch  2243  G loss  0.06922791\n",
      "Epoch  2244  G loss  0.014961709\n",
      "Epoch  2245  G loss  0.034224674\n",
      "Epoch  2246  G loss  0.07960691\n",
      "Epoch  2247  G loss  0.046555832\n",
      "Epoch  2248  G loss  0.008710698\n",
      "Epoch  2249  G loss  0.04828822\n",
      "Epoch  2250  G loss  0.082939975\n",
      "Epoch  2251  G loss  0.03523103\n",
      "Epoch  2252  G loss  0.0091953585\n",
      "Epoch  2253  G loss  0.00718443\n",
      "Epoch  2254  G loss  0.0132006975\n",
      "Epoch  2255  G loss  0.06132085\n",
      "Epoch  2256  G loss  0.21486072\n",
      "Epoch  2257  G loss  0.021141464\n",
      "Epoch  2258  G loss  0.011421576\n",
      "Epoch  2259  G loss  0.025180947\n",
      "Epoch  2260  G loss  0.011336548\n",
      "Epoch  2261  G loss  0.011041436\n",
      "Epoch  2262  G loss  0.036476728\n",
      "Epoch  2263  G loss  0.010649659\n",
      "Epoch  2264  G loss  0.17304128\n",
      "Epoch  2265  G loss  0.040265583\n",
      "Epoch  2266  G loss  0.17984274\n",
      "Epoch  2267  G loss  0.16695432\n",
      "Epoch  2268  G loss  0.057043504\n",
      "Epoch  2269  G loss  0.10942987\n",
      "Epoch  2270  G loss  0.012269735\n",
      "Epoch  2271  G loss  0.09153282\n",
      "Epoch  2272  G loss  0.017241515\n",
      "Epoch  2273  G loss  0.01483796\n",
      "Epoch  2274  G loss  0.11554925\n",
      "Epoch  2275  G loss  0.041663766\n",
      "Epoch  2276  G loss  0.20666823\n",
      "Epoch  2277  G loss  0.033292875\n",
      "Epoch  2278  G loss  0.06270624\n",
      "Epoch  2279  G loss  0.010148238\n",
      "Epoch  2280  G loss  0.018129531\n",
      "Epoch  2281  G loss  0.034466553\n",
      "Epoch  2282  G loss  0.060443215\n",
      "Epoch  2283  G loss  0.104097396\n",
      "Epoch  2284  G loss  0.03325013\n",
      "Epoch  2285  G loss  0.16768391\n",
      "Epoch  2286  G loss  0.20440176\n",
      "Epoch  2287  G loss  0.02616066\n",
      "Epoch  2288  G loss  0.052192077\n",
      "Epoch  2289  G loss  0.014403736\n",
      "Epoch  2290  G loss  0.048191868\n",
      "Epoch  2291  G loss  0.028768875\n",
      "Epoch  2292  G loss  0.024552148\n",
      "Epoch  2293  G loss  0.09863897\n",
      "Epoch  2294  G loss  0.07627402\n",
      "Epoch  2295  G loss  0.04440752\n",
      "Epoch  2296  G loss  0.02344591\n",
      "Epoch  2297  G loss  0.019238455\n",
      "Epoch  2298  G loss  0.25832582\n",
      "Epoch  2299  G loss  0.08736311\n",
      "Epoch  2300  G loss  0.13843763\n",
      "Epoch  2301  G loss  0.11837961\n",
      "Epoch  2302  G loss  0.046329297\n",
      "Epoch  2303  G loss  0.037349846\n",
      "Epoch  2304  G loss  0.014808396\n",
      "Epoch  2305  G loss  0.036133364\n",
      "Epoch  2306  G loss  0.020996531\n",
      "Epoch  2307  G loss  0.1795339\n",
      "Epoch  2308  G loss  0.039553285\n",
      "Epoch  2309  G loss  0.0199253\n",
      "Epoch  2310  G loss  0.10247262\n",
      "Epoch  2311  G loss  0.12282984\n",
      "Epoch  2312  G loss  0.045474842\n",
      "Epoch  2313  G loss  0.02605997\n",
      "Epoch  2314  G loss  0.59326303\n",
      "Epoch  2315  G loss  0.09808408\n",
      "Epoch  2316  G loss  0.014349498\n",
      "Epoch  2317  G loss  0.032250155\n",
      "Epoch  2318  G loss  0.10886434\n",
      "Epoch  2319  G loss  0.010320765\n",
      "Epoch  2320  G loss  0.08228884\n",
      "Epoch  2321  G loss  0.08568049\n",
      "Epoch  2322  G loss  0.29739776\n",
      "Epoch  2323  G loss  0.059879676\n",
      "Epoch  2324  G loss  0.0243128\n",
      "Epoch  2325  G loss  0.010077238\n",
      "Epoch  2326  G loss  0.27232727\n",
      "Epoch  2327  G loss  0.024123408\n",
      "Epoch  2328  G loss  0.058176458\n",
      "Epoch  2329  G loss  0.057949044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  2330  G loss  0.28379035\n",
      "Epoch  2331  G loss  0.052925102\n",
      "Epoch  2332  G loss  0.04458444\n",
      "Epoch  2333  G loss  0.15877306\n",
      "Epoch  2334  G loss  0.029512718\n",
      "Epoch  2335  G loss  0.20744376\n",
      "Epoch  2336  G loss  0.15596668\n",
      "Epoch  2337  G loss  0.017643731\n",
      "Epoch  2338  G loss  0.051394746\n",
      "Epoch  2339  G loss  0.020294236\n",
      "Epoch  2340  G loss  0.013556439\n",
      "Epoch  2341  G loss  0.3600781\n",
      "Epoch  2342  G loss  0.010400652\n",
      "Epoch  2343  G loss  0.03284154\n",
      "Epoch  2344  G loss  0.030547991\n",
      "Epoch  2345  G loss  0.07677761\n",
      "Epoch  2346  G loss  0.060980618\n",
      "Epoch  2347  G loss  0.026471261\n",
      "Epoch  2348  G loss  0.05351112\n",
      "Epoch  2349  G loss  0.06680837\n",
      "Epoch  2350  G loss  0.026450114\n",
      "Epoch  2351  G loss  0.1538947\n",
      "Epoch  2352  G loss  0.02957532\n",
      "Epoch  2353  G loss  0.013003257\n",
      "Epoch  2354  G loss  0.02787532\n",
      "Epoch  2355  G loss  0.08953291\n",
      "Epoch  2356  G loss  0.088537596\n",
      "Epoch  2357  G loss  0.016618865\n",
      "Epoch  2358  G loss  0.011911027\n",
      "Epoch  2359  G loss  0.037137136\n",
      "Epoch  2360  G loss  0.0435734\n",
      "Epoch  2361  G loss  0.10348154\n",
      "Epoch  2362  G loss  0.09865156\n",
      "Epoch  2363  G loss  0.01129525\n",
      "Epoch  2364  G loss  0.009663695\n",
      "Epoch  2365  G loss  0.007946634\n",
      "Epoch  2366  G loss  0.0106621375\n",
      "Epoch  2367  G loss  0.007927796\n",
      "Epoch  2368  G loss  0.021934234\n",
      "Epoch  2369  G loss  0.12102747\n",
      "Epoch  2370  G loss  0.07204555\n",
      "Epoch  2371  G loss  0.0056671933\n",
      "Epoch  2372  G loss  0.0388415\n",
      "Epoch  2373  G loss  0.09166147\n",
      "Epoch  2374  G loss  0.026136653\n",
      "Epoch  2375  G loss  0.011444027\n",
      "Epoch  2376  G loss  0.055665135\n",
      "Epoch  2377  G loss  0.2117098\n",
      "Epoch  2378  G loss  0.0075934753\n",
      "Epoch  2379  G loss  0.024580175\n",
      "Epoch  2380  G loss  0.012102877\n",
      "Epoch  2381  G loss  0.25490803\n",
      "Epoch  2382  G loss  0.063222945\n",
      "Epoch  2383  G loss  0.01697445\n",
      "Epoch  2384  G loss  0.06917561\n",
      "Epoch  2385  G loss  0.05350107\n",
      "Epoch  2386  G loss  0.060459696\n",
      "Epoch  2387  G loss  0.078070454\n",
      "Epoch  2388  G loss  0.20732439\n",
      "Epoch  2389  G loss  0.06537763\n",
      "Epoch  2390  G loss  0.009330954\n",
      "Epoch  2391  G loss  0.033811606\n",
      "Epoch  2392  G loss  0.03821124\n",
      "Epoch  2393  G loss  0.0623046\n",
      "Epoch  2394  G loss  0.07933991\n",
      "Epoch  2395  G loss  0.02641228\n",
      "Epoch  2396  G loss  0.033558175\n",
      "Epoch  2397  G loss  0.030545477\n",
      "Epoch  2398  G loss  0.028289065\n",
      "Epoch  2399  G loss  0.040699676\n",
      "Epoch  2400  G loss  0.021973781\n",
      "Epoch  2401  G loss  0.020295963\n",
      "Epoch  2402  G loss  0.025407277\n",
      "Epoch  2403  G loss  0.096809044\n",
      "Epoch  2404  G loss  0.1291252\n",
      "Epoch  2405  G loss  0.015095649\n",
      "Epoch  2406  G loss  0.012859158\n",
      "Epoch  2407  G loss  0.058130868\n",
      "Epoch  2408  G loss  0.034053653\n",
      "Epoch  2409  G loss  0.019421823\n",
      "Epoch  2410  G loss  0.004384432\n",
      "Epoch  2411  G loss  0.018909127\n",
      "Epoch  2412  G loss  0.010270449\n",
      "Epoch  2413  G loss  0.075249664\n",
      "Epoch  2414  G loss  0.019037044\n",
      "Epoch  2415  G loss  0.013179105\n",
      "Epoch  2416  G loss  0.10320866\n",
      "Epoch  2417  G loss  0.011795227\n",
      "Epoch  2418  G loss  0.0069023645\n",
      "Epoch  2419  G loss  0.056791686\n",
      "Epoch  2420  G loss  0.022375356\n",
      "Epoch  2421  G loss  0.32231164\n",
      "Epoch  2422  G loss  0.030078713\n",
      "Epoch  2423  G loss  0.08692315\n",
      "Epoch  2424  G loss  0.07467053\n",
      "Epoch  2425  G loss  0.005388738\n",
      "Epoch  2426  G loss  0.013645909\n",
      "Epoch  2427  G loss  0.043200396\n",
      "Epoch  2428  G loss  0.06170594\n",
      "Epoch  2429  G loss  0.009233534\n",
      "Epoch  2430  G loss  0.0055777263\n",
      "Epoch  2431  G loss  0.4433726\n",
      "Epoch  2432  G loss  0.015825015\n",
      "Epoch  2433  G loss  0.05050002\n",
      "Epoch  2434  G loss  0.01687786\n",
      "Epoch  2435  G loss  0.012456121\n",
      "Epoch  2436  G loss  0.029131051\n",
      "Epoch  2437  G loss  0.01693907\n",
      "Epoch  2438  G loss  0.10159817\n",
      "Epoch  2439  G loss  0.05236648\n",
      "Epoch  2440  G loss  0.010418192\n",
      "Epoch  2441  G loss  0.26513487\n",
      "Epoch  2442  G loss  0.029123258\n",
      "Epoch  2443  G loss  0.042511713\n",
      "Epoch  2444  G loss  0.008057917\n",
      "Epoch  2445  G loss  0.032274686\n",
      "Epoch  2446  G loss  0.039544303\n",
      "Epoch  2447  G loss  0.026351023\n",
      "Epoch  2448  G loss  0.054138266\n",
      "Epoch  2449  G loss  0.20429558\n",
      "Epoch  2450  G loss  0.014654629\n",
      "Epoch  2451  G loss  0.12900816\n",
      "Epoch  2452  G loss  0.010311624\n",
      "Epoch  2453  G loss  0.030545617\n",
      "Epoch  2454  G loss  0.063344\n",
      "Epoch  2455  G loss  0.026877958\n",
      "Epoch  2456  G loss  0.037339825\n",
      "Epoch  2457  G loss  0.02259745\n",
      "Epoch  2458  G loss  0.013484195\n",
      "Epoch  2459  G loss  0.042950917\n",
      "Epoch  2460  G loss  0.015988829\n",
      "Epoch  2461  G loss  0.13060367\n",
      "Epoch  2462  G loss  0.1257726\n",
      "Epoch  2463  G loss  0.063624926\n",
      "Epoch  2464  G loss  0.034833033\n",
      "Epoch  2465  G loss  0.079432085\n",
      "Epoch  2466  G loss  0.07079486\n",
      "Epoch  2467  G loss  0.037246563\n",
      "Epoch  2468  G loss  0.018309085\n",
      "Epoch  2469  G loss  0.0046888506\n",
      "Epoch  2470  G loss  0.027361128\n",
      "Epoch  2471  G loss  0.11520474\n",
      "Epoch  2472  G loss  0.069740556\n",
      "Epoch  2473  G loss  0.019115077\n",
      "Epoch  2474  G loss  0.05023624\n",
      "Epoch  2475  G loss  0.011741886\n",
      "Epoch  2476  G loss  0.010802345\n",
      "Epoch  2477  G loss  0.11036587\n",
      "Epoch  2478  G loss  0.07147825\n",
      "Epoch  2479  G loss  0.05064313\n",
      "Epoch  2480  G loss  0.22141035\n",
      "Epoch  2481  G loss  0.011872381\n",
      "Epoch  2482  G loss  0.012386066\n",
      "Epoch  2483  G loss  0.005765138\n",
      "Epoch  2484  G loss  0.01680303\n",
      "Epoch  2485  G loss  0.043322027\n",
      "Epoch  2486  G loss  0.18160073\n",
      "Epoch  2487  G loss  0.027824383\n",
      "Epoch  2488  G loss  0.042911973\n",
      "Epoch  2489  G loss  0.009638273\n",
      "Epoch  2490  G loss  0.0150517505\n",
      "Epoch  2491  G loss  0.09776041\n",
      "Epoch  2492  G loss  0.01301547\n",
      "Epoch  2493  G loss  0.010507042\n",
      "Epoch  2494  G loss  0.031979166\n",
      "Epoch  2495  G loss  0.083555125\n",
      "Epoch  2496  G loss  0.014787786\n",
      "Epoch  2497  G loss  0.047268502\n",
      "Epoch  2498  G loss  0.014583076\n",
      "Epoch  2499  G loss  0.023002278\n",
      "Epoch  2500  G loss  0.04166221\n",
      "Epoch  2501  G loss  0.10294331\n",
      "Epoch  2502  G loss  0.09596332\n",
      "Epoch  2503  G loss  0.1213144\n",
      "Epoch  2504  G loss  0.035573386\n",
      "Epoch  2505  G loss  0.03752372\n",
      "Epoch  2506  G loss  0.16575435\n",
      "Epoch  2507  G loss  0.05626689\n",
      "Epoch  2508  G loss  0.0062639546\n",
      "Epoch  2509  G loss  0.004560292\n",
      "Epoch  2510  G loss  0.25597325\n",
      "Epoch  2511  G loss  0.062063545\n",
      "Epoch  2512  G loss  0.015199237\n",
      "Epoch  2513  G loss  0.014161047\n",
      "Epoch  2514  G loss  0.018710198\n",
      "Epoch  2515  G loss  0.100743644\n",
      "Epoch  2516  G loss  0.017713508\n",
      "Epoch  2517  G loss  0.101304814\n",
      "Epoch  2518  G loss  0.023409199\n",
      "Epoch  2519  G loss  0.09637561\n",
      "Epoch  2520  G loss  0.067274444\n",
      "Epoch  2521  G loss  0.00793418\n",
      "Epoch  2522  G loss  0.008772687\n",
      "Epoch  2523  G loss  0.20943029\n",
      "Epoch  2524  G loss  0.030592777\n",
      "Epoch  2525  G loss  0.028433345\n",
      "Epoch  2526  G loss  0.019463643\n",
      "Epoch  2527  G loss  0.040833205\n",
      "Epoch  2528  G loss  0.013295943\n",
      "Epoch  2529  G loss  0.037680566\n",
      "Epoch  2530  G loss  0.009178703\n",
      "Epoch  2531  G loss  0.05033394\n",
      "Epoch  2532  G loss  0.08910383\n",
      "Epoch  2533  G loss  0.1732907\n",
      "Epoch  2534  G loss  0.031515796\n",
      "Epoch  2535  G loss  0.09840979\n",
      "Epoch  2536  G loss  0.018787827\n",
      "Epoch  2537  G loss  0.013469363\n",
      "Epoch  2538  G loss  0.08485386\n",
      "Epoch  2539  G loss  0.04189316\n",
      "Epoch  2540  G loss  0.027390024\n",
      "Epoch  2541  G loss  0.10071973\n",
      "Epoch  2542  G loss  0.14056462\n",
      "Epoch  2543  G loss  0.11777792\n",
      "Epoch  2544  G loss  0.0055699754\n",
      "Epoch  2545  G loss  0.12392582\n",
      "Epoch  2546  G loss  0.078283\n",
      "Epoch  2547  G loss  0.049682308\n",
      "Epoch  2548  G loss  0.010048915\n",
      "Epoch  2549  G loss  0.15669124\n",
      "Epoch  2550  G loss  0.015790613\n",
      "Epoch  2551  G loss  0.06896232\n",
      "Epoch  2552  G loss  0.025558183\n",
      "Epoch  2553  G loss  0.0378616\n",
      "Epoch  2554  G loss  0.07278398\n",
      "Epoch  2555  G loss  0.077231154\n",
      "Epoch  2556  G loss  0.024147857\n",
      "Epoch  2557  G loss  0.023170434\n",
      "Epoch  2558  G loss  0.07797041\n",
      "Epoch  2559  G loss  0.03571476\n",
      "Epoch  2560  G loss  0.010042927\n",
      "Epoch  2561  G loss  0.2538991\n",
      "Epoch  2562  G loss  0.057315297\n",
      "Epoch  2563  G loss  0.09269539\n",
      "Epoch  2564  G loss  0.01864976\n",
      "Epoch  2565  G loss  0.035202615\n",
      "Epoch  2566  G loss  0.047486886\n",
      "Epoch  2567  G loss  0.022083376\n",
      "Epoch  2568  G loss  0.06807087\n",
      "Epoch  2569  G loss  0.021776067\n",
      "Epoch  2570  G loss  0.0439732\n",
      "Epoch  2571  G loss  0.090888605\n",
      "Epoch  2572  G loss  0.028559174\n",
      "Epoch  2573  G loss  0.046784323\n",
      "Epoch  2574  G loss  0.14792229\n",
      "Epoch  2575  G loss  0.07474973\n",
      "Epoch  2576  G loss  0.0047170673\n",
      "Epoch  2577  G loss  0.07892732\n",
      "Epoch  2578  G loss  0.046776164\n",
      "Epoch  2579  G loss  0.0104500055\n",
      "Epoch  2580  G loss  0.026772393\n",
      "Epoch  2581  G loss  0.07115437\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  2582  G loss  0.04401742\n",
      "Epoch  2583  G loss  0.010839982\n",
      "Epoch  2584  G loss  0.009504331\n",
      "Epoch  2585  G loss  0.0414746\n",
      "Epoch  2586  G loss  0.013719814\n",
      "Epoch  2587  G loss  0.033749137\n",
      "Epoch  2588  G loss  0.012056238\n",
      "Epoch  2589  G loss  0.041601613\n",
      "Epoch  2590  G loss  0.0110534225\n",
      "Epoch  2591  G loss  0.07960294\n",
      "Epoch  2592  G loss  0.2462777\n",
      "Epoch  2593  G loss  0.039038807\n",
      "Epoch  2594  G loss  0.02935711\n",
      "Epoch  2595  G loss  0.012802707\n",
      "Epoch  2596  G loss  0.02600309\n",
      "Epoch  2597  G loss  0.03755272\n",
      "Epoch  2598  G loss  0.25109142\n",
      "Epoch  2599  G loss  0.034615844\n",
      "Epoch  2600  G loss  0.008905609\n",
      "Epoch  2601  G loss  0.017795883\n",
      "Epoch  2602  G loss  0.09263363\n",
      "Epoch  2603  G loss  0.11248174\n",
      "Epoch  2604  G loss  0.007989065\n",
      "Epoch  2605  G loss  0.11669385\n",
      "Epoch  2606  G loss  0.008721627\n",
      "Epoch  2607  G loss  0.015685638\n",
      "Epoch  2608  G loss  0.011422675\n",
      "Epoch  2609  G loss  0.035697963\n",
      "Epoch  2610  G loss  0.01694731\n",
      "Epoch  2611  G loss  0.06431463\n",
      "Epoch  2612  G loss  0.02114055\n",
      "Epoch  2613  G loss  0.032090466\n",
      "Epoch  2614  G loss  0.011476655\n",
      "Epoch  2615  G loss  0.017928394\n",
      "Epoch  2616  G loss  0.013863323\n",
      "Epoch  2617  G loss  0.011933521\n",
      "Epoch  2618  G loss  0.0076176487\n",
      "Epoch  2619  G loss  0.06036502\n",
      "Epoch  2620  G loss  0.014310237\n",
      "Epoch  2621  G loss  0.05800631\n",
      "Epoch  2622  G loss  0.2114959\n",
      "Epoch  2623  G loss  0.035863545\n",
      "Epoch  2624  G loss  0.1705245\n",
      "Epoch  2625  G loss  0.02194485\n",
      "Epoch  2626  G loss  0.057405945\n",
      "Epoch  2627  G loss  0.09131336\n",
      "Epoch  2628  G loss  0.017627897\n",
      "Epoch  2629  G loss  0.11844633\n",
      "Epoch  2630  G loss  0.07170271\n",
      "Epoch  2631  G loss  0.022361495\n",
      "Epoch  2632  G loss  0.09318581\n",
      "Epoch  2633  G loss  0.024830077\n",
      "Epoch  2634  G loss  0.013202885\n",
      "Epoch  2635  G loss  0.009476858\n",
      "Epoch  2636  G loss  0.022695646\n",
      "Epoch  2637  G loss  0.023756841\n",
      "Epoch  2638  G loss  0.037905738\n",
      "Epoch  2639  G loss  0.04841176\n",
      "Epoch  2640  G loss  0.013462357\n",
      "Epoch  2641  G loss  0.013691554\n",
      "Epoch  2642  G loss  0.027054274\n",
      "Epoch  2643  G loss  0.056218285\n",
      "Epoch  2644  G loss  0.014454207\n",
      "Epoch  2645  G loss  0.0922003\n",
      "Epoch  2646  G loss  0.2448959\n",
      "Epoch  2647  G loss  0.11015032\n",
      "Epoch  2648  G loss  0.020805156\n",
      "Epoch  2649  G loss  0.01664525\n",
      "Epoch  2650  G loss  0.013186381\n",
      "Epoch  2651  G loss  0.012975799\n",
      "Epoch  2652  G loss  0.10448635\n",
      "Epoch  2653  G loss  0.06398599\n",
      "Epoch  2654  G loss  0.09953998\n",
      "Epoch  2655  G loss  0.01746217\n",
      "Epoch  2656  G loss  0.07603769\n",
      "Epoch  2657  G loss  0.04304625\n",
      "Epoch  2658  G loss  0.13797611\n",
      "Epoch  2659  G loss  0.024450846\n",
      "Epoch  2660  G loss  0.043422416\n",
      "Epoch  2661  G loss  0.0140235815\n",
      "Epoch  2662  G loss  0.33183476\n",
      "Epoch  2663  G loss  0.04908149\n",
      "Epoch  2664  G loss  0.08885437\n",
      "Epoch  2665  G loss  0.037916362\n",
      "Epoch  2666  G loss  0.0423887\n",
      "Epoch  2667  G loss  0.024017466\n",
      "Epoch  2668  G loss  0.019440455\n",
      "Epoch  2669  G loss  0.0229595\n",
      "Epoch  2670  G loss  0.023488387\n",
      "Epoch  2671  G loss  0.043162376\n",
      "Epoch  2672  G loss  0.22385132\n",
      "Epoch  2673  G loss  0.055552885\n",
      "Epoch  2674  G loss  0.042462513\n",
      "Epoch  2675  G loss  0.031203493\n",
      "Epoch  2676  G loss  0.056633547\n",
      "Epoch  2677  G loss  0.007464771\n",
      "Epoch  2678  G loss  0.007231002\n",
      "Epoch  2679  G loss  0.015116774\n",
      "Epoch  2680  G loss  0.046436638\n",
      "Epoch  2681  G loss  0.05258124\n",
      "Epoch  2682  G loss  0.01465711\n",
      "Epoch  2683  G loss  0.067550346\n",
      "Epoch  2684  G loss  0.06884639\n",
      "Epoch  2685  G loss  0.012449541\n",
      "Epoch  2686  G loss  0.027388897\n",
      "Epoch  2687  G loss  0.01385531\n",
      "Epoch  2688  G loss  0.027786605\n",
      "Epoch  2689  G loss  0.010546543\n",
      "Epoch  2690  G loss  0.042927343\n",
      "Epoch  2691  G loss  0.20992684\n",
      "Epoch  2692  G loss  0.013146374\n",
      "Epoch  2693  G loss  0.009371491\n",
      "Epoch  2694  G loss  0.1596478\n",
      "Epoch  2695  G loss  0.09829399\n",
      "Epoch  2696  G loss  0.014304551\n",
      "Epoch  2697  G loss  0.03131484\n",
      "Epoch  2698  G loss  0.22984564\n",
      "Epoch  2699  G loss  0.16572654\n",
      "Epoch  2700  G loss  0.019122595\n",
      "Epoch  2701  G loss  0.03006424\n",
      "Epoch  2702  G loss  0.011195595\n",
      "Epoch  2703  G loss  0.0062013497\n",
      "Epoch  2704  G loss  0.03568896\n",
      "Epoch  2705  G loss  0.012684027\n",
      "Epoch  2706  G loss  0.012759345\n",
      "Epoch  2707  G loss  0.039267275\n",
      "Epoch  2708  G loss  0.014072218\n",
      "Epoch  2709  G loss  0.028172394\n",
      "Epoch  2710  G loss  0.012396639\n",
      "Epoch  2711  G loss  0.01958829\n",
      "Epoch  2712  G loss  0.024409514\n",
      "Epoch  2713  G loss  0.03511399\n",
      "Epoch  2714  G loss  0.010486472\n",
      "Epoch  2715  G loss  0.056755677\n",
      "Epoch  2716  G loss  0.04259574\n",
      "Epoch  2717  G loss  0.012871059\n",
      "Epoch  2718  G loss  0.009557463\n",
      "Epoch  2719  G loss  0.22290744\n",
      "Epoch  2720  G loss  0.060183916\n",
      "Epoch  2721  G loss  0.017981585\n",
      "Epoch  2722  G loss  0.038860574\n",
      "Epoch  2723  G loss  0.031231914\n",
      "Epoch  2724  G loss  0.008857222\n",
      "Epoch  2725  G loss  0.12658255\n",
      "Epoch  2726  G loss  0.0208641\n",
      "Epoch  2727  G loss  0.047007322\n",
      "Epoch  2728  G loss  0.029169906\n",
      "Epoch  2729  G loss  0.06870497\n",
      "Epoch  2730  G loss  0.026041832\n",
      "Epoch  2731  G loss  0.061003912\n",
      "Epoch  2732  G loss  0.100063205\n",
      "Epoch  2733  G loss  0.026615864\n",
      "Epoch  2734  G loss  0.008845916\n",
      "Epoch  2735  G loss  0.1494554\n",
      "Epoch  2736  G loss  0.097898036\n",
      "Epoch  2737  G loss  0.014665084\n",
      "Epoch  2738  G loss  0.030237399\n",
      "Epoch  2739  G loss  0.0619428\n",
      "Epoch  2740  G loss  0.017423999\n",
      "Epoch  2741  G loss  0.28418708\n",
      "Epoch  2742  G loss  0.10532376\n",
      "Epoch  2743  G loss  0.011281115\n",
      "Epoch  2744  G loss  0.13496369\n",
      "Epoch  2745  G loss  0.12852967\n",
      "Epoch  2746  G loss  0.08712421\n",
      "Epoch  2747  G loss  0.019633971\n",
      "Epoch  2748  G loss  0.04513112\n",
      "Epoch  2749  G loss  0.0678739\n",
      "Epoch  2750  G loss  0.047090903\n",
      "Epoch  2751  G loss  0.0119488835\n",
      "Epoch  2752  G loss  0.06930713\n",
      "Epoch  2753  G loss  0.027397705\n",
      "Epoch  2754  G loss  0.17949012\n",
      "Epoch  2755  G loss  0.20743573\n",
      "Epoch  2756  G loss  0.09541646\n",
      "Epoch  2757  G loss  0.055783123\n",
      "Epoch  2758  G loss  0.022202965\n",
      "Epoch  2759  G loss  0.06533018\n",
      "Epoch  2760  G loss  0.03586573\n",
      "Epoch  2761  G loss  0.051247694\n",
      "Epoch  2762  G loss  0.028940652\n",
      "Epoch  2763  G loss  0.009637938\n",
      "Epoch  2764  G loss  0.041289814\n",
      "Epoch  2765  G loss  0.10561739\n",
      "Epoch  2766  G loss  0.021709021\n",
      "Epoch  2767  G loss  0.04285992\n",
      "Epoch  2768  G loss  0.23882864\n",
      "Epoch  2769  G loss  0.013409449\n",
      "Epoch  2770  G loss  0.013294276\n",
      "Epoch  2771  G loss  0.038519613\n",
      "Epoch  2772  G loss  0.070891485\n",
      "Epoch  2773  G loss  0.011532999\n",
      "Epoch  2774  G loss  0.03200091\n",
      "Epoch  2775  G loss  0.019427639\n",
      "Epoch  2776  G loss  0.013080696\n",
      "Epoch  2777  G loss  0.021429755\n",
      "Epoch  2778  G loss  0.10700533\n",
      "Epoch  2779  G loss  0.015211048\n",
      "Epoch  2780  G loss  0.010252092\n",
      "Epoch  2781  G loss  0.020220954\n",
      "Epoch  2782  G loss  0.027141437\n",
      "Epoch  2783  G loss  0.029435717\n",
      "Epoch  2784  G loss  0.01737322\n",
      "Epoch  2785  G loss  0.15750903\n",
      "Epoch  2786  G loss  0.122287214\n",
      "Epoch  2787  G loss  0.074187495\n",
      "Epoch  2788  G loss  0.062593274\n",
      "Epoch  2789  G loss  0.012273381\n",
      "Epoch  2790  G loss  0.054034397\n",
      "Epoch  2791  G loss  0.039190717\n",
      "Epoch  2792  G loss  0.11756569\n",
      "Epoch  2793  G loss  0.02354115\n",
      "Epoch  2794  G loss  0.018919442\n",
      "Epoch  2795  G loss  0.015539933\n",
      "Epoch  2796  G loss  0.020393867\n",
      "Epoch  2797  G loss  0.009666424\n",
      "Epoch  2798  G loss  0.039533094\n",
      "Epoch  2799  G loss  0.008295797\n",
      "Epoch  2800  G loss  0.024960008\n",
      "Epoch  2801  G loss  0.009809672\n",
      "Epoch  2802  G loss  0.09572956\n",
      "Epoch  2803  G loss  0.0436201\n",
      "Epoch  2804  G loss  0.012793056\n",
      "Epoch  2805  G loss  0.027985547\n",
      "Epoch  2806  G loss  0.028281383\n",
      "Epoch  2807  G loss  0.13365196\n",
      "Epoch  2808  G loss  0.0123493\n",
      "Epoch  2809  G loss  0.023926232\n",
      "Epoch  2810  G loss  0.014839543\n",
      "Epoch  2811  G loss  0.11232544\n",
      "Epoch  2812  G loss  0.044570886\n",
      "Epoch  2813  G loss  0.034351572\n",
      "Epoch  2814  G loss  0.015037116\n",
      "Epoch  2815  G loss  0.016224666\n",
      "Epoch  2816  G loss  0.017194191\n",
      "Epoch  2817  G loss  0.023410205\n",
      "Epoch  2818  G loss  0.010859513\n",
      "Epoch  2819  G loss  0.015334574\n",
      "Epoch  2820  G loss  0.016369034\n",
      "Epoch  2821  G loss  0.03841148\n",
      "Epoch  2822  G loss  0.0061941957\n",
      "Epoch  2823  G loss  0.17292239\n",
      "Epoch  2824  G loss  0.052367438\n",
      "Epoch  2825  G loss  0.009080721\n",
      "Epoch  2826  G loss  0.02226556\n",
      "Epoch  2827  G loss  0.008820648\n",
      "Epoch  2828  G loss  0.011663673\n",
      "Epoch  2829  G loss  0.11352208\n",
      "Epoch  2830  G loss  0.007882522\n",
      "Epoch  2831  G loss  0.017720878\n",
      "Epoch  2832  G loss  0.052449822\n",
      "Epoch  2833  G loss  0.07261704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  2834  G loss  0.024243236\n",
      "Epoch  2835  G loss  0.091980845\n",
      "Epoch  2836  G loss  0.051224023\n",
      "Epoch  2837  G loss  0.010115055\n",
      "Epoch  2838  G loss  0.2327918\n",
      "Epoch  2839  G loss  0.077172354\n",
      "Epoch  2840  G loss  0.055954274\n",
      "Epoch  2841  G loss  0.026596967\n",
      "Epoch  2842  G loss  0.19676547\n",
      "Epoch  2843  G loss  0.062262133\n",
      "Epoch  2844  G loss  0.0150140105\n",
      "Epoch  2845  G loss  0.02507717\n",
      "Epoch  2846  G loss  0.020218242\n",
      "Epoch  2847  G loss  0.026590684\n",
      "Epoch  2848  G loss  0.007270042\n",
      "Epoch  2849  G loss  0.055467512\n",
      "Epoch  2850  G loss  0.1046043\n",
      "Epoch  2851  G loss  0.010585681\n",
      "Epoch  2852  G loss  0.0139326\n",
      "Epoch  2853  G loss  0.02599591\n",
      "Epoch  2854  G loss  0.02751309\n",
      "Epoch  2855  G loss  0.013101979\n",
      "Epoch  2856  G loss  0.010474688\n",
      "Epoch  2857  G loss  0.18876973\n",
      "Epoch  2858  G loss  0.02887391\n",
      "Epoch  2859  G loss  0.053998675\n",
      "Epoch  2860  G loss  0.013352329\n",
      "Epoch  2861  G loss  0.12831724\n",
      "Epoch  2862  G loss  0.008574272\n",
      "Epoch  2863  G loss  0.028603831\n",
      "Epoch  2864  G loss  0.06286281\n",
      "Epoch  2865  G loss  0.011597061\n",
      "Epoch  2866  G loss  0.10226451\n",
      "Epoch  2867  G loss  0.038858064\n",
      "Epoch  2868  G loss  0.04798851\n",
      "Epoch  2869  G loss  0.012275935\n",
      "Epoch  2870  G loss  0.0238642\n",
      "Epoch  2871  G loss  0.07910125\n",
      "Epoch  2872  G loss  0.029006666\n",
      "Epoch  2873  G loss  0.11114937\n",
      "Epoch  2874  G loss  0.039958313\n",
      "Epoch  2875  G loss  0.015463477\n",
      "Epoch  2876  G loss  0.036616582\n",
      "Epoch  2877  G loss  0.01619436\n",
      "Epoch  2878  G loss  0.042865872\n",
      "Epoch  2879  G loss  0.058792774\n",
      "Epoch  2880  G loss  0.007579879\n",
      "Epoch  2881  G loss  0.010796042\n",
      "Epoch  2882  G loss  0.01085466\n",
      "Epoch  2883  G loss  0.019217135\n",
      "Epoch  2884  G loss  0.018577999\n",
      "Epoch  2885  G loss  0.06697394\n",
      "Epoch  2886  G loss  0.042162426\n",
      "Epoch  2887  G loss  0.04023437\n",
      "Epoch  2888  G loss  0.0065121516\n",
      "Epoch  2889  G loss  0.01688286\n",
      "Epoch  2890  G loss  0.009111848\n",
      "Epoch  2891  G loss  0.019199567\n",
      "Epoch  2892  G loss  0.009396886\n",
      "Epoch  2893  G loss  0.00809714\n",
      "Epoch  2894  G loss  0.04591293\n",
      "Epoch  2895  G loss  0.013944266\n",
      "Epoch  2896  G loss  0.012436562\n",
      "Epoch  2897  G loss  0.0055288025\n",
      "Epoch  2898  G loss  0.005933691\n",
      "Epoch  2899  G loss  0.162488\n",
      "Epoch  2900  G loss  0.063682795\n",
      "Epoch  2901  G loss  0.022315683\n",
      "Epoch  2902  G loss  0.16181228\n",
      "Epoch  2903  G loss  0.019811943\n",
      "Epoch  2904  G loss  0.05386544\n",
      "Epoch  2905  G loss  0.04962243\n",
      "Epoch  2906  G loss  0.02676796\n",
      "Epoch  2907  G loss  0.027786482\n",
      "Epoch  2908  G loss  0.02666406\n",
      "Epoch  2909  G loss  0.033872165\n",
      "Epoch  2910  G loss  0.053526394\n",
      "Epoch  2911  G loss  0.056190014\n",
      "Epoch  2912  G loss  0.101364255\n",
      "Epoch  2913  G loss  0.01206958\n",
      "Epoch  2914  G loss  0.034881584\n",
      "Epoch  2915  G loss  0.067562595\n",
      "Epoch  2916  G loss  0.051375076\n",
      "Epoch  2917  G loss  0.0981138\n",
      "Epoch  2918  G loss  0.14770845\n",
      "Epoch  2919  G loss  0.076600656\n",
      "Epoch  2920  G loss  0.17434274\n",
      "Epoch  2921  G loss  0.022453621\n",
      "Epoch  2922  G loss  0.08428495\n",
      "Epoch  2923  G loss  0.028678134\n",
      "Epoch  2924  G loss  0.01429272\n",
      "Epoch  2925  G loss  0.060837794\n",
      "Epoch  2926  G loss  0.15205148\n",
      "Epoch  2927  G loss  0.0996941\n",
      "Epoch  2928  G loss  0.023741897\n",
      "Epoch  2929  G loss  0.03555163\n",
      "Epoch  2930  G loss  0.0077005588\n",
      "Epoch  2931  G loss  0.01742538\n",
      "Epoch  2932  G loss  0.048328094\n",
      "Epoch  2933  G loss  0.14943892\n",
      "Epoch  2934  G loss  0.12702148\n",
      "Epoch  2935  G loss  0.1257029\n",
      "Epoch  2936  G loss  0.034693703\n",
      "Epoch  2937  G loss  0.050258957\n",
      "Epoch  2938  G loss  0.029634828\n",
      "Epoch  2939  G loss  0.044483874\n",
      "Epoch  2940  G loss  0.019114466\n",
      "Epoch  2941  G loss  0.03318624\n",
      "Epoch  2942  G loss  0.121610194\n",
      "Epoch  2943  G loss  0.04089633\n",
      "Epoch  2944  G loss  0.044070967\n",
      "Epoch  2945  G loss  0.016063629\n",
      "Epoch  2946  G loss  0.0077557554\n",
      "Epoch  2947  G loss  0.018001977\n",
      "Epoch  2948  G loss  0.043131247\n",
      "Epoch  2949  G loss  0.016065504\n",
      "Epoch  2950  G loss  0.0126615\n",
      "Epoch  2951  G loss  0.020327678\n",
      "Epoch  2952  G loss  0.054116134\n",
      "Epoch  2953  G loss  0.065163635\n",
      "Epoch  2954  G loss  0.15116146\n",
      "Epoch  2955  G loss  0.29767153\n",
      "Epoch  2956  G loss  0.07710057\n",
      "Epoch  2957  G loss  0.018712442\n",
      "Epoch  2958  G loss  0.031130826\n",
      "Epoch  2959  G loss  0.07574062\n",
      "Epoch  2960  G loss  0.01731999\n",
      "Epoch  2961  G loss  0.0467741\n",
      "Epoch  2962  G loss  0.03838267\n",
      "Epoch  2963  G loss  0.015521149\n",
      "Epoch  2964  G loss  0.038143698\n",
      "Epoch  2965  G loss  0.035132412\n",
      "Epoch  2966  G loss  0.028936848\n",
      "Epoch  2967  G loss  0.015702264\n",
      "Epoch  2968  G loss  0.021783631\n",
      "Epoch  2969  G loss  0.0063111307\n",
      "Epoch  2970  G loss  0.0069682933\n",
      "Epoch  2971  G loss  0.05605569\n",
      "Epoch  2972  G loss  0.033184372\n",
      "Epoch  2973  G loss  0.09197901\n",
      "Epoch  2974  G loss  0.07498968\n",
      "Epoch  2975  G loss  0.05974955\n",
      "Epoch  2976  G loss  0.059542775\n",
      "Epoch  2977  G loss  0.02877237\n",
      "Epoch  2978  G loss  0.014201994\n",
      "Epoch  2979  G loss  0.041760623\n",
      "Epoch  2980  G loss  0.112307474\n",
      "Epoch  2981  G loss  0.23275095\n",
      "Epoch  2982  G loss  0.015027225\n",
      "Epoch  2983  G loss  0.073205255\n",
      "Epoch  2984  G loss  0.01571085\n",
      "Epoch  2985  G loss  0.0065280963\n",
      "Epoch  2986  G loss  0.040840656\n",
      "Epoch  2987  G loss  0.019407492\n",
      "Epoch  2988  G loss  0.0075040148\n",
      "Epoch  2989  G loss  0.04682789\n",
      "Epoch  2990  G loss  0.027493495\n",
      "Epoch  2991  G loss  0.02789643\n",
      "Epoch  2992  G loss  0.008340569\n",
      "Epoch  2993  G loss  0.08494547\n",
      "Epoch  2994  G loss  0.012146042\n",
      "Epoch  2995  G loss  0.018722523\n",
      "Epoch  2996  G loss  0.09847412\n",
      "Epoch  2997  G loss  0.02544137\n",
      "Epoch  2998  G loss  0.041182056\n",
      "Epoch  2999  G loss  0.015369999\n",
      "Epoch  3000  G loss  0.026931986\n",
      "Epoch  3001  G loss  0.13214487\n",
      "Epoch  3002  G loss  0.1666128\n",
      "Epoch  3003  G loss  0.041195255\n",
      "Epoch  3004  G loss  0.071186274\n",
      "Epoch  3005  G loss  0.021292346\n",
      "Epoch  3006  G loss  0.032343786\n",
      "Epoch  3007  G loss  0.052055985\n",
      "Epoch  3008  G loss  0.030703794\n",
      "Epoch  3009  G loss  0.016920706\n",
      "Epoch  3010  G loss  0.01870441\n",
      "Epoch  3011  G loss  0.022216957\n",
      "Epoch  3012  G loss  0.035734665\n",
      "Epoch  3013  G loss  0.1492907\n",
      "Epoch  3014  G loss  0.01671711\n",
      "Epoch  3015  G loss  0.009873992\n",
      "Epoch  3016  G loss  0.032315023\n",
      "Epoch  3017  G loss  0.009086518\n",
      "Epoch  3018  G loss  0.024751538\n",
      "Epoch  3019  G loss  0.04177803\n",
      "Epoch  3020  G loss  0.026474342\n",
      "Epoch  3021  G loss  0.0051945737\n",
      "Epoch  3022  G loss  0.042560987\n",
      "Epoch  3023  G loss  0.034627967\n",
      "Epoch  3024  G loss  0.014590301\n",
      "Epoch  3025  G loss  0.061482646\n",
      "Epoch  3026  G loss  0.049285132\n",
      "Epoch  3027  G loss  0.30147642\n",
      "Epoch  3028  G loss  0.024716191\n",
      "Epoch  3029  G loss  0.0273301\n",
      "Epoch  3030  G loss  0.21397473\n",
      "Epoch  3031  G loss  0.051026963\n",
      "Epoch  3032  G loss  0.04554201\n",
      "Epoch  3033  G loss  0.020026926\n",
      "Epoch  3034  G loss  0.0115749845\n",
      "Epoch  3035  G loss  0.03532682\n",
      "Epoch  3036  G loss  0.004351768\n",
      "Epoch  3037  G loss  0.053981904\n",
      "Epoch  3038  G loss  0.12891233\n",
      "Epoch  3039  G loss  0.009027971\n",
      "Epoch  3040  G loss  0.14891468\n",
      "Epoch  3041  G loss  0.018469859\n",
      "Epoch  3042  G loss  0.007956124\n",
      "Epoch  3043  G loss  0.007850528\n",
      "Epoch  3044  G loss  0.16184095\n",
      "Epoch  3045  G loss  0.015819483\n",
      "Epoch  3046  G loss  0.26656836\n",
      "Epoch  3047  G loss  0.023622155\n",
      "Epoch  3048  G loss  0.011542329\n",
      "Epoch  3049  G loss  0.1251185\n",
      "Epoch  3050  G loss  0.017206196\n",
      "Epoch  3051  G loss  0.025174163\n",
      "Epoch  3052  G loss  0.058418415\n",
      "Epoch  3053  G loss  0.016069487\n",
      "Epoch  3054  G loss  0.012185257\n",
      "Epoch  3055  G loss  0.045249384\n",
      "Epoch  3056  G loss  0.100755155\n",
      "Epoch  3057  G loss  0.08478865\n",
      "Epoch  3058  G loss  0.014925187\n",
      "Epoch  3059  G loss  0.009442672\n",
      "Epoch  3060  G loss  0.18417573\n",
      "Epoch  3061  G loss  0.006096609\n",
      "Epoch  3062  G loss  0.07985461\n",
      "Epoch  3063  G loss  0.1543665\n",
      "Epoch  3064  G loss  0.008262197\n",
      "Epoch  3065  G loss  0.025282223\n",
      "Epoch  3066  G loss  0.24848449\n",
      "Epoch  3067  G loss  0.20010786\n",
      "Epoch  3068  G loss  0.1531658\n",
      "Epoch  3069  G loss  0.025569169\n",
      "Epoch  3070  G loss  0.034987815\n",
      "Epoch  3071  G loss  0.11351347\n",
      "Epoch  3072  G loss  0.0264658\n",
      "Epoch  3073  G loss  0.015878163\n",
      "Epoch  3074  G loss  0.04193358\n",
      "Epoch  3075  G loss  0.030191973\n",
      "Epoch  3076  G loss  0.012333974\n",
      "Epoch  3077  G loss  0.01610272\n",
      "Epoch  3078  G loss  0.04796858\n",
      "Epoch  3079  G loss  0.018649459\n",
      "Epoch  3080  G loss  0.022157228\n",
      "Epoch  3081  G loss  0.011956435\n",
      "Epoch  3082  G loss  0.013745868\n",
      "Epoch  3083  G loss  0.09875955\n",
      "Epoch  3084  G loss  0.02280569\n",
      "Epoch  3085  G loss  0.13672987\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  3086  G loss  0.23096977\n",
      "Epoch  3087  G loss  0.26621687\n",
      "Epoch  3088  G loss  0.027534762\n",
      "Epoch  3089  G loss  0.03975656\n",
      "Epoch  3090  G loss  0.12981525\n",
      "Epoch  3091  G loss  0.041918986\n",
      "Epoch  3092  G loss  0.025914233\n",
      "Epoch  3093  G loss  0.06347129\n",
      "Epoch  3094  G loss  0.11957997\n",
      "Epoch  3095  G loss  0.035676576\n",
      "Epoch  3096  G loss  0.021879125\n",
      "Epoch  3097  G loss  0.12159345\n",
      "Epoch  3098  G loss  0.01539132\n",
      "Epoch  3099  G loss  0.018898863\n",
      "Epoch  3100  G loss  0.009596888\n",
      "Epoch  3101  G loss  0.04856706\n",
      "Epoch  3102  G loss  0.053482078\n",
      "Epoch  3103  G loss  0.035324425\n",
      "Epoch  3104  G loss  0.08199421\n",
      "Epoch  3105  G loss  0.14124854\n",
      "Epoch  3106  G loss  0.031444363\n",
      "Epoch  3107  G loss  0.0651498\n",
      "Epoch  3108  G loss  0.006637168\n",
      "Epoch  3109  G loss  0.013832882\n",
      "Epoch  3110  G loss  0.010356599\n",
      "Epoch  3111  G loss  0.013137445\n",
      "Epoch  3112  G loss  0.04594805\n",
      "Epoch  3113  G loss  0.007570774\n",
      "Epoch  3114  G loss  0.044421867\n",
      "Epoch  3115  G loss  0.0029791808\n",
      "Epoch  3116  G loss  0.17007734\n",
      "Epoch  3117  G loss  0.0107053155\n",
      "Epoch  3118  G loss  0.028977513\n",
      "Epoch  3119  G loss  0.0324025\n",
      "Epoch  3120  G loss  0.021967016\n",
      "Epoch  3121  G loss  0.024201268\n",
      "Epoch  3122  G loss  0.025065837\n",
      "Epoch  3123  G loss  0.09166016\n",
      "Epoch  3124  G loss  0.068371214\n",
      "Epoch  3125  G loss  0.010880999\n",
      "Epoch  3126  G loss  0.04635954\n",
      "Epoch  3127  G loss  0.014671424\n",
      "Epoch  3128  G loss  0.031559426\n",
      "Epoch  3129  G loss  0.17777115\n",
      "Epoch  3130  G loss  0.051135223\n",
      "Epoch  3131  G loss  0.06262234\n",
      "Epoch  3132  G loss  0.01610471\n",
      "Epoch  3133  G loss  0.10165752\n",
      "Epoch  3134  G loss  0.14425987\n",
      "Epoch  3135  G loss  0.048934303\n",
      "Epoch  3136  G loss  0.012692452\n",
      "Epoch  3137  G loss  0.020607136\n",
      "Epoch  3138  G loss  0.045920298\n",
      "Epoch  3139  G loss  0.036392216\n",
      "Epoch  3140  G loss  0.019824777\n",
      "Epoch  3141  G loss  0.011762527\n",
      "Epoch  3142  G loss  0.10666652\n",
      "Epoch  3143  G loss  0.014113566\n",
      "Epoch  3144  G loss  0.021147648\n",
      "Epoch  3145  G loss  0.019346168\n",
      "Epoch  3146  G loss  0.10989838\n",
      "Epoch  3147  G loss  0.04829438\n",
      "Epoch  3148  G loss  0.06592758\n",
      "Epoch  3149  G loss  0.013263874\n",
      "Epoch  3150  G loss  0.0352581\n",
      "Epoch  3151  G loss  0.011447171\n",
      "Epoch  3152  G loss  0.013157554\n",
      "Epoch  3153  G loss  0.18577768\n",
      "Epoch  3154  G loss  0.022816602\n",
      "Epoch  3155  G loss  0.018258173\n",
      "Epoch  3156  G loss  0.008074273\n",
      "Epoch  3157  G loss  0.006009877\n",
      "Epoch  3158  G loss  0.033611104\n",
      "Epoch  3159  G loss  0.014269567\n",
      "Epoch  3160  G loss  0.021857461\n",
      "Epoch  3161  G loss  0.029000495\n",
      "Epoch  3162  G loss  0.046844542\n",
      "Epoch  3163  G loss  0.05026749\n",
      "Epoch  3164  G loss  0.04453922\n",
      "Epoch  3165  G loss  0.02397811\n",
      "Epoch  3166  G loss  0.049184777\n",
      "Epoch  3167  G loss  0.018966015\n",
      "Epoch  3168  G loss  0.07733281\n",
      "Epoch  3169  G loss  0.07288156\n",
      "Epoch  3170  G loss  0.07410691\n",
      "Epoch  3171  G loss  0.04740639\n",
      "Epoch  3172  G loss  0.015618506\n",
      "Epoch  3173  G loss  0.043682024\n",
      "Epoch  3174  G loss  0.029389279\n",
      "Epoch  3175  G loss  0.013052273\n",
      "Epoch  3176  G loss  0.07720314\n",
      "Epoch  3177  G loss  0.1087686\n",
      "Epoch  3178  G loss  0.08380218\n",
      "Epoch  3179  G loss  0.0137049705\n",
      "Epoch  3180  G loss  0.017027017\n",
      "Epoch  3181  G loss  0.030270193\n",
      "Epoch  3182  G loss  0.00908391\n",
      "Epoch  3183  G loss  0.013540183\n",
      "Epoch  3184  G loss  0.009415263\n",
      "Epoch  3185  G loss  0.028607702\n",
      "Epoch  3186  G loss  0.013041183\n",
      "Epoch  3187  G loss  0.010827314\n",
      "Epoch  3188  G loss  0.0138108805\n",
      "Epoch  3189  G loss  0.03912291\n",
      "Epoch  3190  G loss  0.0334279\n",
      "Epoch  3191  G loss  0.072506696\n",
      "Epoch  3192  G loss  0.09897085\n",
      "Epoch  3193  G loss  0.018905923\n",
      "Epoch  3194  G loss  0.023073897\n",
      "Epoch  3195  G loss  0.074710295\n",
      "Epoch  3196  G loss  0.015524425\n",
      "Epoch  3197  G loss  0.33635724\n",
      "Epoch  3198  G loss  0.011645431\n",
      "Epoch  3199  G loss  0.07580054\n",
      "Epoch  3200  G loss  0.07819571\n",
      "Epoch  3201  G loss  0.046037078\n",
      "Epoch  3202  G loss  0.030536585\n",
      "Epoch  3203  G loss  0.01559385\n",
      "Epoch  3204  G loss  0.025011845\n",
      "Epoch  3205  G loss  0.015131099\n",
      "Epoch  3206  G loss  0.009104272\n",
      "Epoch  3207  G loss  0.14949581\n",
      "Epoch  3208  G loss  0.096910655\n",
      "Epoch  3209  G loss  0.11328621\n",
      "Epoch  3210  G loss  0.023591759\n",
      "Epoch  3211  G loss  0.010335526\n",
      "Epoch  3212  G loss  0.009424567\n",
      "Epoch  3213  G loss  0.037024952\n",
      "Epoch  3214  G loss  0.049097173\n",
      "Epoch  3215  G loss  0.019955186\n",
      "Epoch  3216  G loss  0.028716002\n",
      "Epoch  3217  G loss  0.080474615\n",
      "Epoch  3218  G loss  0.07873657\n",
      "Epoch  3219  G loss  0.036207832\n",
      "Epoch  3220  G loss  0.042198878\n",
      "Epoch  3221  G loss  0.008963032\n",
      "Epoch  3222  G loss  0.020825412\n",
      "Epoch  3223  G loss  0.01649189\n",
      "Epoch  3224  G loss  0.038587548\n",
      "Epoch  3225  G loss  0.010948561\n",
      "Epoch  3226  G loss  0.05196341\n",
      "Epoch  3227  G loss  0.046183817\n",
      "Epoch  3228  G loss  0.04970955\n",
      "Epoch  3229  G loss  0.051308446\n",
      "Epoch  3230  G loss  0.020442722\n",
      "Epoch  3231  G loss  0.01897581\n",
      "Epoch  3232  G loss  0.03747628\n",
      "Epoch  3233  G loss  0.00488858\n",
      "Epoch  3234  G loss  0.015973438\n",
      "Epoch  3235  G loss  0.016351178\n",
      "Epoch  3236  G loss  0.12818703\n",
      "Epoch  3237  G loss  0.04945644\n",
      "Epoch  3238  G loss  0.0070467726\n",
      "Epoch  3239  G loss  0.19475153\n",
      "Epoch  3240  G loss  0.019745793\n",
      "Epoch  3241  G loss  0.040512074\n",
      "Epoch  3242  G loss  0.071469024\n",
      "Epoch  3243  G loss  0.03309484\n",
      "Epoch  3244  G loss  0.014019256\n",
      "Epoch  3245  G loss  0.036663413\n",
      "Epoch  3246  G loss  0.03446675\n",
      "Epoch  3247  G loss  0.047551382\n",
      "Epoch  3248  G loss  0.026649741\n",
      "Epoch  3249  G loss  0.05194724\n",
      "Epoch  3250  G loss  0.044002417\n",
      "Epoch  3251  G loss  0.18568091\n",
      "Epoch  3252  G loss  0.034294445\n",
      "Epoch  3253  G loss  0.04287284\n",
      "Epoch  3254  G loss  0.01770809\n",
      "Epoch  3255  G loss  0.010359059\n",
      "Epoch  3256  G loss  0.10785038\n",
      "Epoch  3257  G loss  0.042313095\n",
      "Epoch  3258  G loss  0.035378426\n",
      "Epoch  3259  G loss  0.008649684\n",
      "Epoch  3260  G loss  0.008829781\n",
      "Epoch  3261  G loss  0.036539957\n",
      "Epoch  3262  G loss  0.026068453\n",
      "Epoch  3263  G loss  0.0736154\n",
      "Epoch  3264  G loss  0.20044193\n",
      "Epoch  3265  G loss  0.029249825\n",
      "Epoch  3266  G loss  0.026034206\n",
      "Epoch  3267  G loss  0.045623496\n",
      "Epoch  3268  G loss  0.029883306\n",
      "Epoch  3269  G loss  0.07489989\n",
      "Epoch  3270  G loss  0.010237177\n",
      "Epoch  3271  G loss  0.0122083835\n",
      "Epoch  3272  G loss  0.038527176\n",
      "Epoch  3273  G loss  0.0141636375\n",
      "Epoch  3274  G loss  0.030007843\n",
      "Epoch  3275  G loss  0.21580954\n",
      "Epoch  3276  G loss  0.07228234\n",
      "Epoch  3277  G loss  0.017189106\n",
      "Epoch  3278  G loss  0.04148347\n",
      "Epoch  3279  G loss  0.022506308\n",
      "Epoch  3280  G loss  0.038090564\n",
      "Epoch  3281  G loss  0.018819226\n",
      "Epoch  3282  G loss  0.14664122\n",
      "Epoch  3283  G loss  0.07729126\n",
      "Epoch  3284  G loss  0.010345285\n",
      "Epoch  3285  G loss  0.021328948\n",
      "Epoch  3286  G loss  0.013327138\n",
      "Epoch  3287  G loss  0.04527034\n",
      "Epoch  3288  G loss  0.2657436\n",
      "Epoch  3289  G loss  0.0064676395\n",
      "Epoch  3290  G loss  0.057526417\n",
      "Epoch  3291  G loss  0.14499466\n",
      "Epoch  3292  G loss  0.012219291\n",
      "Epoch  3293  G loss  0.0153485555\n",
      "Epoch  3294  G loss  0.047630794\n",
      "Epoch  3295  G loss  0.13044637\n",
      "Epoch  3296  G loss  0.026333636\n",
      "Epoch  3297  G loss  0.0077228686\n",
      "Epoch  3298  G loss  0.07103643\n",
      "Epoch  3299  G loss  0.13447957\n",
      "Epoch  3300  G loss  0.016221467\n",
      "Epoch  3301  G loss  0.0390023\n",
      "Epoch  3302  G loss  0.013655073\n",
      "Epoch  3303  G loss  0.03893823\n",
      "Epoch  3304  G loss  0.11640348\n",
      "Epoch  3305  G loss  0.018935103\n",
      "Epoch  3306  G loss  0.07163911\n",
      "Epoch  3307  G loss  0.020068528\n",
      "Epoch  3308  G loss  0.07629641\n",
      "Epoch  3309  G loss  0.069985814\n",
      "Epoch  3310  G loss  0.06327418\n",
      "Epoch  3311  G loss  0.0540097\n",
      "Epoch  3312  G loss  0.08723815\n",
      "Epoch  3313  G loss  0.018413981\n",
      "Epoch  3314  G loss  0.0059055216\n",
      "Epoch  3315  G loss  0.0469408\n",
      "Epoch  3316  G loss  0.1397712\n",
      "Epoch  3317  G loss  0.03847902\n",
      "Epoch  3318  G loss  0.113895245\n",
      "Epoch  3319  G loss  0.057945535\n",
      "Epoch  3320  G loss  0.006402009\n",
      "Epoch  3321  G loss  0.017863708\n",
      "Epoch  3322  G loss  0.012107652\n",
      "Epoch  3323  G loss  0.025547747\n",
      "Epoch  3324  G loss  0.10658911\n",
      "Epoch  3325  G loss  0.031161025\n",
      "Epoch  3326  G loss  0.007516641\n",
      "Epoch  3327  G loss  0.022742318\n",
      "Epoch  3328  G loss  0.013695948\n",
      "Epoch  3329  G loss  0.21875477\n",
      "Epoch  3330  G loss  0.008662042\n",
      "Epoch  3331  G loss  0.04948128\n",
      "Epoch  3332  G loss  0.09205581\n",
      "Epoch  3333  G loss  0.030269468\n",
      "Epoch  3334  G loss  0.13126147\n",
      "Epoch  3335  G loss  0.08882255\n",
      "Epoch  3336  G loss  0.006402101\n",
      "Epoch  3337  G loss  0.03419627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  3338  G loss  0.037984803\n",
      "Epoch  3339  G loss  0.08787904\n",
      "Epoch  3340  G loss  0.016522754\n",
      "Epoch  3341  G loss  0.051988468\n",
      "Epoch  3342  G loss  0.020858202\n",
      "Epoch  3343  G loss  0.110404074\n",
      "Epoch  3344  G loss  0.06791991\n",
      "Epoch  3345  G loss  0.032510504\n",
      "Epoch  3346  G loss  0.039963253\n",
      "Epoch  3347  G loss  0.054989308\n",
      "Epoch  3348  G loss  0.0104301665\n",
      "Epoch  3349  G loss  0.02380091\n",
      "Epoch  3350  G loss  0.014333145\n",
      "Epoch  3351  G loss  0.23314816\n",
      "Epoch  3352  G loss  0.017614743\n",
      "Epoch  3353  G loss  0.07099232\n",
      "Epoch  3354  G loss  0.05091721\n",
      "Epoch  3355  G loss  0.14144519\n",
      "Epoch  3356  G loss  0.011802204\n",
      "Epoch  3357  G loss  0.07344015\n",
      "Epoch  3358  G loss  0.09204511\n",
      "Epoch  3359  G loss  0.0145319505\n",
      "Epoch  3360  G loss  0.028480712\n",
      "Epoch  3361  G loss  0.014984064\n",
      "Epoch  3362  G loss  0.016382143\n",
      "Epoch  3363  G loss  0.067453936\n",
      "Epoch  3364  G loss  0.09492026\n",
      "Epoch  3365  G loss  0.10309242\n",
      "Epoch  3366  G loss  0.00755237\n",
      "Epoch  3367  G loss  0.085447855\n",
      "Epoch  3368  G loss  0.015634544\n",
      "Epoch  3369  G loss  0.053050242\n",
      "Epoch  3370  G loss  0.022306746\n",
      "Epoch  3371  G loss  0.008405166\n",
      "Epoch  3372  G loss  0.020059317\n",
      "Epoch  3373  G loss  0.03727249\n",
      "Epoch  3374  G loss  0.11153272\n",
      "Epoch  3375  G loss  0.08543483\n",
      "Epoch  3376  G loss  0.18829536\n",
      "Epoch  3377  G loss  0.0067194346\n",
      "Epoch  3378  G loss  0.028268985\n",
      "Epoch  3379  G loss  0.0053334716\n",
      "Epoch  3380  G loss  0.017915212\n",
      "Epoch  3381  G loss  0.02739089\n",
      "Epoch  3382  G loss  0.039160144\n",
      "Epoch  3383  G loss  0.019751418\n",
      "Epoch  3384  G loss  0.045992404\n",
      "Epoch  3385  G loss  0.020109927\n",
      "Epoch  3386  G loss  0.02075101\n",
      "Epoch  3387  G loss  0.018505877\n",
      "Epoch  3388  G loss  0.013994705\n",
      "Epoch  3389  G loss  0.031022247\n",
      "Epoch  3390  G loss  0.0050453097\n",
      "Epoch  3391  G loss  0.02474233\n",
      "Epoch  3392  G loss  0.02262083\n",
      "Epoch  3393  G loss  0.028595995\n",
      "Epoch  3394  G loss  0.0598043\n",
      "Epoch  3395  G loss  0.09220602\n",
      "Epoch  3396  G loss  0.008340158\n",
      "Epoch  3397  G loss  0.14551967\n",
      "Epoch  3398  G loss  0.058805425\n",
      "Epoch  3399  G loss  0.016007062\n",
      "Epoch  3400  G loss  0.04827503\n",
      "Epoch  3401  G loss  0.008617606\n",
      "Epoch  3402  G loss  0.017575616\n",
      "Epoch  3403  G loss  0.01888331\n",
      "Epoch  3404  G loss  0.008836006\n",
      "Epoch  3405  G loss  0.024208188\n",
      "Epoch  3406  G loss  0.042247403\n",
      "Epoch  3407  G loss  0.038646616\n",
      "Epoch  3408  G loss  0.120011955\n",
      "Epoch  3409  G loss  0.07718615\n",
      "Epoch  3410  G loss  0.020539334\n",
      "Epoch  3411  G loss  0.019614423\n",
      "Epoch  3412  G loss  0.014915721\n",
      "Epoch  3413  G loss  0.014100287\n",
      "Epoch  3414  G loss  0.04230254\n",
      "Epoch  3415  G loss  0.009243008\n",
      "Epoch  3416  G loss  0.018787876\n",
      "Epoch  3417  G loss  0.077776685\n",
      "Epoch  3418  G loss  0.0064586625\n",
      "Epoch  3419  G loss  0.016197031\n",
      "Epoch  3420  G loss  0.076068595\n",
      "Epoch  3421  G loss  0.009580083\n",
      "Epoch  3422  G loss  0.005450965\n",
      "Epoch  3423  G loss  0.11337729\n",
      "Epoch  3424  G loss  0.102874056\n",
      "Epoch  3425  G loss  0.027721953\n",
      "Epoch  3426  G loss  0.007349\n",
      "Epoch  3427  G loss  0.06776829\n",
      "Epoch  3428  G loss  0.057359975\n",
      "Epoch  3429  G loss  0.20930031\n",
      "Epoch  3430  G loss  0.04902334\n",
      "Epoch  3431  G loss  0.024483107\n",
      "Epoch  3432  G loss  0.0066727237\n",
      "Epoch  3433  G loss  0.02566044\n",
      "Epoch  3434  G loss  0.039289586\n",
      "Epoch  3435  G loss  0.0061701555\n",
      "Epoch  3436  G loss  0.117126495\n",
      "Epoch  3437  G loss  0.14215104\n",
      "Epoch  3438  G loss  0.010328157\n",
      "Epoch  3439  G loss  0.012631774\n",
      "Epoch  3440  G loss  0.019717827\n",
      "Epoch  3441  G loss  0.07789172\n",
      "Epoch  3442  G loss  0.008706997\n",
      "Epoch  3443  G loss  0.031202124\n",
      "Epoch  3444  G loss  0.09641889\n",
      "Epoch  3445  G loss  0.014835778\n",
      "Epoch  3446  G loss  0.015277428\n",
      "Epoch  3447  G loss  0.024543408\n",
      "Epoch  3448  G loss  0.13459472\n",
      "Epoch  3449  G loss  0.024988893\n",
      "Epoch  3450  G loss  0.044163086\n",
      "Epoch  3451  G loss  0.027539192\n",
      "Epoch  3452  G loss  0.023616489\n",
      "Epoch  3453  G loss  0.055826478\n",
      "Epoch  3454  G loss  0.031086536\n",
      "Epoch  3455  G loss  0.009713549\n",
      "Epoch  3456  G loss  0.05693261\n",
      "Epoch  3457  G loss  0.2504074\n",
      "Epoch  3458  G loss  0.010458897\n",
      "Epoch  3459  G loss  0.007322497\n",
      "Epoch  3460  G loss  0.012946051\n",
      "Epoch  3461  G loss  0.029729538\n",
      "Epoch  3462  G loss  0.14370227\n",
      "Epoch  3463  G loss  0.011669112\n",
      "Epoch  3464  G loss  0.01689186\n",
      "Epoch  3465  G loss  0.18991345\n",
      "Epoch  3466  G loss  0.022339534\n",
      "Epoch  3467  G loss  0.06052178\n",
      "Epoch  3468  G loss  0.009251423\n",
      "Epoch  3469  G loss  0.061158337\n",
      "Epoch  3470  G loss  0.028413417\n",
      "Epoch  3471  G loss  0.024059348\n",
      "Epoch  3472  G loss  0.07228987\n",
      "Epoch  3473  G loss  0.023954354\n",
      "Epoch  3474  G loss  0.0086385235\n",
      "Epoch  3475  G loss  0.011519015\n",
      "Epoch  3476  G loss  0.028572433\n",
      "Epoch  3477  G loss  0.019492393\n",
      "Epoch  3478  G loss  0.079665065\n",
      "Epoch  3479  G loss  0.07821499\n",
      "Epoch  3480  G loss  0.047316976\n",
      "Epoch  3481  G loss  0.013721841\n",
      "Epoch  3482  G loss  0.12123106\n",
      "Epoch  3483  G loss  0.054735\n",
      "Epoch  3484  G loss  0.04784\n",
      "Epoch  3485  G loss  0.014504871\n",
      "Epoch  3486  G loss  0.04180499\n",
      "Epoch  3487  G loss  0.18063489\n",
      "Epoch  3488  G loss  0.021566542\n",
      "Epoch  3489  G loss  0.14104894\n",
      "Epoch  3490  G loss  0.025396623\n",
      "Epoch  3491  G loss  0.02193876\n",
      "Epoch  3492  G loss  0.060996097\n",
      "Epoch  3493  G loss  0.09936685\n",
      "Epoch  3494  G loss  0.014860561\n",
      "Epoch  3495  G loss  0.045401163\n",
      "Epoch  3496  G loss  0.051920824\n",
      "Epoch  3497  G loss  0.027455827\n",
      "Epoch  3498  G loss  0.026828952\n",
      "Epoch  3499  G loss  0.014660122\n",
      "Epoch  3500  G loss  0.081877366\n",
      "Epoch  3501  G loss  0.06526838\n",
      "Epoch  3502  G loss  0.100357\n",
      "Epoch  3503  G loss  0.2435574\n",
      "Epoch  3504  G loss  0.076393425\n",
      "Epoch  3505  G loss  0.019809749\n",
      "Epoch  3506  G loss  0.031987913\n",
      "Epoch  3507  G loss  0.015324715\n",
      "Epoch  3508  G loss  0.105016366\n",
      "Epoch  3509  G loss  0.05433561\n",
      "Epoch  3510  G loss  0.09460927\n",
      "Epoch  3511  G loss  0.042957164\n",
      "Epoch  3512  G loss  0.04841432\n",
      "Epoch  3513  G loss  0.014233321\n",
      "Epoch  3514  G loss  0.10694407\n",
      "Epoch  3515  G loss  0.021278678\n",
      "Epoch  3516  G loss  0.118710846\n",
      "Epoch  3517  G loss  0.01152997\n",
      "Epoch  3518  G loss  0.012056014\n",
      "Epoch  3519  G loss  0.013462364\n",
      "Epoch  3520  G loss  0.00726231\n",
      "Epoch  3521  G loss  0.06513315\n",
      "Epoch  3522  G loss  0.005389521\n",
      "Epoch  3523  G loss  0.0434534\n",
      "Epoch  3524  G loss  0.017196424\n",
      "Epoch  3525  G loss  0.0063248314\n",
      "Epoch  3526  G loss  0.1969923\n",
      "Epoch  3527  G loss  0.022488633\n",
      "Epoch  3528  G loss  0.015326543\n",
      "Epoch  3529  G loss  0.009776564\n",
      "Epoch  3530  G loss  0.015232584\n",
      "Epoch  3531  G loss  0.05019986\n",
      "Epoch  3532  G loss  0.0067081964\n",
      "Epoch  3533  G loss  0.06693305\n",
      "Epoch  3534  G loss  0.015616101\n",
      "Epoch  3535  G loss  0.024842113\n",
      "Epoch  3536  G loss  0.17188118\n",
      "Epoch  3537  G loss  0.013444776\n",
      "Epoch  3538  G loss  0.023744728\n",
      "Epoch  3539  G loss  0.020526355\n",
      "Epoch  3540  G loss  0.008200368\n",
      "Epoch  3541  G loss  0.014896429\n",
      "Epoch  3542  G loss  0.024453003\n",
      "Epoch  3543  G loss  0.013795394\n",
      "Epoch  3544  G loss  0.038390853\n",
      "Epoch  3545  G loss  0.14463483\n",
      "Epoch  3546  G loss  0.060131084\n",
      "Epoch  3547  G loss  0.023391396\n",
      "Epoch  3548  G loss  0.05362552\n",
      "Epoch  3549  G loss  0.016347436\n",
      "Epoch  3550  G loss  0.028973676\n",
      "Epoch  3551  G loss  0.018395891\n",
      "Epoch  3552  G loss  0.016022872\n",
      "Epoch  3553  G loss  0.022661842\n",
      "Epoch  3554  G loss  0.066663824\n",
      "Epoch  3555  G loss  0.007838612\n",
      "Epoch  3556  G loss  0.06171656\n",
      "Epoch  3557  G loss  0.017044235\n",
      "Epoch  3558  G loss  0.01193485\n",
      "Epoch  3559  G loss  0.05962468\n",
      "Epoch  3560  G loss  0.057766397\n",
      "Epoch  3561  G loss  0.095218346\n",
      "Epoch  3562  G loss  0.01203729\n",
      "Epoch  3563  G loss  0.046088032\n",
      "Epoch  3564  G loss  0.031932175\n",
      "Epoch  3565  G loss  0.0062031937\n",
      "Epoch  3566  G loss  0.30930305\n",
      "Epoch  3567  G loss  0.0074685407\n",
      "Epoch  3568  G loss  0.039512724\n",
      "Epoch  3569  G loss  0.043219477\n",
      "Epoch  3570  G loss  0.014709477\n",
      "Epoch  3571  G loss  0.050249934\n",
      "Epoch  3572  G loss  0.05126465\n",
      "Epoch  3573  G loss  0.010291301\n",
      "Epoch  3574  G loss  0.033571415\n",
      "Epoch  3575  G loss  0.011787788\n",
      "Epoch  3576  G loss  0.16765004\n",
      "Epoch  3577  G loss  0.01744847\n",
      "Epoch  3578  G loss  0.08720273\n",
      "Epoch  3579  G loss  0.017656308\n",
      "Epoch  3580  G loss  0.012423219\n",
      "Epoch  3581  G loss  0.053558707\n",
      "Epoch  3582  G loss  0.043249175\n",
      "Epoch  3583  G loss  0.02310999\n",
      "Epoch  3584  G loss  0.34647417\n",
      "Epoch  3585  G loss  0.109125465\n",
      "Epoch  3586  G loss  0.009461147\n",
      "Epoch  3587  G loss  0.02587849\n",
      "Epoch  3588  G loss  0.025461748\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  3589  G loss  0.026898459\n",
      "Epoch  3590  G loss  0.013693788\n",
      "Epoch  3591  G loss  0.04672484\n",
      "Epoch  3592  G loss  0.004411521\n",
      "Epoch  3593  G loss  0.081070274\n",
      "Epoch  3594  G loss  0.023911987\n",
      "Epoch  3595  G loss  0.029821906\n",
      "Epoch  3596  G loss  0.00868801\n",
      "Epoch  3597  G loss  0.13184977\n",
      "Epoch  3598  G loss  0.030595727\n",
      "Epoch  3599  G loss  0.10267229\n",
      "Epoch  3600  G loss  0.03046291\n",
      "Epoch  3601  G loss  0.052467667\n",
      "Epoch  3602  G loss  0.045732986\n",
      "Epoch  3603  G loss  0.082210094\n",
      "Epoch  3604  G loss  0.025343554\n",
      "Epoch  3605  G loss  0.17961577\n",
      "Epoch  3606  G loss  0.10074152\n",
      "Epoch  3607  G loss  0.028282505\n",
      "Epoch  3608  G loss  0.098816596\n",
      "Epoch  3609  G loss  0.0064406414\n",
      "Epoch  3610  G loss  0.014806569\n",
      "Epoch  3611  G loss  0.038917527\n",
      "Epoch  3612  G loss  0.016916811\n",
      "Epoch  3613  G loss  0.022693312\n",
      "Epoch  3614  G loss  0.039133303\n",
      "Epoch  3615  G loss  0.05740099\n",
      "Epoch  3616  G loss  0.020040039\n",
      "Epoch  3617  G loss  0.09723373\n",
      "Epoch  3618  G loss  0.09323309\n",
      "Epoch  3619  G loss  0.012062695\n",
      "Epoch  3620  G loss  0.074941486\n",
      "Epoch  3621  G loss  0.106703795\n",
      "Epoch  3622  G loss  0.029689275\n",
      "Epoch  3623  G loss  0.039752543\n",
      "Epoch  3624  G loss  0.037471578\n",
      "Epoch  3625  G loss  0.0089278845\n",
      "Epoch  3626  G loss  0.04492283\n",
      "Epoch  3627  G loss  0.030021872\n",
      "Epoch  3628  G loss  0.006852351\n",
      "Epoch  3629  G loss  0.010083426\n",
      "Epoch  3630  G loss  0.073174685\n",
      "Epoch  3631  G loss  0.019674536\n",
      "Epoch  3632  G loss  0.00773972\n",
      "Epoch  3633  G loss  0.40954065\n",
      "Epoch  3634  G loss  0.0063923197\n",
      "Epoch  3635  G loss  0.15181127\n",
      "Epoch  3636  G loss  0.03408911\n",
      "Epoch  3637  G loss  0.045412835\n",
      "Epoch  3638  G loss  0.06836072\n",
      "Epoch  3639  G loss  0.019447915\n",
      "Epoch  3640  G loss  0.0071287253\n",
      "Epoch  3641  G loss  0.03576108\n",
      "Epoch  3642  G loss  0.069209516\n",
      "Epoch  3643  G loss  0.026003923\n",
      "Epoch  3644  G loss  0.040792897\n",
      "Epoch  3645  G loss  0.040536348\n",
      "Epoch  3646  G loss  0.04896091\n",
      "Epoch  3647  G loss  0.018054929\n",
      "Epoch  3648  G loss  0.14146015\n",
      "Epoch  3649  G loss  0.02598257\n",
      "Epoch  3650  G loss  0.10516063\n",
      "Epoch  3651  G loss  0.060662434\n",
      "Epoch  3652  G loss  0.10194723\n",
      "Epoch  3653  G loss  0.013061448\n",
      "Epoch  3654  G loss  0.022252945\n",
      "Epoch  3655  G loss  0.02382145\n",
      "Epoch  3656  G loss  0.0060505834\n",
      "Epoch  3657  G loss  0.13508998\n",
      "Epoch  3658  G loss  0.266694\n",
      "Epoch  3659  G loss  0.016357748\n",
      "Epoch  3660  G loss  0.07892242\n",
      "Epoch  3661  G loss  0.040106714\n",
      "Epoch  3662  G loss  0.011829557\n",
      "Epoch  3663  G loss  0.022299843\n",
      "Epoch  3664  G loss  0.016879272\n",
      "Epoch  3665  G loss  0.012504159\n",
      "Epoch  3666  G loss  0.008876193\n",
      "Epoch  3667  G loss  0.009365946\n",
      "Epoch  3668  G loss  0.012765203\n",
      "Epoch  3669  G loss  0.052105658\n",
      "Epoch  3670  G loss  0.0069595138\n",
      "Epoch  3671  G loss  0.022924783\n",
      "Epoch  3672  G loss  0.076046206\n",
      "Epoch  3673  G loss  0.005641691\n",
      "Epoch  3674  G loss  0.025036301\n",
      "Epoch  3675  G loss  0.05749812\n",
      "Epoch  3676  G loss  0.035428617\n",
      "Epoch  3677  G loss  0.026035845\n",
      "Epoch  3678  G loss  0.033542395\n",
      "Epoch  3679  G loss  0.01576383\n",
      "Epoch  3680  G loss  0.030849688\n",
      "Epoch  3681  G loss  0.047079414\n",
      "Epoch  3682  G loss  0.008136728\n",
      "Epoch  3683  G loss  0.024600334\n",
      "Epoch  3684  G loss  0.005194661\n",
      "Epoch  3685  G loss  0.00958922\n",
      "Epoch  3686  G loss  0.101677254\n",
      "Epoch  3687  G loss  0.052263826\n",
      "Epoch  3688  G loss  0.018595915\n",
      "Epoch  3689  G loss  0.0855726\n",
      "Epoch  3690  G loss  0.009215521\n",
      "Epoch  3691  G loss  0.1696942\n",
      "Epoch  3692  G loss  0.055721708\n",
      "Epoch  3693  G loss  0.10510893\n",
      "Epoch  3694  G loss  0.034807358\n",
      "Epoch  3695  G loss  0.049097978\n",
      "Epoch  3696  G loss  0.056034155\n",
      "Epoch  3697  G loss  0.010182412\n",
      "Epoch  3698  G loss  0.012705674\n",
      "Epoch  3699  G loss  0.10189672\n",
      "Epoch  3700  G loss  0.023622682\n",
      "Epoch  3701  G loss  0.008730479\n",
      "Epoch  3702  G loss  0.009904262\n",
      "Epoch  3703  G loss  0.007881813\n",
      "Epoch  3704  G loss  0.15420195\n",
      "Epoch  3705  G loss  0.08064531\n",
      "Epoch  3706  G loss  0.01406512\n",
      "Epoch  3707  G loss  0.15575185\n",
      "Epoch  3708  G loss  0.024281874\n",
      "Epoch  3709  G loss  0.018415377\n",
      "Epoch  3710  G loss  0.050167367\n",
      "Epoch  3711  G loss  0.1064714\n",
      "Epoch  3712  G loss  0.039628115\n",
      "Epoch  3713  G loss  0.06871127\n",
      "Epoch  3714  G loss  0.23268828\n",
      "Epoch  3715  G loss  0.009713812\n",
      "Epoch  3716  G loss  0.022797458\n",
      "Epoch  3717  G loss  0.0130479755\n",
      "Epoch  3718  G loss  0.05987488\n",
      "Epoch  3719  G loss  0.06710282\n",
      "Epoch  3720  G loss  0.007963723\n",
      "Epoch  3721  G loss  0.025836973\n",
      "Epoch  3722  G loss  0.011818403\n",
      "Epoch  3723  G loss  0.122138694\n",
      "Epoch  3724  G loss  0.036393642\n",
      "Epoch  3725  G loss  0.015255682\n",
      "Epoch  3726  G loss  0.0124210585\n",
      "Epoch  3727  G loss  0.0065942295\n",
      "Epoch  3728  G loss  0.10695936\n",
      "Epoch  3729  G loss  0.081503324\n",
      "Epoch  3730  G loss  0.005548615\n",
      "Epoch  3731  G loss  0.01239162\n",
      "Epoch  3732  G loss  0.0156042995\n",
      "Epoch  3733  G loss  0.012168308\n",
      "Epoch  3734  G loss  0.008006224\n",
      "Epoch  3735  G loss  0.064335436\n",
      "Epoch  3736  G loss  0.0130037945\n",
      "Epoch  3737  G loss  0.39275026\n",
      "Epoch  3738  G loss  0.028312668\n",
      "Epoch  3739  G loss  0.036381327\n",
      "Epoch  3740  G loss  0.018277306\n",
      "Epoch  3741  G loss  0.08653362\n",
      "Epoch  3742  G loss  0.011848232\n",
      "Epoch  3743  G loss  0.07813783\n",
      "Epoch  3744  G loss  0.05446534\n",
      "Epoch  3745  G loss  0.015988458\n",
      "Epoch  3746  G loss  0.10485329\n",
      "Epoch  3747  G loss  0.01082497\n",
      "Epoch  3748  G loss  0.15108243\n",
      "Epoch  3749  G loss  0.047389083\n",
      "Epoch  3750  G loss  0.2324546\n",
      "Epoch  3751  G loss  0.1165989\n",
      "Epoch  3752  G loss  0.11591295\n",
      "Epoch  3753  G loss  0.022305856\n",
      "Epoch  3754  G loss  0.010357972\n",
      "Epoch  3755  G loss  0.069731176\n",
      "Epoch  3756  G loss  0.016092956\n",
      "Epoch  3757  G loss  0.0069476776\n",
      "Epoch  3758  G loss  0.03996022\n",
      "Epoch  3759  G loss  0.08369622\n",
      "Epoch  3760  G loss  0.085695356\n",
      "Epoch  3761  G loss  0.047803834\n",
      "Epoch  3762  G loss  0.008341441\n",
      "Epoch  3763  G loss  0.052801713\n",
      "Epoch  3764  G loss  0.06867646\n",
      "Epoch  3765  G loss  0.055850126\n",
      "Epoch  3766  G loss  0.025976926\n",
      "Epoch  3767  G loss  0.08721748\n",
      "Epoch  3768  G loss  0.020282906\n",
      "Epoch  3769  G loss  0.084083065\n",
      "Epoch  3770  G loss  0.008124082\n",
      "Epoch  3771  G loss  0.033477187\n",
      "Epoch  3772  G loss  0.023097686\n",
      "Epoch  3773  G loss  0.017479185\n",
      "Epoch  3774  G loss  0.015693143\n",
      "Epoch  3775  G loss  0.023153637\n",
      "Epoch  3776  G loss  0.053498212\n",
      "Epoch  3777  G loss  0.014014808\n",
      "Epoch  3778  G loss  0.06450005\n",
      "Epoch  3779  G loss  0.027912514\n",
      "Epoch  3780  G loss  0.13783929\n",
      "Epoch  3781  G loss  0.006826019\n",
      "Epoch  3782  G loss  0.031002045\n",
      "Epoch  3783  G loss  0.018879889\n",
      "Epoch  3784  G loss  0.110903904\n",
      "Epoch  3785  G loss  0.027752783\n",
      "Epoch  3786  G loss  0.051270418\n",
      "Epoch  3787  G loss  0.04349346\n",
      "Epoch  3788  G loss  0.014953664\n",
      "Epoch  3789  G loss  0.038593344\n",
      "Epoch  3790  G loss  0.17483991\n",
      "Epoch  3791  G loss  0.02600883\n",
      "Epoch  3792  G loss  0.032488964\n",
      "Epoch  3793  G loss  0.035461284\n",
      "Epoch  3794  G loss  0.014712478\n",
      "Epoch  3795  G loss  0.10324055\n",
      "Epoch  3796  G loss  0.026834648\n",
      "Epoch  3797  G loss  0.08585757\n",
      "Epoch  3798  G loss  0.018536888\n",
      "Epoch  3799  G loss  0.0888983\n",
      "Epoch  3800  G loss  0.17511748\n",
      "Epoch  3801  G loss  0.012994338\n",
      "Epoch  3802  G loss  0.021073066\n",
      "Epoch  3803  G loss  0.080133036\n",
      "Epoch  3804  G loss  0.031483732\n",
      "Epoch  3805  G loss  0.06268148\n",
      "Epoch  3806  G loss  0.17097881\n",
      "Epoch  3807  G loss  0.023945741\n",
      "Epoch  3808  G loss  0.032485947\n",
      "Epoch  3809  G loss  0.010926259\n",
      "Epoch  3810  G loss  0.018352572\n",
      "Epoch  3811  G loss  0.015480662\n",
      "Epoch  3812  G loss  0.17443547\n",
      "Epoch  3813  G loss  0.14079964\n",
      "Epoch  3814  G loss  0.0062262462\n",
      "Epoch  3815  G loss  0.06715022\n",
      "Epoch  3816  G loss  0.013713988\n",
      "Epoch  3817  G loss  0.050720315\n",
      "Epoch  3818  G loss  0.035008818\n",
      "Epoch  3819  G loss  0.024721228\n",
      "Epoch  3820  G loss  0.006251378\n",
      "Epoch  3821  G loss  0.013877864\n",
      "Epoch  3822  G loss  0.0230139\n",
      "Epoch  3823  G loss  0.013124445\n",
      "Epoch  3824  G loss  0.025322974\n",
      "Epoch  3825  G loss  0.022020252\n",
      "Epoch  3826  G loss  0.0066211433\n",
      "Epoch  3827  G loss  0.045085028\n",
      "Epoch  3828  G loss  0.015686406\n",
      "Epoch  3829  G loss  0.054042663\n",
      "Epoch  3830  G loss  0.10411395\n",
      "Epoch  3831  G loss  0.042833038\n",
      "Epoch  3832  G loss  0.013481129\n",
      "Epoch  3833  G loss  0.03181038\n",
      "Epoch  3834  G loss  0.19099513\n",
      "Epoch  3835  G loss  0.0070792334\n",
      "Epoch  3836  G loss  0.006315411\n",
      "Epoch  3837  G loss  0.0398183\n",
      "Epoch  3838  G loss  0.07820439\n",
      "Epoch  3839  G loss  0.23116721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  3840  G loss  0.03230958\n",
      "Epoch  3841  G loss  0.16499838\n",
      "Epoch  3842  G loss  0.15785202\n",
      "Epoch  3843  G loss  0.063796125\n",
      "Epoch  3844  G loss  0.012460843\n",
      "Epoch  3845  G loss  0.01863642\n",
      "Epoch  3846  G loss  0.061864804\n",
      "Epoch  3847  G loss  0.0067614373\n",
      "Epoch  3848  G loss  0.009355008\n",
      "Epoch  3849  G loss  0.01043581\n",
      "Epoch  3850  G loss  0.026881307\n",
      "Epoch  3851  G loss  0.01447629\n",
      "Epoch  3852  G loss  0.014801953\n",
      "Epoch  3853  G loss  0.062765256\n",
      "Epoch  3854  G loss  0.026317442\n",
      "Epoch  3855  G loss  0.03751993\n",
      "Epoch  3856  G loss  0.19559352\n",
      "Epoch  3857  G loss  0.0923194\n",
      "Epoch  3858  G loss  0.017017324\n",
      "Epoch  3859  G loss  0.16760263\n",
      "Epoch  3860  G loss  0.013798181\n",
      "Epoch  3861  G loss  0.02678893\n",
      "Epoch  3862  G loss  0.040821046\n",
      "Epoch  3863  G loss  0.052187104\n",
      "Epoch  3864  G loss  0.038994264\n",
      "Epoch  3865  G loss  0.01731753\n",
      "Epoch  3866  G loss  0.015171469\n",
      "Epoch  3867  G loss  0.04076652\n",
      "Epoch  3868  G loss  0.06262968\n",
      "Epoch  3869  G loss  0.03449841\n",
      "Epoch  3870  G loss  0.0119436495\n",
      "Epoch  3871  G loss  0.0131987985\n",
      "Epoch  3872  G loss  0.28236338\n",
      "Epoch  3873  G loss  0.07597375\n",
      "Epoch  3874  G loss  0.023412026\n",
      "Epoch  3875  G loss  0.005902068\n",
      "Epoch  3876  G loss  0.022104379\n",
      "Epoch  3877  G loss  0.01721592\n",
      "Epoch  3878  G loss  0.035478663\n",
      "Epoch  3879  G loss  0.032244343\n",
      "Epoch  3880  G loss  0.005995902\n",
      "Epoch  3881  G loss  0.047171444\n",
      "Epoch  3882  G loss  0.017369848\n",
      "Epoch  3883  G loss  0.11007302\n",
      "Epoch  3884  G loss  0.009704176\n",
      "Epoch  3885  G loss  0.009815427\n",
      "Epoch  3886  G loss  0.008038541\n",
      "Epoch  3887  G loss  0.07562657\n",
      "Epoch  3888  G loss  0.0239957\n",
      "Epoch  3889  G loss  0.032488786\n",
      "Epoch  3890  G loss  0.008816425\n",
      "Epoch  3891  G loss  0.007143158\n",
      "Epoch  3892  G loss  0.043402188\n",
      "Epoch  3893  G loss  0.19430673\n",
      "Epoch  3894  G loss  0.015438676\n",
      "Epoch  3895  G loss  0.027559463\n",
      "Epoch  3896  G loss  0.3227082\n",
      "Epoch  3897  G loss  0.17840497\n",
      "Epoch  3898  G loss  0.049777873\n",
      "Epoch  3899  G loss  0.1438284\n",
      "Epoch  3900  G loss  0.043372378\n",
      "Epoch  3901  G loss  0.022344036\n",
      "Epoch  3902  G loss  0.0136503\n",
      "Epoch  3903  G loss  0.009181989\n",
      "Epoch  3904  G loss  0.009971999\n",
      "Epoch  3905  G loss  0.054560617\n",
      "Epoch  3906  G loss  0.046252005\n",
      "Epoch  3907  G loss  0.0068523968\n",
      "Epoch  3908  G loss  0.044085987\n",
      "Epoch  3909  G loss  0.28661266\n",
      "Epoch  3910  G loss  0.009083075\n",
      "Epoch  3911  G loss  0.092629775\n",
      "Epoch  3912  G loss  0.023262613\n",
      "Epoch  3913  G loss  0.034884907\n",
      "Epoch  3914  G loss  0.11941439\n",
      "Epoch  3915  G loss  0.010925334\n",
      "Epoch  3916  G loss  0.006531368\n",
      "Epoch  3917  G loss  0.014748375\n",
      "Epoch  3918  G loss  0.026469402\n",
      "Epoch  3919  G loss  0.026422761\n",
      "Epoch  3920  G loss  0.0059124692\n",
      "Epoch  3921  G loss  0.02331471\n",
      "Epoch  3922  G loss  0.052876912\n",
      "Epoch  3923  G loss  0.10595824\n",
      "Epoch  3924  G loss  0.21390387\n",
      "Epoch  3925  G loss  0.017099114\n",
      "Epoch  3926  G loss  0.0061462168\n",
      "Epoch  3927  G loss  0.12220581\n",
      "Epoch  3928  G loss  0.080412306\n",
      "Epoch  3929  G loss  0.010053733\n",
      "Epoch  3930  G loss  0.025133025\n",
      "Epoch  3931  G loss  0.02041654\n",
      "Epoch  3932  G loss  0.09711841\n",
      "Epoch  3933  G loss  0.019677466\n",
      "Epoch  3934  G loss  0.01347328\n",
      "Epoch  3935  G loss  0.06243251\n",
      "Epoch  3936  G loss  0.02239507\n",
      "Epoch  3937  G loss  0.006603691\n",
      "Epoch  3938  G loss  0.019057266\n",
      "Epoch  3939  G loss  0.07480602\n",
      "Epoch  3940  G loss  0.01811388\n",
      "Epoch  3941  G loss  0.21293059\n",
      "Epoch  3942  G loss  0.01698329\n",
      "Epoch  3943  G loss  0.009794595\n",
      "Epoch  3944  G loss  0.12845574\n",
      "Epoch  3945  G loss  0.035913385\n",
      "Epoch  3946  G loss  0.026652753\n",
      "Epoch  3947  G loss  0.032490045\n",
      "Epoch  3948  G loss  0.041366167\n",
      "Epoch  3949  G loss  0.026664106\n",
      "Epoch  3950  G loss  0.013440743\n",
      "Epoch  3951  G loss  0.010623464\n",
      "Epoch  3952  G loss  0.05434303\n",
      "Epoch  3953  G loss  0.07207092\n",
      "Epoch  3954  G loss  0.04406085\n",
      "Epoch  3955  G loss  0.02388351\n",
      "Epoch  3956  G loss  0.074696496\n",
      "Epoch  3957  G loss  0.028578714\n",
      "Epoch  3958  G loss  0.18503687\n",
      "Epoch  3959  G loss  0.07018851\n",
      "Epoch  3960  G loss  0.015203226\n",
      "Epoch  3961  G loss  0.23209777\n",
      "Epoch  3962  G loss  0.037097026\n",
      "Epoch  3963  G loss  0.007606356\n",
      "Epoch  3964  G loss  0.019784626\n",
      "Epoch  3965  G loss  0.0723937\n",
      "Epoch  3966  G loss  0.04656309\n",
      "Epoch  3967  G loss  0.028788643\n",
      "Epoch  3968  G loss  0.055952247\n",
      "Epoch  3969  G loss  0.019339625\n",
      "Epoch  3970  G loss  0.025870604\n",
      "Epoch  3971  G loss  0.10157038\n",
      "Epoch  3972  G loss  0.016567364\n",
      "Epoch  3973  G loss  0.023330498\n",
      "Epoch  3974  G loss  0.024986364\n",
      "Epoch  3975  G loss  0.016416762\n",
      "Epoch  3976  G loss  0.009002748\n",
      "Epoch  3977  G loss  0.17276657\n",
      "Epoch  3978  G loss  0.025307778\n",
      "Epoch  3979  G loss  0.013076023\n",
      "Epoch  3980  G loss  0.025644269\n",
      "Epoch  3981  G loss  0.098529\n",
      "Epoch  3982  G loss  0.11094425\n",
      "Epoch  3983  G loss  0.008377861\n",
      "Epoch  3984  G loss  0.040259454\n",
      "Epoch  3985  G loss  0.012500663\n",
      "Epoch  3986  G loss  0.013001019\n",
      "Epoch  3987  G loss  0.04891196\n",
      "Epoch  3988  G loss  0.07066571\n",
      "Epoch  3989  G loss  0.05301846\n",
      "Epoch  3990  G loss  0.036207013\n",
      "Epoch  3991  G loss  0.024805244\n",
      "Epoch  3992  G loss  0.015990429\n",
      "Epoch  3993  G loss  0.016782148\n",
      "Epoch  3994  G loss  0.08527225\n",
      "Epoch  3995  G loss  0.0630024\n",
      "Epoch  3996  G loss  0.114630744\n",
      "Epoch  3997  G loss  0.04658726\n",
      "Epoch  3998  G loss  0.021813225\n",
      "Epoch  3999  G loss  0.117576845\n",
      "Epoch  4000  G loss  0.07391887\n",
      "Epoch  4001  G loss  0.0960388\n",
      "Epoch  4002  G loss  0.041449264\n",
      "Epoch  4003  G loss  0.007957312\n",
      "Epoch  4004  G loss  0.022387438\n",
      "Epoch  4005  G loss  0.014299064\n",
      "Epoch  4006  G loss  0.010738298\n",
      "Epoch  4007  G loss  0.008850446\n",
      "Epoch  4008  G loss  0.084348306\n",
      "Epoch  4009  G loss  0.05362218\n",
      "Epoch  4010  G loss  0.25326264\n",
      "Epoch  4011  G loss  0.03191478\n",
      "Epoch  4012  G loss  0.29979903\n",
      "Epoch  4013  G loss  0.021662235\n",
      "Epoch  4014  G loss  0.03180253\n",
      "Epoch  4015  G loss  0.019582879\n",
      "Epoch  4016  G loss  0.037822887\n",
      "Epoch  4017  G loss  0.18964761\n",
      "Epoch  4018  G loss  0.010416592\n",
      "Epoch  4019  G loss  0.082421854\n",
      "Epoch  4020  G loss  0.06134894\n",
      "Epoch  4021  G loss  0.024680026\n",
      "Epoch  4022  G loss  0.036844492\n",
      "Epoch  4023  G loss  0.10690307\n",
      "Epoch  4024  G loss  0.119302675\n",
      "Epoch  4025  G loss  0.010892022\n",
      "Epoch  4026  G loss  0.04495717\n",
      "Epoch  4027  G loss  0.1343105\n",
      "Epoch  4028  G loss  0.015933687\n",
      "Epoch  4029  G loss  0.031934947\n",
      "Epoch  4030  G loss  0.20493896\n",
      "Epoch  4031  G loss  0.04472357\n",
      "Epoch  4032  G loss  0.012463383\n",
      "Epoch  4033  G loss  0.046762682\n",
      "Epoch  4034  G loss  0.013013424\n",
      "Epoch  4035  G loss  0.022482753\n",
      "Epoch  4036  G loss  0.037053756\n",
      "Epoch  4037  G loss  0.0061528524\n",
      "Epoch  4038  G loss  0.051849607\n",
      "Epoch  4039  G loss  0.02394573\n",
      "Epoch  4040  G loss  0.050415024\n",
      "Epoch  4041  G loss  0.015248677\n",
      "Epoch  4042  G loss  0.014643593\n",
      "Epoch  4043  G loss  0.07695894\n",
      "Epoch  4044  G loss  0.060158607\n",
      "Epoch  4045  G loss  0.0467038\n",
      "Epoch  4046  G loss  0.044720653\n",
      "Epoch  4047  G loss  0.03810692\n",
      "Epoch  4048  G loss  0.033790536\n",
      "Epoch  4049  G loss  0.019183323\n",
      "Epoch  4050  G loss  0.09127973\n",
      "Epoch  4051  G loss  0.020361159\n",
      "Epoch  4052  G loss  0.1880093\n",
      "Epoch  4053  G loss  0.09256371\n",
      "Epoch  4054  G loss  0.08349064\n",
      "Epoch  4055  G loss  0.0079911295\n",
      "Epoch  4056  G loss  0.06491617\n",
      "Epoch  4057  G loss  0.084054895\n",
      "Epoch  4058  G loss  0.18276305\n",
      "Epoch  4059  G loss  0.12760541\n",
      "Epoch  4060  G loss  0.01161702\n",
      "Epoch  4061  G loss  0.021250945\n",
      "Epoch  4062  G loss  0.11259972\n",
      "Epoch  4063  G loss  0.067188196\n",
      "Epoch  4064  G loss  0.15915984\n",
      "Epoch  4065  G loss  0.022151459\n",
      "Epoch  4066  G loss  0.010756262\n",
      "Epoch  4067  G loss  0.047331564\n",
      "Epoch  4068  G loss  0.021061594\n",
      "Epoch  4069  G loss  0.025865514\n",
      "Epoch  4070  G loss  0.054121833\n",
      "Epoch  4071  G loss  0.025772456\n",
      "Epoch  4072  G loss  0.0067904247\n",
      "Epoch  4073  G loss  0.020162635\n",
      "Epoch  4074  G loss  0.024752056\n",
      "Epoch  4075  G loss  0.2533459\n",
      "Epoch  4076  G loss  0.04221053\n",
      "Epoch  4077  G loss  0.043614946\n",
      "Epoch  4078  G loss  0.003000918\n",
      "Epoch  4079  G loss  0.11547908\n",
      "Epoch  4080  G loss  0.047004007\n",
      "Epoch  4081  G loss  0.008259794\n",
      "Epoch  4082  G loss  0.0056771133\n",
      "Epoch  4083  G loss  0.017308172\n",
      "Epoch  4084  G loss  0.07248314\n",
      "Epoch  4085  G loss  0.044830255\n",
      "Epoch  4086  G loss  0.016106214\n",
      "Epoch  4087  G loss  0.049008615\n",
      "Epoch  4088  G loss  0.03171163\n",
      "Epoch  4089  G loss  0.05421522\n",
      "Epoch  4090  G loss  0.014174162\n",
      "Epoch  4091  G loss  0.043786447\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  4092  G loss  0.08617368\n",
      "Epoch  4093  G loss  0.1269841\n",
      "Epoch  4094  G loss  0.01459467\n",
      "Epoch  4095  G loss  0.13461047\n",
      "Epoch  4096  G loss  0.025339598\n",
      "Epoch  4097  G loss  0.025821343\n",
      "Epoch  4098  G loss  0.05408496\n",
      "Epoch  4099  G loss  0.049954396\n",
      "Epoch  4100  G loss  0.008267254\n",
      "Epoch  4101  G loss  0.0285848\n",
      "Epoch  4102  G loss  0.051863562\n",
      "Epoch  4103  G loss  0.04919712\n",
      "Epoch  4104  G loss  0.064056635\n",
      "Epoch  4105  G loss  0.0056013553\n",
      "Epoch  4106  G loss  0.032581173\n",
      "Epoch  4107  G loss  0.010127822\n",
      "Epoch  4108  G loss  0.030476367\n",
      "Epoch  4109  G loss  0.05434393\n",
      "Epoch  4110  G loss  0.04697295\n",
      "Epoch  4111  G loss  0.12138356\n",
      "Epoch  4112  G loss  0.07807225\n",
      "Epoch  4113  G loss  0.04836329\n",
      "Epoch  4114  G loss  0.02002363\n",
      "Epoch  4115  G loss  0.08135\n",
      "Epoch  4116  G loss  0.048857793\n",
      "Epoch  4117  G loss  0.041315094\n",
      "Epoch  4118  G loss  0.04384704\n",
      "Epoch  4119  G loss  0.02539266\n",
      "Epoch  4120  G loss  0.020284273\n",
      "Epoch  4121  G loss  0.009338021\n",
      "Epoch  4122  G loss  0.117771685\n",
      "Epoch  4123  G loss  0.05567283\n",
      "Epoch  4124  G loss  0.02615876\n",
      "Epoch  4125  G loss  0.094063684\n",
      "Epoch  4126  G loss  0.05891548\n",
      "Epoch  4127  G loss  0.100198746\n",
      "Epoch  4128  G loss  0.025824118\n",
      "Epoch  4129  G loss  0.012963716\n",
      "Epoch  4130  G loss  0.0049040955\n",
      "Epoch  4131  G loss  0.14205325\n",
      "Epoch  4132  G loss  0.015639722\n",
      "Epoch  4133  G loss  0.017742293\n",
      "Epoch  4134  G loss  0.04670115\n",
      "Epoch  4135  G loss  0.034963056\n",
      "Epoch  4136  G loss  0.1160482\n",
      "Epoch  4137  G loss  0.03002766\n",
      "Epoch  4138  G loss  0.029221158\n",
      "Epoch  4139  G loss  0.0092167165\n",
      "Epoch  4140  G loss  0.007851677\n",
      "Epoch  4141  G loss  0.0327102\n",
      "Epoch  4142  G loss  0.017808571\n",
      "Epoch  4143  G loss  0.12702614\n",
      "Epoch  4144  G loss  0.19976884\n",
      "Epoch  4145  G loss  0.08341016\n",
      "Epoch  4146  G loss  0.027328499\n",
      "Epoch  4147  G loss  0.020427372\n",
      "Epoch  4148  G loss  0.019385446\n",
      "Epoch  4149  G loss  0.010960918\n",
      "Epoch  4150  G loss  0.028520988\n",
      "Epoch  4151  G loss  0.014055146\n",
      "Epoch  4152  G loss  0.033700526\n",
      "Epoch  4153  G loss  0.008721613\n",
      "Epoch  4154  G loss  0.19943552\n",
      "Epoch  4155  G loss  0.011764191\n",
      "Epoch  4156  G loss  0.042781577\n",
      "Epoch  4157  G loss  0.060078125\n",
      "Epoch  4158  G loss  0.0099336915\n",
      "Epoch  4159  G loss  0.058202937\n",
      "Epoch  4160  G loss  0.012035023\n",
      "Epoch  4161  G loss  0.01728084\n",
      "Epoch  4162  G loss  0.01961369\n",
      "Epoch  4163  G loss  0.052783117\n",
      "Epoch  4164  G loss  0.038907144\n",
      "Epoch  4165  G loss  0.011390571\n",
      "Epoch  4166  G loss  0.008730644\n",
      "Epoch  4167  G loss  0.15118754\n",
      "Epoch  4168  G loss  0.024721574\n",
      "Epoch  4169  G loss  0.12194528\n",
      "Epoch  4170  G loss  0.11852535\n",
      "Epoch  4171  G loss  0.05225148\n",
      "Epoch  4172  G loss  0.02775643\n",
      "Epoch  4173  G loss  0.10694504\n",
      "Epoch  4174  G loss  0.06380272\n",
      "Epoch  4175  G loss  0.012895107\n",
      "Epoch  4176  G loss  0.009786403\n",
      "Epoch  4177  G loss  0.016415048\n",
      "Epoch  4178  G loss  0.0080322\n",
      "Epoch  4179  G loss  0.11077277\n",
      "Epoch  4180  G loss  0.029204361\n",
      "Epoch  4181  G loss  0.18512589\n",
      "Epoch  4182  G loss  0.023363441\n",
      "Epoch  4183  G loss  0.02352824\n",
      "Epoch  4184  G loss  0.06333467\n",
      "Epoch  4185  G loss  0.011515716\n",
      "Epoch  4186  G loss  0.008241272\n",
      "Epoch  4187  G loss  0.2885242\n",
      "Epoch  4188  G loss  0.016354926\n",
      "Epoch  4189  G loss  0.03709358\n",
      "Epoch  4190  G loss  0.018551894\n",
      "Epoch  4191  G loss  0.021542381\n",
      "Epoch  4192  G loss  0.15977597\n",
      "Epoch  4193  G loss  0.05449048\n",
      "Epoch  4194  G loss  0.0133834835\n",
      "Epoch  4195  G loss  0.0117095085\n",
      "Epoch  4196  G loss  0.012557296\n",
      "Epoch  4197  G loss  0.034715418\n",
      "Epoch  4198  G loss  0.08519902\n",
      "Epoch  4199  G loss  0.027660307\n",
      "Epoch  4200  G loss  0.1245407\n",
      "Epoch  4201  G loss  0.040992253\n",
      "Epoch  4202  G loss  0.016462127\n",
      "Epoch  4203  G loss  0.124347925\n",
      "Epoch  4204  G loss  0.026902076\n",
      "Epoch  4205  G loss  0.049203925\n",
      "Epoch  4206  G loss  0.022433538\n",
      "Epoch  4207  G loss  0.033995777\n",
      "Epoch  4208  G loss  0.07552722\n",
      "Epoch  4209  G loss  0.029661395\n",
      "Epoch  4210  G loss  0.017512878\n",
      "Epoch  4211  G loss  0.13904624\n",
      "Epoch  4212  G loss  0.039868813\n",
      "Epoch  4213  G loss  0.028283173\n",
      "Epoch  4214  G loss  0.041768283\n",
      "Epoch  4215  G loss  0.023716316\n",
      "Epoch  4216  G loss  0.045220062\n",
      "Epoch  4217  G loss  0.016365793\n",
      "Epoch  4218  G loss  0.019033877\n",
      "Epoch  4219  G loss  0.041721135\n",
      "Epoch  4220  G loss  0.041711077\n",
      "Epoch  4221  G loss  0.0041150097\n",
      "Epoch  4222  G loss  0.017207036\n",
      "Epoch  4223  G loss  0.010887764\n",
      "Epoch  4224  G loss  0.10157232\n",
      "Epoch  4225  G loss  0.10146507\n",
      "Epoch  4226  G loss  0.20769788\n",
      "Epoch  4227  G loss  0.017606469\n",
      "Epoch  4228  G loss  0.13705215\n",
      "Epoch  4229  G loss  0.1003204\n",
      "Epoch  4230  G loss  0.021772513\n",
      "Epoch  4231  G loss  0.05394937\n",
      "Epoch  4232  G loss  0.09544905\n",
      "Epoch  4233  G loss  0.036445357\n",
      "Epoch  4234  G loss  0.031751394\n",
      "Epoch  4235  G loss  0.031727307\n",
      "Epoch  4236  G loss  0.007746784\n",
      "Epoch  4237  G loss  0.010353238\n",
      "Epoch  4238  G loss  0.017273363\n",
      "Epoch  4239  G loss  0.023773495\n",
      "Epoch  4240  G loss  0.053050004\n",
      "Epoch  4241  G loss  0.0077048396\n",
      "Epoch  4242  G loss  0.025965855\n",
      "Epoch  4243  G loss  0.012499256\n",
      "Epoch  4244  G loss  0.034027353\n",
      "Epoch  4245  G loss  0.05585377\n",
      "Epoch  4246  G loss  0.14674784\n",
      "Epoch  4247  G loss  0.024926752\n",
      "Epoch  4248  G loss  0.03564153\n",
      "Epoch  4249  G loss  0.087226465\n",
      "Epoch  4250  G loss  0.009289354\n",
      "Epoch  4251  G loss  0.04707263\n",
      "Epoch  4252  G loss  0.06957537\n",
      "Epoch  4253  G loss  0.015183266\n",
      "Epoch  4254  G loss  0.11035643\n",
      "Epoch  4255  G loss  0.032047704\n",
      "Epoch  4256  G loss  0.009699389\n",
      "Epoch  4257  G loss  0.024265487\n",
      "Epoch  4258  G loss  0.03184689\n",
      "Epoch  4259  G loss  0.01611244\n",
      "Epoch  4260  G loss  0.02210528\n",
      "Epoch  4261  G loss  0.018527636\n",
      "Epoch  4262  G loss  0.010217692\n",
      "Epoch  4263  G loss  0.011331048\n",
      "Epoch  4264  G loss  0.038695015\n",
      "Epoch  4265  G loss  0.043992925\n",
      "Epoch  4266  G loss  0.011636686\n",
      "Epoch  4267  G loss  0.040082097\n",
      "Epoch  4268  G loss  0.012457071\n",
      "Epoch  4269  G loss  0.05155745\n",
      "Epoch  4270  G loss  0.009212354\n",
      "Epoch  4271  G loss  0.01861542\n",
      "Epoch  4272  G loss  0.08030494\n",
      "Epoch  4273  G loss  0.092254944\n",
      "Epoch  4274  G loss  0.059073523\n",
      "Epoch  4275  G loss  0.007064188\n",
      "Epoch  4276  G loss  0.067302935\n",
      "Epoch  4277  G loss  0.02497985\n",
      "Epoch  4278  G loss  0.019623142\n",
      "Epoch  4279  G loss  0.010583246\n",
      "Epoch  4280  G loss  0.06832552\n",
      "Epoch  4281  G loss  0.09519469\n",
      "Epoch  4282  G loss  0.1287474\n",
      "Epoch  4283  G loss  0.008693167\n",
      "Epoch  4284  G loss  0.06464817\n",
      "Epoch  4285  G loss  0.011404656\n",
      "Epoch  4286  G loss  0.09802747\n",
      "Epoch  4287  G loss  0.058937542\n",
      "Epoch  4288  G loss  0.0512433\n",
      "Epoch  4289  G loss  0.029796273\n",
      "Epoch  4290  G loss  0.09422263\n",
      "Epoch  4291  G loss  0.13833544\n",
      "Epoch  4292  G loss  0.08034733\n",
      "Epoch  4293  G loss  0.17076845\n",
      "Epoch  4294  G loss  0.07719936\n",
      "Epoch  4295  G loss  0.006727214\n",
      "Epoch  4296  G loss  0.020728735\n",
      "Epoch  4297  G loss  0.038681645\n",
      "Epoch  4298  G loss  0.018681776\n",
      "Epoch  4299  G loss  0.015691135\n",
      "Epoch  4300  G loss  0.024282828\n",
      "Epoch  4301  G loss  0.04789491\n",
      "Epoch  4302  G loss  0.069414005\n",
      "Epoch  4303  G loss  0.04022976\n",
      "Epoch  4304  G loss  0.02213825\n",
      "Epoch  4305  G loss  0.03107708\n",
      "Epoch  4306  G loss  0.02122067\n",
      "Epoch  4307  G loss  0.039617702\n",
      "Epoch  4308  G loss  0.052495103\n",
      "Epoch  4309  G loss  0.019268228\n",
      "Epoch  4310  G loss  0.031805143\n",
      "Epoch  4311  G loss  0.039205372\n",
      "Epoch  4312  G loss  0.010330813\n",
      "Epoch  4313  G loss  0.018912042\n",
      "Epoch  4314  G loss  0.19731537\n",
      "Epoch  4315  G loss  0.08731698\n",
      "Epoch  4316  G loss  0.09916316\n",
      "Epoch  4317  G loss  0.010163022\n",
      "Epoch  4318  G loss  0.012782922\n",
      "Epoch  4319  G loss  0.071757786\n",
      "Epoch  4320  G loss  0.2877046\n",
      "Epoch  4321  G loss  0.08209936\n",
      "Epoch  4322  G loss  0.033073887\n",
      "Epoch  4323  G loss  0.06416843\n",
      "Epoch  4324  G loss  0.23231296\n",
      "Epoch  4325  G loss  0.038622323\n",
      "Epoch  4326  G loss  0.009863462\n",
      "Epoch  4327  G loss  0.012561852\n",
      "Epoch  4328  G loss  0.014811942\n",
      "Epoch  4329  G loss  0.0051735146\n",
      "Epoch  4330  G loss  0.07383807\n",
      "Epoch  4331  G loss  0.05233989\n",
      "Epoch  4332  G loss  0.052710004\n",
      "Epoch  4333  G loss  0.025807867\n",
      "Epoch  4334  G loss  0.009841458\n",
      "Epoch  4335  G loss  0.0055139377\n",
      "Epoch  4336  G loss  0.045508295\n",
      "Epoch  4337  G loss  0.026059067\n",
      "Epoch  4338  G loss  0.011025054\n",
      "Epoch  4339  G loss  0.04461615\n",
      "Epoch  4340  G loss  0.009922389\n",
      "Epoch  4341  G loss  0.035363644\n",
      "Epoch  4342  G loss  0.004322288\n",
      "Epoch  4343  G loss  0.10885149\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  4344  G loss  0.022935804\n",
      "Epoch  4345  G loss  0.022718586\n",
      "Epoch  4346  G loss  0.016807534\n",
      "Epoch  4347  G loss  0.013572421\n",
      "Epoch  4348  G loss  0.004384466\n",
      "Epoch  4349  G loss  0.17809412\n",
      "Epoch  4350  G loss  0.035631835\n",
      "Epoch  4351  G loss  0.01535983\n",
      "Epoch  4352  G loss  0.11745186\n",
      "Epoch  4353  G loss  0.058278397\n",
      "Epoch  4354  G loss  0.061680086\n",
      "Epoch  4355  G loss  0.039606206\n",
      "Epoch  4356  G loss  0.0087981885\n",
      "Epoch  4357  G loss  0.027469043\n",
      "Epoch  4358  G loss  0.015119621\n",
      "Epoch  4359  G loss  0.016643342\n",
      "Epoch  4360  G loss  0.079639144\n",
      "Epoch  4361  G loss  0.051640607\n",
      "Epoch  4362  G loss  0.031802766\n",
      "Epoch  4363  G loss  0.014904838\n",
      "Epoch  4364  G loss  0.033651523\n",
      "Epoch  4365  G loss  0.10716685\n",
      "Epoch  4366  G loss  0.104281425\n",
      "Epoch  4367  G loss  0.024493344\n",
      "Epoch  4368  G loss  0.026209895\n",
      "Epoch  4369  G loss  0.010323163\n",
      "Epoch  4370  G loss  0.0047610137\n",
      "Epoch  4371  G loss  0.07193411\n",
      "Epoch  4372  G loss  0.0069181058\n",
      "Epoch  4373  G loss  0.022906534\n",
      "Epoch  4374  G loss  0.016323002\n",
      "Epoch  4375  G loss  0.051353846\n",
      "Epoch  4376  G loss  0.045437366\n",
      "Epoch  4377  G loss  0.056522466\n",
      "Epoch  4378  G loss  0.076936215\n",
      "Epoch  4379  G loss  0.0481406\n",
      "Epoch  4380  G loss  0.013882796\n",
      "Epoch  4381  G loss  0.057194263\n",
      "Epoch  4382  G loss  0.0144849885\n",
      "Epoch  4383  G loss  0.22606277\n",
      "Epoch  4384  G loss  0.026117705\n",
      "Epoch  4385  G loss  0.01400936\n",
      "Epoch  4386  G loss  0.048191383\n",
      "Epoch  4387  G loss  0.038330205\n",
      "Epoch  4388  G loss  0.016970385\n",
      "Epoch  4389  G loss  0.03134946\n",
      "Epoch  4390  G loss  0.017260876\n",
      "Epoch  4391  G loss  0.037424527\n",
      "Epoch  4392  G loss  0.35180983\n",
      "Epoch  4393  G loss  0.18878508\n",
      "Epoch  4394  G loss  0.10844195\n",
      "Epoch  4395  G loss  0.009092916\n",
      "Epoch  4396  G loss  0.035321984\n",
      "Epoch  4397  G loss  0.014788527\n",
      "Epoch  4398  G loss  0.14675726\n",
      "Epoch  4399  G loss  0.040830713\n",
      "Epoch  4400  G loss  0.022549111\n",
      "Epoch  4401  G loss  0.0134934075\n",
      "Epoch  4402  G loss  0.009644364\n",
      "Epoch  4403  G loss  0.18867534\n",
      "Epoch  4404  G loss  0.027977953\n",
      "Epoch  4405  G loss  0.23932964\n",
      "Epoch  4406  G loss  0.039508823\n",
      "Epoch  4407  G loss  0.0076781507\n",
      "Epoch  4408  G loss  0.13573283\n",
      "Epoch  4409  G loss  0.013088735\n",
      "Epoch  4410  G loss  0.48156533\n",
      "Epoch  4411  G loss  0.021200694\n",
      "Epoch  4412  G loss  0.07043572\n",
      "Epoch  4413  G loss  0.03691952\n",
      "Epoch  4414  G loss  0.20648727\n",
      "Epoch  4415  G loss  0.030549264\n",
      "Epoch  4416  G loss  0.011210279\n",
      "Epoch  4417  G loss  0.1080702\n",
      "Epoch  4418  G loss  0.011931727\n",
      "Epoch  4419  G loss  0.024294918\n",
      "Epoch  4420  G loss  0.009604049\n",
      "Epoch  4421  G loss  0.096550494\n",
      "Epoch  4422  G loss  0.09678474\n",
      "Epoch  4423  G loss  0.06464921\n",
      "Epoch  4424  G loss  0.09653458\n",
      "Epoch  4425  G loss  0.020982973\n",
      "Epoch  4426  G loss  0.05871255\n",
      "Epoch  4427  G loss  0.05881193\n",
      "Epoch  4428  G loss  0.18006174\n",
      "Epoch  4429  G loss  0.023394985\n",
      "Epoch  4430  G loss  0.01578507\n",
      "Epoch  4431  G loss  0.01770094\n",
      "Epoch  4432  G loss  0.052658733\n",
      "Epoch  4433  G loss  0.019657075\n",
      "Epoch  4434  G loss  0.03563907\n",
      "Epoch  4435  G loss  0.15974762\n",
      "Epoch  4436  G loss  0.08806485\n",
      "Epoch  4437  G loss  0.0875003\n",
      "Epoch  4438  G loss  0.011167929\n",
      "Epoch  4439  G loss  0.020653825\n",
      "Epoch  4440  G loss  0.019588169\n",
      "Epoch  4441  G loss  0.07689257\n",
      "Epoch  4442  G loss  0.045305736\n",
      "Epoch  4443  G loss  0.043047726\n",
      "Epoch  4444  G loss  0.035832882\n",
      "Epoch  4445  G loss  0.068831086\n",
      "Epoch  4446  G loss  0.045356728\n",
      "Epoch  4447  G loss  0.06336495\n",
      "Epoch  4448  G loss  0.0272238\n",
      "Epoch  4449  G loss  0.03296031\n",
      "Epoch  4450  G loss  0.09473494\n",
      "Epoch  4451  G loss  0.01346961\n",
      "Epoch  4452  G loss  0.009641247\n",
      "Epoch  4453  G loss  0.026055356\n",
      "Epoch  4454  G loss  0.19322449\n",
      "Epoch  4455  G loss  0.011267718\n",
      "Epoch  4456  G loss  0.14978442\n",
      "Epoch  4457  G loss  0.046824034\n",
      "Epoch  4458  G loss  0.008026546\n",
      "Epoch  4459  G loss  0.019302556\n",
      "Epoch  4460  G loss  0.0071582226\n",
      "Epoch  4461  G loss  0.013033422\n",
      "Epoch  4462  G loss  0.106766105\n",
      "Epoch  4463  G loss  0.038791753\n",
      "Epoch  4464  G loss  0.063797444\n",
      "Epoch  4465  G loss  0.020647705\n",
      "Epoch  4466  G loss  0.005680312\n",
      "Epoch  4467  G loss  0.021165159\n",
      "Epoch  4468  G loss  0.04477414\n",
      "Epoch  4469  G loss  0.07353102\n",
      "Epoch  4470  G loss  0.110657595\n",
      "Epoch  4471  G loss  0.08032803\n",
      "Epoch  4472  G loss  0.1782752\n",
      "Epoch  4473  G loss  0.015930183\n",
      "Epoch  4474  G loss  0.0075892555\n",
      "Epoch  4475  G loss  0.021617334\n",
      "Epoch  4476  G loss  0.03264699\n",
      "Epoch  4477  G loss  0.05072958\n",
      "Epoch  4478  G loss  0.013652317\n",
      "Epoch  4479  G loss  0.055571254\n",
      "Epoch  4480  G loss  0.014478425\n",
      "Epoch  4481  G loss  0.29583523\n",
      "Epoch  4482  G loss  0.076801\n",
      "Epoch  4483  G loss  0.08910462\n",
      "Epoch  4484  G loss  0.05147125\n",
      "Epoch  4485  G loss  0.005708981\n",
      "Epoch  4486  G loss  0.062327344\n",
      "Epoch  4487  G loss  0.0049891016\n",
      "Epoch  4488  G loss  0.0044604726\n",
      "Epoch  4489  G loss  0.018050805\n",
      "Epoch  4490  G loss  0.023691278\n",
      "Epoch  4491  G loss  0.022939589\n",
      "Epoch  4492  G loss  0.04301306\n",
      "Epoch  4493  G loss  0.035853416\n",
      "Epoch  4494  G loss  0.005893815\n",
      "Epoch  4495  G loss  0.040993735\n",
      "Epoch  4496  G loss  0.067482546\n",
      "Epoch  4497  G loss  0.03930869\n",
      "Epoch  4498  G loss  0.11193164\n",
      "Epoch  4499  G loss  0.103087924\n",
      "Epoch  4500  G loss  0.033350497\n",
      "Epoch  4501  G loss  0.011515113\n",
      "Epoch  4502  G loss  0.08725634\n",
      "Epoch  4503  G loss  0.0062027248\n",
      "Epoch  4504  G loss  0.012822511\n",
      "Epoch  4505  G loss  0.042734344\n",
      "Epoch  4506  G loss  0.1624158\n",
      "Epoch  4507  G loss  0.104355395\n",
      "Epoch  4508  G loss  0.027446086\n",
      "Epoch  4509  G loss  0.010463887\n",
      "Epoch  4510  G loss  0.011276687\n",
      "Epoch  4511  G loss  0.23237655\n",
      "Epoch  4512  G loss  0.055498697\n",
      "Epoch  4513  G loss  0.03039876\n",
      "Epoch  4514  G loss  0.08649875\n",
      "Epoch  4515  G loss  0.0078497995\n",
      "Epoch  4516  G loss  0.007336954\n",
      "Epoch  4517  G loss  0.042090386\n",
      "Epoch  4518  G loss  0.06546373\n",
      "Epoch  4519  G loss  0.03907397\n",
      "Epoch  4520  G loss  0.13051003\n",
      "Epoch  4521  G loss  0.0946439\n",
      "Epoch  4522  G loss  0.05751353\n",
      "Epoch  4523  G loss  0.08543773\n",
      "Epoch  4524  G loss  0.022300426\n",
      "Epoch  4525  G loss  0.02174998\n",
      "Epoch  4526  G loss  0.013367385\n",
      "Epoch  4527  G loss  0.18989405\n",
      "Epoch  4528  G loss  0.03360645\n",
      "Epoch  4529  G loss  0.031880643\n",
      "Epoch  4530  G loss  0.0244526\n",
      "Epoch  4531  G loss  0.01847097\n",
      "Epoch  4532  G loss  0.031379774\n",
      "Epoch  4533  G loss  0.006731646\n",
      "Epoch  4534  G loss  0.01792337\n",
      "Epoch  4535  G loss  0.009966128\n",
      "Epoch  4536  G loss  0.032352954\n",
      "Epoch  4537  G loss  0.014546118\n",
      "Epoch  4538  G loss  0.05981998\n",
      "Epoch  4539  G loss  0.0405412\n",
      "Epoch  4540  G loss  0.049676795\n",
      "Epoch  4541  G loss  0.119846605\n",
      "Epoch  4542  G loss  0.032429136\n",
      "Epoch  4543  G loss  0.031239267\n",
      "Epoch  4544  G loss  0.030423388\n",
      "Epoch  4545  G loss  0.05616409\n",
      "Epoch  4546  G loss  0.029892597\n",
      "Epoch  4547  G loss  0.011270015\n",
      "Epoch  4548  G loss  0.052978016\n",
      "Epoch  4549  G loss  0.047083948\n",
      "Epoch  4550  G loss  0.023468822\n",
      "Epoch  4551  G loss  0.018492047\n",
      "Epoch  4552  G loss  0.005754646\n",
      "Epoch  4553  G loss  0.02902429\n",
      "Epoch  4554  G loss  0.012976972\n",
      "Epoch  4555  G loss  0.025186013\n",
      "Epoch  4556  G loss  0.009697214\n",
      "Epoch  4557  G loss  0.04486467\n",
      "Epoch  4558  G loss  0.16338617\n",
      "Epoch  4559  G loss  0.065709986\n",
      "Epoch  4560  G loss  0.015442165\n",
      "Epoch  4561  G loss  0.047599167\n",
      "Epoch  4562  G loss  0.0042549265\n",
      "Epoch  4563  G loss  0.062972896\n",
      "Epoch  4564  G loss  0.032924417\n",
      "Epoch  4565  G loss  0.008928204\n",
      "Epoch  4566  G loss  0.0118807405\n",
      "Epoch  4567  G loss  0.02659827\n",
      "Epoch  4568  G loss  0.035840902\n",
      "Epoch  4569  G loss  0.111708924\n",
      "Epoch  4570  G loss  0.01701521\n",
      "Epoch  4571  G loss  0.035038933\n",
      "Epoch  4572  G loss  0.023895256\n",
      "Epoch  4573  G loss  0.029024467\n",
      "Epoch  4574  G loss  0.054196194\n",
      "Epoch  4575  G loss  0.008491117\n",
      "Epoch  4576  G loss  0.012607527\n",
      "Epoch  4577  G loss  0.0681366\n",
      "Epoch  4578  G loss  0.029283378\n",
      "Epoch  4579  G loss  0.010900986\n",
      "Epoch  4580  G loss  0.103031054\n",
      "Epoch  4581  G loss  0.09519221\n",
      "Epoch  4582  G loss  0.11565985\n",
      "Epoch  4583  G loss  0.014117127\n",
      "Epoch  4584  G loss  0.016661828\n",
      "Epoch  4585  G loss  0.15365157\n",
      "Epoch  4586  G loss  0.08198014\n",
      "Epoch  4587  G loss  0.049243696\n",
      "Epoch  4588  G loss  0.014494746\n",
      "Epoch  4589  G loss  0.09458516\n",
      "Epoch  4590  G loss  0.011333186\n",
      "Epoch  4591  G loss  0.0052951453\n",
      "Epoch  4592  G loss  0.035320565\n",
      "Epoch  4593  G loss  0.014054916\n",
      "Epoch  4594  G loss  0.024715811\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  4595  G loss  0.025643818\n",
      "Epoch  4596  G loss  0.15286249\n",
      "Epoch  4597  G loss  0.0112253595\n",
      "Epoch  4598  G loss  0.025138762\n",
      "Epoch  4599  G loss  0.063591614\n",
      "Epoch  4600  G loss  0.009267614\n",
      "Epoch  4601  G loss  0.008688959\n",
      "Epoch  4602  G loss  0.060292754\n",
      "Epoch  4603  G loss  0.02035712\n",
      "Epoch  4604  G loss  0.041973826\n",
      "Epoch  4605  G loss  0.012805346\n",
      "Epoch  4606  G loss  0.14537328\n",
      "Epoch  4607  G loss  0.030291675\n",
      "Epoch  4608  G loss  0.0134729\n",
      "Epoch  4609  G loss  0.027838754\n",
      "Epoch  4610  G loss  0.038879685\n",
      "Epoch  4611  G loss  0.01617492\n",
      "Epoch  4612  G loss  0.028571976\n",
      "Epoch  4613  G loss  0.061267108\n",
      "Epoch  4614  G loss  0.29444444\n",
      "Epoch  4615  G loss  0.1390109\n",
      "Epoch  4616  G loss  0.008230986\n",
      "Epoch  4617  G loss  0.012086522\n",
      "Epoch  4618  G loss  0.11705762\n",
      "Epoch  4619  G loss  0.055736743\n",
      "Epoch  4620  G loss  0.13372055\n",
      "Epoch  4621  G loss  0.020580236\n",
      "Epoch  4622  G loss  0.035827972\n",
      "Epoch  4623  G loss  0.06301452\n",
      "Epoch  4624  G loss  0.0069748657\n",
      "Epoch  4625  G loss  0.028616443\n",
      "Epoch  4626  G loss  0.041868865\n",
      "Epoch  4627  G loss  0.019425152\n",
      "Epoch  4628  G loss  0.08162652\n",
      "Epoch  4629  G loss  0.023305751\n",
      "Epoch  4630  G loss  0.04335368\n",
      "Epoch  4631  G loss  0.034970544\n",
      "Epoch  4632  G loss  0.035948843\n",
      "Epoch  4633  G loss  0.10487951\n",
      "Epoch  4634  G loss  0.0183816\n",
      "Epoch  4635  G loss  0.015468702\n",
      "Epoch  4636  G loss  0.016606476\n",
      "Epoch  4637  G loss  0.15511061\n",
      "Epoch  4638  G loss  0.01648856\n",
      "Epoch  4639  G loss  0.03998507\n",
      "Epoch  4640  G loss  0.057366006\n",
      "Epoch  4641  G loss  0.024792247\n",
      "Epoch  4642  G loss  0.072146244\n",
      "Epoch  4643  G loss  0.016919151\n",
      "Epoch  4644  G loss  0.026150893\n",
      "Epoch  4645  G loss  0.028151376\n",
      "Epoch  4646  G loss  0.010866508\n",
      "Epoch  4647  G loss  0.0627469\n",
      "Epoch  4648  G loss  0.016381964\n",
      "Epoch  4649  G loss  0.27824867\n",
      "Epoch  4650  G loss  0.27513793\n",
      "Epoch  4651  G loss  0.12028886\n",
      "Epoch  4652  G loss  0.053256527\n",
      "Epoch  4653  G loss  0.09646095\n",
      "Epoch  4654  G loss  0.010494257\n",
      "Epoch  4655  G loss  0.02084166\n",
      "Epoch  4656  G loss  0.032044142\n",
      "Epoch  4657  G loss  0.018365186\n",
      "Epoch  4658  G loss  0.049890473\n",
      "Epoch  4659  G loss  0.038720906\n",
      "Epoch  4660  G loss  0.08339126\n",
      "Epoch  4661  G loss  0.57524616\n",
      "Epoch  4662  G loss  0.032851912\n",
      "Epoch  4663  G loss  0.039522324\n",
      "Epoch  4664  G loss  0.05788583\n",
      "Epoch  4665  G loss  0.016082527\n",
      "Epoch  4666  G loss  0.016466849\n",
      "Epoch  4667  G loss  0.03270725\n",
      "Epoch  4668  G loss  0.0048754755\n",
      "Epoch  4669  G loss  0.007161125\n",
      "Epoch  4670  G loss  0.023443222\n",
      "Epoch  4671  G loss  0.016391933\n",
      "Epoch  4672  G loss  0.047628522\n",
      "Epoch  4673  G loss  0.08188659\n",
      "Epoch  4674  G loss  0.092470884\n",
      "Epoch  4675  G loss  0.040709257\n",
      "Epoch  4676  G loss  0.03058369\n",
      "Epoch  4677  G loss  0.122745834\n",
      "Epoch  4678  G loss  0.076159\n",
      "Epoch  4679  G loss  0.017745651\n",
      "Epoch  4680  G loss  0.25793207\n",
      "Epoch  4681  G loss  0.022652073\n",
      "Epoch  4682  G loss  0.013596996\n",
      "Epoch  4683  G loss  0.018747352\n",
      "Epoch  4684  G loss  0.038766086\n",
      "Epoch  4685  G loss  0.03342739\n",
      "Epoch  4686  G loss  0.048390277\n",
      "Epoch  4687  G loss  0.02621591\n",
      "Epoch  4688  G loss  0.084562734\n",
      "Epoch  4689  G loss  0.010219161\n",
      "Epoch  4690  G loss  0.088094845\n",
      "Epoch  4691  G loss  0.020636389\n",
      "Epoch  4692  G loss  0.03388708\n",
      "Epoch  4693  G loss  0.024375746\n",
      "Epoch  4694  G loss  0.009291338\n",
      "Epoch  4695  G loss  0.04380594\n",
      "Epoch  4696  G loss  0.086349234\n",
      "Epoch  4697  G loss  0.026429057\n",
      "Epoch  4698  G loss  0.095792726\n",
      "Epoch  4699  G loss  0.044941157\n",
      "Epoch  4700  G loss  0.024652908\n",
      "Epoch  4701  G loss  0.03121771\n",
      "Epoch  4702  G loss  0.03697104\n",
      "Epoch  4703  G loss  0.004754643\n",
      "Epoch  4704  G loss  0.0131341815\n",
      "Epoch  4705  G loss  0.18409544\n",
      "Epoch  4706  G loss  0.011457523\n",
      "Epoch  4707  G loss  0.008347194\n",
      "Epoch  4708  G loss  0.013392492\n",
      "Epoch  4709  G loss  0.032860294\n",
      "Epoch  4710  G loss  0.008474324\n",
      "Epoch  4711  G loss  0.10635341\n",
      "Epoch  4712  G loss  0.0488862\n",
      "Epoch  4713  G loss  0.007853897\n",
      "Epoch  4714  G loss  0.007016257\n",
      "Epoch  4715  G loss  0.008330473\n",
      "Epoch  4716  G loss  0.013678097\n",
      "Epoch  4717  G loss  0.05409633\n",
      "Epoch  4718  G loss  0.012338797\n",
      "Epoch  4719  G loss  0.0063430187\n",
      "Epoch  4720  G loss  0.06741065\n",
      "Epoch  4721  G loss  0.06992727\n",
      "Epoch  4722  G loss  0.045759734\n",
      "Epoch  4723  G loss  0.033883326\n",
      "Epoch  4724  G loss  0.081017435\n",
      "Epoch  4725  G loss  0.084340334\n",
      "Epoch  4726  G loss  0.01878009\n",
      "Epoch  4727  G loss  0.014747223\n",
      "Epoch  4728  G loss  0.013437874\n",
      "Epoch  4729  G loss  0.007721363\n",
      "Epoch  4730  G loss  0.046396192\n",
      "Epoch  4731  G loss  0.13494357\n",
      "Epoch  4732  G loss  0.011279827\n",
      "Epoch  4733  G loss  0.011813967\n",
      "Epoch  4734  G loss  0.009791725\n",
      "Epoch  4735  G loss  0.13045412\n",
      "Epoch  4736  G loss  0.013530131\n",
      "Epoch  4737  G loss  0.039379615\n",
      "Epoch  4738  G loss  0.0796892\n",
      "Epoch  4739  G loss  0.027355365\n",
      "Epoch  4740  G loss  0.29579055\n",
      "Epoch  4741  G loss  0.047101047\n",
      "Epoch  4742  G loss  0.016295778\n",
      "Epoch  4743  G loss  0.041102998\n",
      "Epoch  4744  G loss  0.066522196\n",
      "Epoch  4745  G loss  0.09082351\n",
      "Epoch  4746  G loss  0.0072418163\n",
      "Epoch  4747  G loss  0.02814455\n",
      "Epoch  4748  G loss  0.0072376183\n",
      "Epoch  4749  G loss  0.017436564\n",
      "Epoch  4750  G loss  0.2085462\n",
      "Epoch  4751  G loss  0.041493572\n",
      "Epoch  4752  G loss  0.019543076\n",
      "Epoch  4753  G loss  0.06776796\n",
      "Epoch  4754  G loss  0.021424433\n",
      "Epoch  4755  G loss  0.03184691\n",
      "Epoch  4756  G loss  0.16540575\n",
      "Epoch  4757  G loss  0.012607405\n",
      "Epoch  4758  G loss  0.022873022\n",
      "Epoch  4759  G loss  0.010224626\n",
      "Epoch  4760  G loss  0.009845667\n",
      "Epoch  4761  G loss  0.031800598\n",
      "Epoch  4762  G loss  0.04537116\n",
      "Epoch  4763  G loss  0.04844867\n",
      "Epoch  4764  G loss  0.14758031\n",
      "Epoch  4765  G loss  0.04202535\n",
      "Epoch  4766  G loss  0.005802878\n",
      "Epoch  4767  G loss  0.094104685\n",
      "Epoch  4768  G loss  0.06507863\n",
      "Epoch  4769  G loss  0.01343506\n",
      "Epoch  4770  G loss  0.01695953\n",
      "Epoch  4771  G loss  0.016973194\n",
      "Epoch  4772  G loss  0.07321344\n",
      "Epoch  4773  G loss  0.025185183\n",
      "Epoch  4774  G loss  0.049770493\n",
      "Epoch  4775  G loss  0.12297286\n",
      "Epoch  4776  G loss  0.18539575\n",
      "Epoch  4777  G loss  0.017730337\n",
      "Epoch  4778  G loss  0.05451443\n",
      "Epoch  4779  G loss  0.023216082\n",
      "Epoch  4780  G loss  0.024013154\n",
      "Epoch  4781  G loss  0.0064675706\n",
      "Epoch  4782  G loss  0.036599547\n",
      "Epoch  4783  G loss  0.19441086\n",
      "Epoch  4784  G loss  0.110983916\n",
      "Epoch  4785  G loss  0.077757776\n",
      "Epoch  4786  G loss  0.028122956\n",
      "Epoch  4787  G loss  0.01729909\n",
      "Epoch  4788  G loss  0.040221177\n",
      "Epoch  4789  G loss  0.13690019\n",
      "Epoch  4790  G loss  0.054960623\n",
      "Epoch  4791  G loss  0.054091167\n",
      "Epoch  4792  G loss  0.057445124\n",
      "Epoch  4793  G loss  0.09272577\n",
      "Epoch  4794  G loss  0.008302284\n",
      "Epoch  4795  G loss  0.035347506\n",
      "Epoch  4796  G loss  0.030487971\n",
      "Epoch  4797  G loss  0.010434983\n",
      "Epoch  4798  G loss  0.009952198\n",
      "Epoch  4799  G loss  0.05468071\n",
      "Epoch  4800  G loss  0.025282955\n",
      "Epoch  4801  G loss  0.006750315\n",
      "Epoch  4802  G loss  0.004745959\n",
      "Epoch  4803  G loss  0.047806337\n",
      "Epoch  4804  G loss  0.044288803\n",
      "Epoch  4805  G loss  0.006091135\n",
      "Epoch  4806  G loss  0.0062948796\n",
      "Epoch  4807  G loss  0.07434319\n",
      "Epoch  4808  G loss  0.016151339\n",
      "Epoch  4809  G loss  0.050463334\n",
      "Epoch  4810  G loss  0.14569479\n",
      "Epoch  4811  G loss  0.05403219\n",
      "Epoch  4812  G loss  0.048687562\n",
      "Epoch  4813  G loss  0.0099605005\n",
      "Epoch  4814  G loss  0.01399024\n",
      "Epoch  4815  G loss  0.13280532\n",
      "Epoch  4816  G loss  0.0443305\n",
      "Epoch  4817  G loss  0.060376458\n",
      "Epoch  4818  G loss  0.117792845\n",
      "Epoch  4819  G loss  0.012211384\n",
      "Epoch  4820  G loss  0.011321489\n",
      "Epoch  4821  G loss  0.020633886\n",
      "Epoch  4822  G loss  0.01746549\n",
      "Epoch  4823  G loss  0.16712388\n",
      "Epoch  4824  G loss  0.012516117\n",
      "Epoch  4825  G loss  0.010226218\n",
      "Epoch  4826  G loss  0.06369403\n",
      "Epoch  4827  G loss  0.045900196\n",
      "Epoch  4828  G loss  0.0617934\n",
      "Epoch  4829  G loss  0.008864891\n",
      "Epoch  4830  G loss  0.108994514\n",
      "Epoch  4831  G loss  0.0420212\n",
      "Epoch  4832  G loss  0.011671892\n",
      "Epoch  4833  G loss  0.010141012\n",
      "Epoch  4834  G loss  0.026939407\n",
      "Epoch  4835  G loss  0.009756146\n",
      "Epoch  4836  G loss  0.05577566\n",
      "Epoch  4837  G loss  0.1372177\n",
      "Epoch  4838  G loss  0.0124112945\n",
      "Epoch  4839  G loss  0.23188989\n",
      "Epoch  4840  G loss  0.15241624\n",
      "Epoch  4841  G loss  0.018914219\n",
      "Epoch  4842  G loss  0.11196278\n",
      "Epoch  4843  G loss  0.06032202\n",
      "Epoch  4844  G loss  0.012526536\n",
      "Epoch  4845  G loss  0.028674362\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  4846  G loss  0.115100235\n",
      "Epoch  4847  G loss  0.026666433\n",
      "Epoch  4848  G loss  0.014397189\n",
      "Epoch  4849  G loss  0.076013416\n",
      "Epoch  4850  G loss  0.021897357\n",
      "Epoch  4851  G loss  0.08443686\n",
      "Epoch  4852  G loss  0.05246588\n",
      "Epoch  4853  G loss  0.027251676\n",
      "Epoch  4854  G loss  0.0125430785\n",
      "Epoch  4855  G loss  0.020837\n",
      "Epoch  4856  G loss  0.022395303\n",
      "Epoch  4857  G loss  0.03793218\n",
      "Epoch  4858  G loss  0.045861393\n",
      "Epoch  4859  G loss  0.029056098\n",
      "Epoch  4860  G loss  0.019445363\n",
      "Epoch  4861  G loss  0.032731272\n",
      "Epoch  4862  G loss  0.0124886595\n",
      "Epoch  4863  G loss  0.015704721\n",
      "Epoch  4864  G loss  0.031706072\n",
      "Epoch  4865  G loss  0.01860781\n",
      "Epoch  4866  G loss  0.024611786\n",
      "Epoch  4867  G loss  0.0067924974\n",
      "Epoch  4868  G loss  0.13610044\n",
      "Epoch  4869  G loss  0.07826158\n",
      "Epoch  4870  G loss  0.023842206\n",
      "Epoch  4871  G loss  0.05887664\n",
      "Epoch  4872  G loss  0.12102394\n",
      "Epoch  4873  G loss  0.11532672\n",
      "Epoch  4874  G loss  0.17838782\n",
      "Epoch  4875  G loss  0.0069948295\n",
      "Epoch  4876  G loss  0.017886275\n",
      "Epoch  4877  G loss  0.10686195\n",
      "Epoch  4878  G loss  0.010003049\n",
      "Epoch  4879  G loss  0.022289893\n",
      "Epoch  4880  G loss  0.060028013\n",
      "Epoch  4881  G loss  0.1711504\n",
      "Epoch  4882  G loss  0.041913882\n",
      "Epoch  4883  G loss  0.046155885\n",
      "Epoch  4884  G loss  0.18516278\n",
      "Epoch  4885  G loss  0.09425297\n",
      "Epoch  4886  G loss  0.023814138\n",
      "Epoch  4887  G loss  0.037857376\n",
      "Epoch  4888  G loss  0.028994914\n",
      "Epoch  4889  G loss  0.025378402\n",
      "Epoch  4890  G loss  0.04750235\n",
      "Epoch  4891  G loss  0.05871015\n",
      "Epoch  4892  G loss  0.16002278\n",
      "Epoch  4893  G loss  0.014994308\n",
      "Epoch  4894  G loss  0.02921028\n",
      "Epoch  4895  G loss  0.017690295\n",
      "Epoch  4896  G loss  0.026426464\n",
      "Epoch  4897  G loss  0.010877965\n",
      "Epoch  4898  G loss  0.020668482\n",
      "Epoch  4899  G loss  0.02871344\n",
      "Epoch  4900  G loss  0.08963129\n",
      "Epoch  4901  G loss  0.03950143\n",
      "Epoch  4902  G loss  0.005581031\n",
      "Epoch  4903  G loss  0.08086313\n",
      "Epoch  4904  G loss  0.004376293\n",
      "Epoch  4905  G loss  0.08123575\n",
      "Epoch  4906  G loss  0.016588561\n",
      "Epoch  4907  G loss  0.01370218\n",
      "Epoch  4908  G loss  0.26199603\n",
      "Epoch  4909  G loss  0.013294569\n",
      "Epoch  4910  G loss  0.09101112\n",
      "Epoch  4911  G loss  0.07275203\n",
      "Epoch  4912  G loss  0.4126811\n",
      "Epoch  4913  G loss  0.023455268\n",
      "Epoch  4914  G loss  0.082363695\n",
      "Epoch  4915  G loss  0.024659619\n",
      "Epoch  4916  G loss  0.022032429\n",
      "Epoch  4917  G loss  0.027094403\n",
      "Epoch  4918  G loss  0.017648779\n",
      "Epoch  4919  G loss  0.010077251\n",
      "Epoch  4920  G loss  0.0067209215\n",
      "Epoch  4921  G loss  0.05356449\n",
      "Epoch  4922  G loss  0.016160058\n",
      "Epoch  4923  G loss  0.07468306\n",
      "Epoch  4924  G loss  0.061144017\n",
      "Epoch  4925  G loss  0.033131\n",
      "Epoch  4926  G loss  0.033729576\n",
      "Epoch  4927  G loss  0.06530302\n",
      "Epoch  4928  G loss  0.022112163\n",
      "Epoch  4929  G loss  0.0068723913\n",
      "Epoch  4930  G loss  0.10921034\n",
      "Epoch  4931  G loss  0.30060643\n",
      "Epoch  4932  G loss  0.05552562\n",
      "Epoch  4933  G loss  0.022759933\n",
      "Epoch  4934  G loss  0.051490515\n",
      "Epoch  4935  G loss  0.08545193\n",
      "Epoch  4936  G loss  0.03409235\n",
      "Epoch  4937  G loss  0.01325347\n",
      "Epoch  4938  G loss  0.004733342\n",
      "Epoch  4939  G loss  0.021299353\n",
      "Epoch  4940  G loss  0.013228738\n",
      "Epoch  4941  G loss  0.05334533\n",
      "Epoch  4942  G loss  0.025893914\n",
      "Epoch  4943  G loss  0.039192628\n",
      "Epoch  4944  G loss  0.004706228\n",
      "Epoch  4945  G loss  0.024562642\n",
      "Epoch  4946  G loss  0.015042195\n",
      "Epoch  4947  G loss  0.026973214\n",
      "Epoch  4948  G loss  0.15957725\n",
      "Epoch  4949  G loss  0.051701963\n",
      "Epoch  4950  G loss  0.048247892\n",
      "Epoch  4951  G loss  0.03291696\n",
      "Epoch  4952  G loss  0.014976258\n",
      "Epoch  4953  G loss  0.011584382\n",
      "Epoch  4954  G loss  0.122738525\n",
      "Epoch  4955  G loss  0.01991161\n",
      "Epoch  4956  G loss  0.07365615\n",
      "Epoch  4957  G loss  0.02243919\n",
      "Epoch  4958  G loss  0.028079784\n",
      "Epoch  4959  G loss  0.009229322\n",
      "Epoch  4960  G loss  0.011559534\n",
      "Epoch  4961  G loss  0.060386285\n",
      "Epoch  4962  G loss  0.12546304\n",
      "Epoch  4963  G loss  0.02205515\n",
      "Epoch  4964  G loss  0.03758855\n",
      "Epoch  4965  G loss  0.012889605\n",
      "Epoch  4966  G loss  0.03450822\n",
      "Epoch  4967  G loss  0.0126897\n",
      "Epoch  4968  G loss  0.022136334\n",
      "Epoch  4969  G loss  0.08497746\n",
      "Epoch  4970  G loss  0.0065432526\n",
      "Epoch  4971  G loss  0.13882023\n",
      "Epoch  4972  G loss  0.019909706\n",
      "Epoch  4973  G loss  0.056343406\n",
      "Epoch  4974  G loss  0.009079779\n",
      "Epoch  4975  G loss  0.06531916\n",
      "Epoch  4976  G loss  0.033884995\n",
      "Epoch  4977  G loss  0.12135352\n",
      "Epoch  4978  G loss  0.028379979\n",
      "Epoch  4979  G loss  0.088789806\n",
      "Epoch  4980  G loss  0.050960004\n",
      "Epoch  4981  G loss  0.03887161\n",
      "Epoch  4982  G loss  0.026928391\n",
      "Epoch  4983  G loss  0.08394911\n",
      "Epoch  4984  G loss  0.05738404\n",
      "Epoch  4985  G loss  0.0106483055\n",
      "Epoch  4986  G loss  0.050961588\n",
      "Epoch  4987  G loss  0.021476595\n",
      "Epoch  4988  G loss  0.012370888\n",
      "Epoch  4989  G loss  0.019931821\n",
      "Epoch  4990  G loss  0.03896521\n",
      "Epoch  4991  G loss  0.086851135\n",
      "Epoch  4992  G loss  0.010417517\n",
      "Epoch  4993  G loss  0.012113945\n",
      "Epoch  4994  G loss  0.015169655\n",
      "Epoch  4995  G loss  0.045294926\n",
      "Epoch  4996  G loss  0.033588253\n",
      "Epoch  4997  G loss  0.045521326\n",
      "Epoch  4998  G loss  0.028577397\n",
      "Epoch  4999  G loss  0.01176949\n",
      "Epoch  5000  G loss  0.01619187\n",
      "Epoch  5001  G loss  0.11104421\n",
      "Epoch  5002  G loss  0.022448417\n",
      "Epoch  5003  G loss  0.009541975\n",
      "Epoch  5004  G loss  0.07594437\n",
      "Epoch  5005  G loss  0.07176608\n",
      "Epoch  5006  G loss  0.012282612\n",
      "Epoch  5007  G loss  0.015704388\n",
      "Epoch  5008  G loss  0.018759891\n",
      "Epoch  5009  G loss  0.04247053\n",
      "Epoch  5010  G loss  0.009254744\n",
      "Epoch  5011  G loss  0.006227165\n",
      "Epoch  5012  G loss  0.010622475\n",
      "Epoch  5013  G loss  0.031836674\n",
      "Epoch  5014  G loss  0.014971877\n",
      "Epoch  5015  G loss  0.023327317\n",
      "Epoch  5016  G loss  0.021697633\n",
      "Epoch  5017  G loss  0.00909376\n",
      "Epoch  5018  G loss  0.007758626\n",
      "Epoch  5019  G loss  0.0063876403\n",
      "Epoch  5020  G loss  0.008728563\n",
      "Epoch  5021  G loss  0.07543951\n",
      "Epoch  5022  G loss  0.087999076\n",
      "Epoch  5023  G loss  0.049874663\n",
      "Epoch  5024  G loss  0.039311066\n",
      "Epoch  5025  G loss  0.16342932\n",
      "Epoch  5026  G loss  0.008772731\n",
      "Epoch  5027  G loss  0.23016971\n",
      "Epoch  5028  G loss  0.12039348\n",
      "Epoch  5029  G loss  0.018595994\n",
      "Epoch  5030  G loss  0.038476773\n",
      "Epoch  5031  G loss  0.020443678\n",
      "Epoch  5032  G loss  0.049480416\n",
      "Epoch  5033  G loss  0.0041037425\n",
      "Epoch  5034  G loss  0.186272\n",
      "Epoch  5035  G loss  0.020597115\n",
      "Epoch  5036  G loss  0.020591956\n",
      "Epoch  5037  G loss  0.016953712\n",
      "Epoch  5038  G loss  0.01921895\n",
      "Epoch  5039  G loss  0.01817334\n",
      "Epoch  5040  G loss  0.01479809\n",
      "Epoch  5041  G loss  0.026403131\n",
      "Epoch  5042  G loss  0.012101998\n",
      "Epoch  5043  G loss  0.0091739455\n",
      "Epoch  5044  G loss  0.105943374\n",
      "Epoch  5045  G loss  0.13732399\n",
      "Epoch  5046  G loss  0.07346418\n",
      "Epoch  5047  G loss  0.036361925\n",
      "Epoch  5048  G loss  0.009588062\n",
      "Epoch  5049  G loss  0.027014935\n",
      "Epoch  5050  G loss  0.02414169\n",
      "Epoch  5051  G loss  0.0129959835\n",
      "Epoch  5052  G loss  0.03788532\n",
      "Epoch  5053  G loss  0.024399903\n",
      "Epoch  5054  G loss  0.028925896\n",
      "Epoch  5055  G loss  0.02449954\n",
      "Epoch  5056  G loss  0.007430275\n",
      "Epoch  5057  G loss  0.01567921\n",
      "Epoch  5058  G loss  0.019267892\n",
      "Epoch  5059  G loss  0.067657165\n",
      "Epoch  5060  G loss  0.066656835\n",
      "Epoch  5061  G loss  0.06884996\n",
      "Epoch  5062  G loss  0.020728631\n",
      "Epoch  5063  G loss  0.0065503693\n",
      "Epoch  5064  G loss  0.09476626\n",
      "Epoch  5065  G loss  0.059120867\n",
      "Epoch  5066  G loss  0.038835377\n",
      "Epoch  5067  G loss  0.01781446\n",
      "Epoch  5068  G loss  0.044077974\n",
      "Epoch  5069  G loss  0.009925197\n",
      "Epoch  5070  G loss  0.10039785\n",
      "Epoch  5071  G loss  0.013353692\n",
      "Epoch  5072  G loss  0.015106894\n",
      "Epoch  5073  G loss  0.054410953\n",
      "Epoch  5074  G loss  0.11778643\n",
      "Epoch  5075  G loss  0.022192417\n",
      "Epoch  5076  G loss  0.011602351\n",
      "Epoch  5077  G loss  0.19769342\n",
      "Epoch  5078  G loss  0.030129198\n",
      "Epoch  5079  G loss  0.10265129\n",
      "Epoch  5080  G loss  0.023780907\n",
      "Epoch  5081  G loss  0.04275229\n",
      "Epoch  5082  G loss  0.038980067\n",
      "Epoch  5083  G loss  0.041355543\n",
      "Epoch  5084  G loss  0.0046808263\n",
      "Epoch  5085  G loss  0.04888413\n",
      "Epoch  5086  G loss  0.018418383\n",
      "Epoch  5087  G loss  0.08990745\n",
      "Epoch  5088  G loss  0.08570947\n",
      "Epoch  5089  G loss  0.013183901\n",
      "Epoch  5090  G loss  0.019543089\n",
      "Epoch  5091  G loss  0.01391054\n",
      "Epoch  5092  G loss  0.040506154\n",
      "Epoch  5093  G loss  0.023207027\n",
      "Epoch  5094  G loss  0.007793818\n",
      "Epoch  5095  G loss  0.032465614\n",
      "Epoch  5096  G loss  0.021230826\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  5097  G loss  0.009371252\n",
      "Epoch  5098  G loss  0.008176908\n",
      "Epoch  5099  G loss  0.020692274\n",
      "Epoch  5100  G loss  0.021287281\n",
      "Epoch  5101  G loss  0.053392157\n",
      "Epoch  5102  G loss  0.028225638\n",
      "Epoch  5103  G loss  0.0066632032\n",
      "Epoch  5104  G loss  0.050395176\n",
      "Epoch  5105  G loss  0.007841982\n",
      "Epoch  5106  G loss  0.006785306\n",
      "Epoch  5107  G loss  0.022187637\n",
      "Epoch  5108  G loss  0.011730427\n",
      "Epoch  5109  G loss  0.05775804\n",
      "Epoch  5110  G loss  0.019710004\n",
      "Epoch  5111  G loss  0.05343212\n",
      "Epoch  5112  G loss  0.047325943\n",
      "Epoch  5113  G loss  0.11230922\n",
      "Epoch  5114  G loss  0.02286058\n",
      "Epoch  5115  G loss  0.19291198\n",
      "Epoch  5116  G loss  0.03666554\n",
      "Epoch  5117  G loss  0.08800308\n",
      "Epoch  5118  G loss  0.047292616\n",
      "Epoch  5119  G loss  0.0239567\n",
      "Epoch  5120  G loss  0.032601845\n",
      "Epoch  5121  G loss  0.07865917\n",
      "Epoch  5122  G loss  0.0467756\n",
      "Epoch  5123  G loss  0.016384609\n",
      "Epoch  5124  G loss  0.07703397\n",
      "Epoch  5125  G loss  0.04481496\n",
      "Epoch  5126  G loss  0.037508808\n",
      "Epoch  5127  G loss  0.13004038\n",
      "Epoch  5128  G loss  0.04776042\n",
      "Epoch  5129  G loss  0.006969669\n",
      "Epoch  5130  G loss  0.015932616\n",
      "Epoch  5131  G loss  0.03410599\n",
      "Epoch  5132  G loss  0.28017583\n",
      "Epoch  5133  G loss  0.0042721997\n",
      "Epoch  5134  G loss  0.054774713\n",
      "Epoch  5135  G loss  0.019927999\n",
      "Epoch  5136  G loss  0.011667327\n",
      "Epoch  5137  G loss  0.017823018\n",
      "Epoch  5138  G loss  0.036759853\n",
      "Epoch  5139  G loss  0.018956194\n",
      "Epoch  5140  G loss  0.02314611\n",
      "Epoch  5141  G loss  0.0052277804\n",
      "Epoch  5142  G loss  0.045512673\n",
      "Epoch  5143  G loss  0.016547661\n",
      "Epoch  5144  G loss  0.06719134\n",
      "Epoch  5145  G loss  0.12574542\n",
      "Epoch  5146  G loss  0.011237299\n",
      "Epoch  5147  G loss  0.01104482\n",
      "Epoch  5148  G loss  0.064388365\n",
      "Epoch  5149  G loss  0.019345788\n",
      "Epoch  5150  G loss  0.022481997\n",
      "Epoch  5151  G loss  0.07421963\n",
      "Epoch  5152  G loss  0.07477394\n",
      "Epoch  5153  G loss  0.01169955\n",
      "Epoch  5154  G loss  0.043138422\n",
      "Epoch  5155  G loss  0.009658335\n",
      "Epoch  5156  G loss  0.013747556\n",
      "Epoch  5157  G loss  0.021743372\n",
      "Epoch  5158  G loss  0.08853932\n",
      "Epoch  5159  G loss  0.10773301\n",
      "Epoch  5160  G loss  0.009506887\n",
      "Epoch  5161  G loss  0.058572948\n",
      "Epoch  5162  G loss  0.10926098\n",
      "Epoch  5163  G loss  0.024308493\n",
      "Epoch  5164  G loss  0.017969403\n",
      "Epoch  5165  G loss  0.14087775\n",
      "Epoch  5166  G loss  0.102084495\n",
      "Epoch  5167  G loss  0.01644693\n",
      "Epoch  5168  G loss  0.040172126\n",
      "Epoch  5169  G loss  0.023037726\n",
      "Epoch  5170  G loss  0.016851861\n",
      "Epoch  5171  G loss  0.026671039\n",
      "Epoch  5172  G loss  0.009121304\n",
      "Epoch  5173  G loss  0.0047686077\n",
      "Epoch  5174  G loss  0.014562732\n",
      "Epoch  5175  G loss  0.014157084\n",
      "Epoch  5176  G loss  0.01656431\n",
      "Epoch  5177  G loss  0.022453098\n",
      "Epoch  5178  G loss  0.008677623\n",
      "Epoch  5179  G loss  0.016241912\n",
      "Epoch  5180  G loss  0.021746106\n",
      "Epoch  5181  G loss  0.13574253\n",
      "Epoch  5182  G loss  0.015770564\n",
      "Epoch  5183  G loss  0.021702945\n",
      "Epoch  5184  G loss  0.03350929\n",
      "Epoch  5185  G loss  0.06310734\n",
      "Epoch  5186  G loss  0.028089702\n",
      "Epoch  5187  G loss  0.07682179\n",
      "Epoch  5188  G loss  0.029352326\n",
      "Epoch  5189  G loss  0.011961708\n",
      "Epoch  5190  G loss  0.029680204\n",
      "Epoch  5191  G loss  0.12531336\n",
      "Epoch  5192  G loss  0.22142744\n",
      "Epoch  5193  G loss  0.14607541\n",
      "Epoch  5194  G loss  0.05759021\n",
      "Epoch  5195  G loss  0.06452578\n",
      "Epoch  5196  G loss  0.0087039145\n",
      "Epoch  5197  G loss  0.0057264194\n",
      "Epoch  5198  G loss  0.05634764\n",
      "Epoch  5199  G loss  0.03200533\n",
      "Epoch  5200  G loss  0.09599831\n",
      "Epoch  5201  G loss  0.016400298\n",
      "Epoch  5202  G loss  0.28482413\n",
      "Epoch  5203  G loss  0.07131854\n",
      "Epoch  5204  G loss  0.021121621\n",
      "Epoch  5205  G loss  0.115585566\n",
      "Epoch  5206  G loss  0.03327654\n",
      "Epoch  5207  G loss  0.16069031\n",
      "Epoch  5208  G loss  0.27368653\n",
      "Epoch  5209  G loss  0.008594833\n",
      "Epoch  5210  G loss  0.022629272\n",
      "Epoch  5211  G loss  0.07420023\n",
      "Epoch  5212  G loss  0.036118817\n",
      "Epoch  5213  G loss  0.013171013\n",
      "Epoch  5214  G loss  0.024282455\n",
      "Epoch  5215  G loss  0.025412418\n",
      "Epoch  5216  G loss  0.1001004\n",
      "Epoch  5217  G loss  0.060261108\n",
      "Epoch  5218  G loss  0.017554902\n",
      "Epoch  5219  G loss  0.0104111675\n",
      "Epoch  5220  G loss  0.01397766\n",
      "Epoch  5221  G loss  0.016560875\n",
      "Epoch  5222  G loss  0.03925502\n",
      "Epoch  5223  G loss  0.026449462\n",
      "Epoch  5224  G loss  0.020385511\n",
      "Epoch  5225  G loss  0.034708582\n",
      "Epoch  5226  G loss  0.034886785\n",
      "Epoch  5227  G loss  0.04146781\n",
      "Epoch  5228  G loss  0.007322084\n",
      "Epoch  5229  G loss  0.031285457\n",
      "Epoch  5230  G loss  0.059632577\n",
      "Epoch  5231  G loss  0.009368114\n",
      "Epoch  5232  G loss  0.110205546\n",
      "Epoch  5233  G loss  0.025638461\n",
      "Epoch  5234  G loss  0.022540918\n",
      "Epoch  5235  G loss  0.09340578\n",
      "Epoch  5236  G loss  0.015918259\n",
      "Epoch  5237  G loss  0.061434165\n",
      "Epoch  5238  G loss  0.054901965\n",
      "Epoch  5239  G loss  0.04729361\n",
      "Epoch  5240  G loss  0.12177948\n",
      "Epoch  5241  G loss  0.012369463\n",
      "Epoch  5242  G loss  0.014958755\n",
      "Epoch  5243  G loss  0.020423196\n",
      "Epoch  5244  G loss  0.07054702\n",
      "Epoch  5245  G loss  0.08519215\n",
      "Epoch  5246  G loss  0.010404437\n",
      "Epoch  5247  G loss  0.012639555\n",
      "Epoch  5248  G loss  0.032508064\n",
      "Epoch  5249  G loss  0.038652834\n",
      "Epoch  5250  G loss  0.12983839\n",
      "Epoch  5251  G loss  0.025639324\n",
      "Epoch  5252  G loss  0.12057823\n",
      "Epoch  5253  G loss  0.03167432\n",
      "Epoch  5254  G loss  0.10142742\n",
      "Epoch  5255  G loss  0.011495525\n",
      "Epoch  5256  G loss  0.019729584\n",
      "Epoch  5257  G loss  0.024264427\n",
      "Epoch  5258  G loss  0.02234653\n",
      "Epoch  5259  G loss  0.013754722\n",
      "Epoch  5260  G loss  0.07099204\n",
      "Epoch  5261  G loss  0.03570047\n",
      "Epoch  5262  G loss  0.017765775\n",
      "Epoch  5263  G loss  0.010451748\n",
      "Epoch  5264  G loss  0.14595246\n",
      "Epoch  5265  G loss  0.084679805\n",
      "Epoch  5266  G loss  0.04622207\n",
      "Epoch  5267  G loss  0.013875161\n",
      "Epoch  5268  G loss  0.030287843\n",
      "Epoch  5269  G loss  0.022638604\n",
      "Epoch  5270  G loss  0.009125407\n",
      "Epoch  5271  G loss  0.015406531\n",
      "Epoch  5272  G loss  0.014115754\n",
      "Epoch  5273  G loss  0.019571923\n",
      "Epoch  5274  G loss  0.100298546\n",
      "Epoch  5275  G loss  0.050012764\n",
      "Epoch  5276  G loss  0.006778439\n",
      "Epoch  5277  G loss  0.013845168\n",
      "Epoch  5278  G loss  0.051094174\n",
      "Epoch  5279  G loss  0.009951713\n",
      "Epoch  5280  G loss  0.028507985\n",
      "Epoch  5281  G loss  0.030260447\n",
      "Epoch  5282  G loss  0.006215824\n",
      "Epoch  5283  G loss  0.0062765977\n",
      "Epoch  5284  G loss  0.107490316\n",
      "Epoch  5285  G loss  0.013958291\n",
      "Epoch  5286  G loss  0.06541717\n",
      "Epoch  5287  G loss  0.035933074\n",
      "Epoch  5288  G loss  0.010239303\n",
      "Epoch  5289  G loss  0.025226474\n",
      "Epoch  5290  G loss  0.019230474\n",
      "Epoch  5291  G loss  0.0061495965\n",
      "Epoch  5292  G loss  0.092895694\n",
      "Epoch  5293  G loss  0.019494312\n",
      "Epoch  5294  G loss  0.05652418\n",
      "Epoch  5295  G loss  0.019492887\n",
      "Epoch  5296  G loss  0.0229664\n",
      "Epoch  5297  G loss  0.030253787\n",
      "Epoch  5298  G loss  0.015113048\n",
      "Epoch  5299  G loss  0.016852828\n",
      "Epoch  5300  G loss  0.097381584\n",
      "Epoch  5301  G loss  0.016172484\n",
      "Epoch  5302  G loss  0.013515303\n",
      "Epoch  5303  G loss  0.2298872\n",
      "Epoch  5304  G loss  0.045275353\n",
      "Epoch  5305  G loss  0.026705176\n",
      "Epoch  5306  G loss  0.053329963\n",
      "Epoch  5307  G loss  0.0079241255\n",
      "Epoch  5308  G loss  0.018703979\n",
      "Epoch  5309  G loss  0.115347214\n",
      "Epoch  5310  G loss  0.020055216\n",
      "Epoch  5311  G loss  0.05260764\n",
      "Epoch  5312  G loss  0.12924737\n",
      "Epoch  5313  G loss  0.034427136\n",
      "Epoch  5314  G loss  0.02172882\n",
      "Epoch  5315  G loss  0.16479707\n",
      "Epoch  5316  G loss  0.09336597\n",
      "Epoch  5317  G loss  0.015798269\n",
      "Epoch  5318  G loss  0.10356389\n",
      "Epoch  5319  G loss  0.01110334\n",
      "Epoch  5320  G loss  0.14157782\n",
      "Epoch  5321  G loss  0.07163874\n",
      "Epoch  5322  G loss  0.076807834\n",
      "Epoch  5323  G loss  0.005620367\n",
      "Epoch  5324  G loss  0.016772151\n",
      "Epoch  5325  G loss  0.016927226\n",
      "Epoch  5326  G loss  0.017668149\n",
      "Epoch  5327  G loss  0.022650767\n",
      "Epoch  5328  G loss  0.081114486\n",
      "Epoch  5329  G loss  0.021471497\n",
      "Epoch  5330  G loss  0.16703013\n",
      "Epoch  5331  G loss  0.06941068\n",
      "Epoch  5332  G loss  0.00943706\n",
      "Epoch  5333  G loss  0.049585655\n",
      "Epoch  5334  G loss  0.09959049\n",
      "Epoch  5335  G loss  0.16193524\n",
      "Epoch  5336  G loss  0.014520755\n",
      "Epoch  5337  G loss  0.03110495\n",
      "Epoch  5338  G loss  0.039355695\n",
      "Epoch  5339  G loss  0.048602067\n",
      "Epoch  5340  G loss  0.039998773\n",
      "Epoch  5341  G loss  0.018843692\n",
      "Epoch  5342  G loss  0.14430109\n",
      "Epoch  5343  G loss  0.01838712\n",
      "Epoch  5344  G loss  0.008047605\n",
      "Epoch  5345  G loss  0.03557005\n",
      "Epoch  5346  G loss  0.14002162\n",
      "Epoch  5347  G loss  0.0070709316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  5348  G loss  0.15968357\n",
      "Epoch  5349  G loss  0.0145349065\n",
      "Epoch  5350  G loss  0.011279686\n",
      "Epoch  5351  G loss  0.04391639\n",
      "Epoch  5352  G loss  0.023396311\n",
      "Epoch  5353  G loss  0.07661552\n",
      "Epoch  5354  G loss  0.09085904\n",
      "Epoch  5355  G loss  0.038804054\n",
      "Epoch  5356  G loss  0.25375736\n",
      "Epoch  5357  G loss  0.042633858\n",
      "Epoch  5358  G loss  0.01924574\n",
      "Epoch  5359  G loss  0.08326258\n",
      "Epoch  5360  G loss  0.03446372\n",
      "Epoch  5361  G loss  0.014235652\n",
      "Epoch  5362  G loss  0.0062401723\n",
      "Epoch  5363  G loss  0.021440534\n",
      "Epoch  5364  G loss  0.06343068\n",
      "Epoch  5365  G loss  0.017429616\n",
      "Epoch  5366  G loss  0.23561928\n",
      "Epoch  5367  G loss  0.12027092\n",
      "Epoch  5368  G loss  0.085500665\n",
      "Epoch  5369  G loss  0.08450185\n",
      "Epoch  5370  G loss  0.11986011\n",
      "Epoch  5371  G loss  0.019691072\n",
      "Epoch  5372  G loss  0.15462959\n",
      "Epoch  5373  G loss  0.022191243\n",
      "Epoch  5374  G loss  0.012869168\n",
      "Epoch  5375  G loss  0.013109298\n",
      "Epoch  5376  G loss  0.046665266\n",
      "Epoch  5377  G loss  0.036202114\n",
      "Epoch  5378  G loss  0.05154488\n",
      "Epoch  5379  G loss  0.17929846\n",
      "Epoch  5380  G loss  0.13473156\n",
      "Epoch  5381  G loss  0.03676369\n",
      "Epoch  5382  G loss  0.032632288\n",
      "Epoch  5383  G loss  0.017326782\n",
      "Epoch  5384  G loss  0.11161251\n",
      "Epoch  5385  G loss  0.019397754\n",
      "Epoch  5386  G loss  0.012275884\n",
      "Epoch  5387  G loss  0.017258871\n",
      "Epoch  5388  G loss  0.042782463\n",
      "Epoch  5389  G loss  0.00875164\n",
      "Epoch  5390  G loss  0.079124585\n",
      "Epoch  5391  G loss  0.012108595\n",
      "Epoch  5392  G loss  0.10106881\n",
      "Epoch  5393  G loss  0.036866467\n",
      "Epoch  5394  G loss  0.058386713\n",
      "Epoch  5395  G loss  0.01217027\n",
      "Epoch  5396  G loss  0.021277267\n",
      "Epoch  5397  G loss  0.011972435\n",
      "Epoch  5398  G loss  0.030461641\n",
      "Epoch  5399  G loss  0.038833983\n",
      "Epoch  5400  G loss  0.10650769\n",
      "Epoch  5401  G loss  0.04747021\n",
      "Epoch  5402  G loss  0.007475091\n",
      "Epoch  5403  G loss  0.009797858\n",
      "Epoch  5404  G loss  0.022392077\n",
      "Epoch  5405  G loss  0.008623101\n",
      "Epoch  5406  G loss  0.074524075\n",
      "Epoch  5407  G loss  0.037614003\n",
      "Epoch  5408  G loss  0.05839722\n",
      "Epoch  5409  G loss  0.018750146\n",
      "Epoch  5410  G loss  0.092633195\n",
      "Epoch  5411  G loss  0.3396147\n",
      "Epoch  5412  G loss  0.28119385\n",
      "Epoch  5413  G loss  0.038049474\n",
      "Epoch  5414  G loss  0.020406127\n",
      "Epoch  5415  G loss  0.049519744\n",
      "Epoch  5416  G loss  0.018638171\n",
      "Epoch  5417  G loss  0.007266919\n",
      "Epoch  5418  G loss  0.009842797\n",
      "Epoch  5419  G loss  0.010082952\n",
      "Epoch  5420  G loss  0.12131299\n",
      "Epoch  5421  G loss  0.08520104\n",
      "Epoch  5422  G loss  0.013432268\n",
      "Epoch  5423  G loss  0.011344715\n",
      "Epoch  5424  G loss  0.038133446\n",
      "Epoch  5425  G loss  0.020102724\n",
      "Epoch  5426  G loss  0.0150829535\n",
      "Epoch  5427  G loss  0.006893302\n",
      "Epoch  5428  G loss  0.015256032\n",
      "Epoch  5429  G loss  0.015075633\n",
      "Epoch  5430  G loss  0.062350087\n",
      "Epoch  5431  G loss  0.01616661\n",
      "Epoch  5432  G loss  0.0047745304\n",
      "Epoch  5433  G loss  0.06808687\n",
      "Epoch  5434  G loss  0.01578034\n",
      "Epoch  5435  G loss  0.027218193\n",
      "Epoch  5436  G loss  0.035884686\n",
      "Epoch  5437  G loss  0.018808752\n",
      "Epoch  5438  G loss  0.049536847\n",
      "Epoch  5439  G loss  0.016039914\n",
      "Epoch  5440  G loss  0.006316174\n",
      "Epoch  5441  G loss  0.037447702\n",
      "Epoch  5442  G loss  0.006312763\n",
      "Epoch  5443  G loss  0.012777595\n",
      "Epoch  5444  G loss  0.017319415\n",
      "Epoch  5445  G loss  0.0040420773\n",
      "Epoch  5446  G loss  0.49047238\n",
      "Epoch  5447  G loss  0.030803807\n",
      "Epoch  5448  G loss  0.015570214\n",
      "Epoch  5449  G loss  0.006699646\n",
      "Epoch  5450  G loss  0.053927645\n",
      "Epoch  5451  G loss  0.008560369\n",
      "Epoch  5452  G loss  0.013361869\n",
      "Epoch  5453  G loss  0.022343308\n",
      "Epoch  5454  G loss  0.15814266\n",
      "Epoch  5455  G loss  0.07054083\n",
      "Epoch  5456  G loss  0.05189058\n",
      "Epoch  5457  G loss  0.015921792\n",
      "Epoch  5458  G loss  0.013624642\n",
      "Epoch  5459  G loss  0.02385924\n",
      "Epoch  5460  G loss  0.0058612553\n",
      "Epoch  5461  G loss  0.013666829\n",
      "Epoch  5462  G loss  0.016220959\n",
      "Epoch  5463  G loss  0.070508495\n",
      "Epoch  5464  G loss  0.009961292\n",
      "Epoch  5465  G loss  0.019457884\n",
      "Epoch  5466  G loss  0.07177244\n",
      "Epoch  5467  G loss  0.009332895\n",
      "Epoch  5468  G loss  0.0143603105\n",
      "Epoch  5469  G loss  0.04391987\n",
      "Epoch  5470  G loss  0.004412261\n",
      "Epoch  5471  G loss  0.10951164\n",
      "Epoch  5472  G loss  0.014580546\n",
      "Epoch  5473  G loss  0.05857999\n",
      "Epoch  5474  G loss  0.008787038\n",
      "Epoch  5475  G loss  0.011265341\n",
      "Epoch  5476  G loss  0.014893236\n",
      "Epoch  5477  G loss  0.025263092\n",
      "Epoch  5478  G loss  0.033216275\n",
      "Epoch  5479  G loss  0.028926095\n",
      "Epoch  5480  G loss  0.018959947\n",
      "Epoch  5481  G loss  0.15092283\n",
      "Epoch  5482  G loss  0.017450847\n",
      "Epoch  5483  G loss  0.030787237\n",
      "Epoch  5484  G loss  0.04058917\n",
      "Epoch  5485  G loss  0.09804009\n",
      "Epoch  5486  G loss  0.048616983\n",
      "Epoch  5487  G loss  0.037124567\n",
      "Epoch  5488  G loss  0.06160782\n",
      "Epoch  5489  G loss  0.21108489\n",
      "Epoch  5490  G loss  0.048684254\n",
      "Epoch  5491  G loss  0.054670617\n",
      "Epoch  5492  G loss  0.008465715\n",
      "Epoch  5493  G loss  0.014757224\n",
      "Epoch  5494  G loss  0.15126544\n",
      "Epoch  5495  G loss  0.027470788\n",
      "Epoch  5496  G loss  0.22161576\n",
      "Epoch  5497  G loss  0.02662852\n",
      "Epoch  5498  G loss  0.016228236\n",
      "Epoch  5499  G loss  0.010494657\n",
      "Epoch  5500  G loss  0.02044699\n",
      "Epoch  5501  G loss  0.055858277\n",
      "Epoch  5502  G loss  0.06585791\n",
      "Epoch  5503  G loss  0.009767012\n",
      "Epoch  5504  G loss  0.04424011\n",
      "Epoch  5505  G loss  0.22051795\n",
      "Epoch  5506  G loss  0.09604503\n",
      "Epoch  5507  G loss  0.050453804\n",
      "Epoch  5508  G loss  0.15019003\n",
      "Epoch  5509  G loss  0.05911843\n",
      "Epoch  5510  G loss  0.015525408\n",
      "Epoch  5511  G loss  0.14709839\n",
      "Epoch  5512  G loss  0.017439514\n",
      "Epoch  5513  G loss  0.018325146\n",
      "Epoch  5514  G loss  0.0054713227\n",
      "Epoch  5515  G loss  0.04514634\n",
      "Epoch  5516  G loss  0.009009431\n",
      "Epoch  5517  G loss  0.021270245\n",
      "Epoch  5518  G loss  0.024992991\n",
      "Epoch  5519  G loss  0.0033461463\n",
      "Epoch  5520  G loss  0.029629476\n",
      "Epoch  5521  G loss  0.014075788\n",
      "Epoch  5522  G loss  0.080563664\n",
      "Epoch  5523  G loss  0.028032172\n",
      "Epoch  5524  G loss  0.0075704027\n",
      "Epoch  5525  G loss  0.14045063\n",
      "Epoch  5526  G loss  0.08760789\n",
      "Epoch  5527  G loss  0.03560571\n",
      "Epoch  5528  G loss  0.05875902\n",
      "Epoch  5529  G loss  0.27131826\n",
      "Epoch  5530  G loss  0.13614394\n",
      "Epoch  5531  G loss  0.09242438\n",
      "Epoch  5532  G loss  0.083138734\n",
      "Epoch  5533  G loss  0.022577472\n",
      "Epoch  5534  G loss  0.12306647\n",
      "Epoch  5535  G loss  0.022126792\n",
      "Epoch  5536  G loss  0.048951\n",
      "Epoch  5537  G loss  0.025614226\n",
      "Epoch  5538  G loss  0.015380875\n",
      "Epoch  5539  G loss  0.010362065\n",
      "Epoch  5540  G loss  0.23950231\n",
      "Epoch  5541  G loss  0.006379945\n",
      "Epoch  5542  G loss  0.01539973\n",
      "Epoch  5543  G loss  0.014754749\n",
      "Epoch  5544  G loss  0.0028217784\n",
      "Epoch  5545  G loss  0.029545447\n",
      "Epoch  5546  G loss  0.0067009996\n",
      "Epoch  5547  G loss  0.07286824\n",
      "Epoch  5548  G loss  0.04227683\n",
      "Epoch  5549  G loss  0.0192631\n",
      "Epoch  5550  G loss  0.049062096\n",
      "Epoch  5551  G loss  0.018877491\n",
      "Epoch  5552  G loss  0.060396336\n",
      "Epoch  5553  G loss  0.06552659\n",
      "Epoch  5554  G loss  0.007913314\n",
      "Epoch  5555  G loss  0.048032932\n",
      "Epoch  5556  G loss  0.04013451\n",
      "Epoch  5557  G loss  0.016840644\n",
      "Epoch  5558  G loss  0.16241491\n",
      "Epoch  5559  G loss  0.014839908\n",
      "Epoch  5560  G loss  0.0068567796\n",
      "Epoch  5561  G loss  0.053057574\n",
      "Epoch  5562  G loss  0.09510028\n",
      "Epoch  5563  G loss  0.10659737\n",
      "Epoch  5564  G loss  0.12039327\n",
      "Epoch  5565  G loss  0.07733834\n",
      "Epoch  5566  G loss  0.028524008\n",
      "Epoch  5567  G loss  0.07779904\n",
      "Epoch  5568  G loss  0.042932037\n",
      "Epoch  5569  G loss  0.11016175\n",
      "Epoch  5570  G loss  0.06019534\n",
      "Epoch  5571  G loss  0.024347054\n",
      "Epoch  5572  G loss  0.028602634\n",
      "Epoch  5573  G loss  0.017040703\n",
      "Epoch  5574  G loss  0.10360967\n",
      "Epoch  5575  G loss  0.23178121\n",
      "Epoch  5576  G loss  0.029369544\n",
      "Epoch  5577  G loss  0.05304898\n",
      "Epoch  5578  G loss  0.10219528\n",
      "Epoch  5579  G loss  0.0045118844\n",
      "Epoch  5580  G loss  0.009672234\n",
      "Epoch  5581  G loss  0.016387504\n",
      "Epoch  5582  G loss  0.016522048\n",
      "Epoch  5583  G loss  0.008084745\n",
      "Epoch  5584  G loss  0.048694056\n",
      "Epoch  5585  G loss  0.0046383906\n",
      "Epoch  5586  G loss  0.008358613\n",
      "Epoch  5587  G loss  0.040303193\n",
      "Epoch  5588  G loss  0.013567777\n",
      "Epoch  5589  G loss  0.11744359\n",
      "Epoch  5590  G loss  0.007101389\n",
      "Epoch  5591  G loss  0.24066451\n",
      "Epoch  5592  G loss  0.12025638\n",
      "Epoch  5593  G loss  0.035657406\n",
      "Epoch  5594  G loss  0.023319842\n",
      "Epoch  5595  G loss  0.005795666\n",
      "Epoch  5596  G loss  0.13373595\n",
      "Epoch  5597  G loss  0.034011625\n",
      "Epoch  5598  G loss  0.013601844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  5599  G loss  0.05828759\n",
      "Epoch  5600  G loss  0.056613542\n",
      "Epoch  5601  G loss  0.14005749\n",
      "Epoch  5602  G loss  0.02334781\n",
      "Epoch  5603  G loss  0.084960386\n",
      "Epoch  5604  G loss  0.024540108\n",
      "Epoch  5605  G loss  0.01240787\n",
      "Epoch  5606  G loss  0.024129488\n",
      "Epoch  5607  G loss  0.0868923\n",
      "Epoch  5608  G loss  0.01273994\n",
      "Epoch  5609  G loss  0.020532787\n",
      "Epoch  5610  G loss  0.014185877\n",
      "Epoch  5611  G loss  0.018716458\n",
      "Epoch  5612  G loss  0.03920346\n",
      "Epoch  5613  G loss  0.039033905\n",
      "Epoch  5614  G loss  0.020554006\n",
      "Epoch  5615  G loss  0.031854518\n",
      "Epoch  5616  G loss  0.040610127\n",
      "Epoch  5617  G loss  0.013241188\n",
      "Epoch  5618  G loss  0.011217708\n",
      "Epoch  5619  G loss  0.09683656\n",
      "Epoch  5620  G loss  0.006618548\n",
      "Epoch  5621  G loss  0.19617322\n",
      "Epoch  5622  G loss  0.03086931\n",
      "Epoch  5623  G loss  0.08200711\n",
      "Epoch  5624  G loss  0.0063803243\n",
      "Epoch  5625  G loss  0.029221108\n",
      "Epoch  5626  G loss  0.062381864\n",
      "Epoch  5627  G loss  0.049637854\n",
      "Epoch  5628  G loss  0.013477145\n",
      "Epoch  5629  G loss  0.009563097\n",
      "Epoch  5630  G loss  0.07982465\n",
      "Epoch  5631  G loss  0.12333001\n",
      "Epoch  5632  G loss  0.03490884\n",
      "Epoch  5633  G loss  0.0848379\n",
      "Epoch  5634  G loss  0.05057987\n",
      "Epoch  5635  G loss  0.0059799775\n",
      "Epoch  5636  G loss  0.17887568\n",
      "Epoch  5637  G loss  0.028032415\n",
      "Epoch  5638  G loss  0.006486743\n",
      "Epoch  5639  G loss  0.048859343\n",
      "Epoch  5640  G loss  0.041771375\n",
      "Epoch  5641  G loss  0.01090044\n",
      "Epoch  5642  G loss  0.057394624\n",
      "Epoch  5643  G loss  0.019080743\n",
      "Epoch  5644  G loss  0.011667052\n",
      "Epoch  5645  G loss  0.04271678\n",
      "Epoch  5646  G loss  0.027403042\n",
      "Epoch  5647  G loss  0.026316896\n",
      "Epoch  5648  G loss  0.0354946\n",
      "Epoch  5649  G loss  0.021411177\n",
      "Epoch  5650  G loss  0.09251454\n",
      "Epoch  5651  G loss  0.03701347\n",
      "Epoch  5652  G loss  0.013993378\n",
      "Epoch  5653  G loss  0.06690076\n",
      "Epoch  5654  G loss  0.068050735\n",
      "Epoch  5655  G loss  0.01198351\n",
      "Epoch  5656  G loss  0.17143707\n",
      "Epoch  5657  G loss  0.07230027\n",
      "Epoch  5658  G loss  0.034355458\n",
      "Epoch  5659  G loss  0.017670117\n",
      "Epoch  5660  G loss  0.017732263\n",
      "Epoch  5661  G loss  0.019918755\n",
      "Epoch  5662  G loss  0.11333754\n",
      "Epoch  5663  G loss  0.10307663\n",
      "Epoch  5664  G loss  0.015627882\n",
      "Epoch  5665  G loss  0.009901674\n",
      "Epoch  5666  G loss  0.014768058\n",
      "Epoch  5667  G loss  0.021382567\n",
      "Epoch  5668  G loss  0.07154332\n",
      "Epoch  5669  G loss  0.049852826\n",
      "Epoch  5670  G loss  0.3034516\n",
      "Epoch  5671  G loss  0.02722014\n",
      "Epoch  5672  G loss  0.04128926\n",
      "Epoch  5673  G loss  0.052187525\n",
      "Epoch  5674  G loss  0.083287664\n",
      "Epoch  5675  G loss  0.012989705\n",
      "Epoch  5676  G loss  0.124542035\n",
      "Epoch  5677  G loss  0.2017416\n",
      "Epoch  5678  G loss  0.030953059\n",
      "Epoch  5679  G loss  0.012674289\n",
      "Epoch  5680  G loss  0.22538626\n",
      "Epoch  5681  G loss  0.09627069\n",
      "Epoch  5682  G loss  0.037249014\n",
      "Epoch  5683  G loss  0.1663226\n",
      "Epoch  5684  G loss  0.08862649\n",
      "Epoch  5685  G loss  0.013911102\n",
      "Epoch  5686  G loss  0.009276435\n",
      "Epoch  5687  G loss  0.008676808\n",
      "Epoch  5688  G loss  0.03391962\n",
      "Epoch  5689  G loss  0.0377127\n",
      "Epoch  5690  G loss  0.12296677\n",
      "Epoch  5691  G loss  0.033073418\n",
      "Epoch  5692  G loss  0.018901173\n",
      "Epoch  5693  G loss  0.06342002\n",
      "Epoch  5694  G loss  0.03522689\n",
      "Epoch  5695  G loss  0.014546068\n",
      "Epoch  5696  G loss  0.096466795\n",
      "Epoch  5697  G loss  0.032431312\n",
      "Epoch  5698  G loss  0.041369326\n",
      "Epoch  5699  G loss  0.06525515\n",
      "Epoch  5700  G loss  0.25463995\n",
      "Epoch  5701  G loss  0.059579715\n",
      "Epoch  5702  G loss  0.009573808\n",
      "Epoch  5703  G loss  0.107865565\n",
      "Epoch  5704  G loss  0.17253546\n",
      "Epoch  5705  G loss  0.04508507\n",
      "Epoch  5706  G loss  0.005932978\n",
      "Epoch  5707  G loss  0.011039447\n",
      "Epoch  5708  G loss  0.07886498\n",
      "Epoch  5709  G loss  0.024399467\n",
      "Epoch  5710  G loss  0.06242106\n",
      "Epoch  5711  G loss  0.11859658\n",
      "Epoch  5712  G loss  0.050080743\n",
      "Epoch  5713  G loss  0.04569379\n",
      "Epoch  5714  G loss  0.0136330575\n",
      "Epoch  5715  G loss  0.017782588\n",
      "Epoch  5716  G loss  0.13987148\n",
      "Epoch  5717  G loss  0.053259023\n",
      "Epoch  5718  G loss  0.024091475\n",
      "Epoch  5719  G loss  0.014639834\n",
      "Epoch  5720  G loss  0.075812\n",
      "Epoch  5721  G loss  0.052290097\n",
      "Epoch  5722  G loss  0.00747604\n",
      "Epoch  5723  G loss  0.007368258\n",
      "Epoch  5724  G loss  0.08108646\n",
      "Epoch  5725  G loss  0.015744183\n",
      "Epoch  5726  G loss  0.04404526\n",
      "Epoch  5727  G loss  0.00516529\n",
      "Epoch  5728  G loss  0.1518745\n",
      "Epoch  5729  G loss  0.29501554\n",
      "Epoch  5730  G loss  0.02422192\n",
      "Epoch  5731  G loss  0.12143513\n",
      "Epoch  5732  G loss  0.035584137\n",
      "Epoch  5733  G loss  0.017880876\n",
      "Epoch  5734  G loss  0.02327853\n",
      "Epoch  5735  G loss  0.019690597\n",
      "Epoch  5736  G loss  0.016464453\n",
      "Epoch  5737  G loss  0.0075037726\n",
      "Epoch  5738  G loss  0.045827307\n",
      "Epoch  5739  G loss  0.2707138\n",
      "Epoch  5740  G loss  0.03240958\n",
      "Epoch  5741  G loss  0.0703866\n",
      "Epoch  5742  G loss  0.022715082\n",
      "Epoch  5743  G loss  0.027346034\n",
      "Epoch  5744  G loss  0.044147313\n",
      "Epoch  5745  G loss  0.025338076\n",
      "Epoch  5746  G loss  0.0049832915\n",
      "Epoch  5747  G loss  0.03364575\n",
      "Epoch  5748  G loss  0.018313378\n",
      "Epoch  5749  G loss  0.15855318\n",
      "Epoch  5750  G loss  0.012974571\n",
      "Epoch  5751  G loss  0.017952733\n",
      "Epoch  5752  G loss  0.033163544\n",
      "Epoch  5753  G loss  0.030878048\n",
      "Epoch  5754  G loss  0.008394392\n",
      "Epoch  5755  G loss  0.2485666\n",
      "Epoch  5756  G loss  0.015097537\n",
      "Epoch  5757  G loss  0.010550371\n",
      "Epoch  5758  G loss  0.012778676\n",
      "Epoch  5759  G loss  0.024251089\n",
      "Epoch  5760  G loss  0.013051049\n",
      "Epoch  5761  G loss  0.01962468\n",
      "Epoch  5762  G loss  0.08074699\n",
      "Epoch  5763  G loss  0.015022716\n",
      "Epoch  5764  G loss  0.009445165\n",
      "Epoch  5765  G loss  0.020013414\n",
      "Epoch  5766  G loss  0.00875265\n",
      "Epoch  5767  G loss  0.03948311\n",
      "Epoch  5768  G loss  0.046483934\n",
      "Epoch  5769  G loss  0.05183179\n",
      "Epoch  5770  G loss  0.022510616\n",
      "Epoch  5771  G loss  0.06395152\n",
      "Epoch  5772  G loss  0.019665698\n",
      "Epoch  5773  G loss  0.010621656\n",
      "Epoch  5774  G loss  0.02285463\n",
      "Epoch  5775  G loss  0.030157544\n",
      "Epoch  5776  G loss  0.0459435\n",
      "Epoch  5777  G loss  0.017830893\n",
      "Epoch  5778  G loss  0.014907289\n",
      "Epoch  5779  G loss  0.065569505\n",
      "Epoch  5780  G loss  0.0066995285\n",
      "Epoch  5781  G loss  0.043476023\n",
      "Epoch  5782  G loss  0.028473664\n",
      "Epoch  5783  G loss  0.23202163\n",
      "Epoch  5784  G loss  0.052832454\n",
      "Epoch  5785  G loss  0.018026697\n",
      "Epoch  5786  G loss  0.046695076\n",
      "Epoch  5787  G loss  0.015054677\n",
      "Epoch  5788  G loss  0.14065227\n",
      "Epoch  5789  G loss  0.04844917\n",
      "Epoch  5790  G loss  0.010942449\n",
      "Epoch  5791  G loss  0.0045047477\n",
      "Epoch  5792  G loss  0.21758442\n",
      "Epoch  5793  G loss  0.07298471\n",
      "Epoch  5794  G loss  0.015441387\n",
      "Epoch  5795  G loss  0.0700763\n",
      "Epoch  5796  G loss  0.09896718\n",
      "Epoch  5797  G loss  0.063446745\n",
      "Epoch  5798  G loss  0.09027468\n",
      "Epoch  5799  G loss  0.076130345\n",
      "Epoch  5800  G loss  0.03516276\n",
      "Epoch  5801  G loss  0.036964107\n",
      "Epoch  5802  G loss  0.07275491\n",
      "Epoch  5803  G loss  0.03849253\n",
      "Epoch  5804  G loss  0.08355247\n",
      "Epoch  5805  G loss  0.014261838\n",
      "Epoch  5806  G loss  0.018683836\n",
      "Epoch  5807  G loss  0.034108818\n",
      "Epoch  5808  G loss  0.009337897\n",
      "Epoch  5809  G loss  0.018232701\n",
      "Epoch  5810  G loss  0.0059604887\n",
      "Epoch  5811  G loss  0.021799875\n",
      "Epoch  5812  G loss  0.030583031\n",
      "Epoch  5813  G loss  0.017202964\n",
      "Epoch  5814  G loss  0.035758853\n",
      "Epoch  5815  G loss  0.0033385977\n",
      "Epoch  5816  G loss  0.031431686\n",
      "Epoch  5817  G loss  0.021766348\n",
      "Epoch  5818  G loss  0.060133934\n",
      "Epoch  5819  G loss  0.024803143\n",
      "Epoch  5820  G loss  0.025881566\n",
      "Epoch  5821  G loss  0.014504731\n",
      "Epoch  5822  G loss  0.015308283\n",
      "Epoch  5823  G loss  0.009373682\n",
      "Epoch  5824  G loss  0.049427655\n",
      "Epoch  5825  G loss  0.044724427\n",
      "Epoch  5826  G loss  0.011925256\n",
      "Epoch  5827  G loss  0.040202737\n",
      "Epoch  5828  G loss  0.018650074\n",
      "Epoch  5829  G loss  0.028814571\n",
      "Epoch  5830  G loss  0.058397993\n",
      "Epoch  5831  G loss  0.044126175\n",
      "Epoch  5832  G loss  0.053715415\n",
      "Epoch  5833  G loss  0.018506918\n",
      "Epoch  5834  G loss  0.06574858\n",
      "Epoch  5835  G loss  0.022495894\n",
      "Epoch  5836  G loss  0.0075249337\n",
      "Epoch  5837  G loss  0.015708499\n",
      "Epoch  5838  G loss  0.012437122\n",
      "Epoch  5839  G loss  0.022051034\n",
      "Epoch  5840  G loss  0.03005924\n",
      "Epoch  5841  G loss  0.029724073\n",
      "Epoch  5842  G loss  0.027684793\n",
      "Epoch  5843  G loss  0.03320659\n",
      "Epoch  5844  G loss  0.058356807\n",
      "Epoch  5845  G loss  0.16056347\n",
      "Epoch  5846  G loss  0.014846073\n",
      "Epoch  5847  G loss  0.044736728\n",
      "Epoch  5848  G loss  0.024065278\n",
      "Epoch  5849  G loss  0.28520685\n",
      "Epoch  5850  G loss  0.01925911\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  5851  G loss  0.034104567\n",
      "Epoch  5852  G loss  0.01041742\n",
      "Epoch  5853  G loss  0.07301584\n",
      "Epoch  5854  G loss  0.19254638\n",
      "Epoch  5855  G loss  0.00858525\n",
      "Epoch  5856  G loss  0.0074634855\n",
      "Epoch  5857  G loss  0.012967572\n",
      "Epoch  5858  G loss  0.006999396\n",
      "Epoch  5859  G loss  0.05855053\n",
      "Epoch  5860  G loss  0.018909946\n",
      "Epoch  5861  G loss  0.12532127\n",
      "Epoch  5862  G loss  0.043623433\n",
      "Epoch  5863  G loss  0.0039500906\n",
      "Epoch  5864  G loss  0.015747536\n",
      "Epoch  5865  G loss  0.25945547\n",
      "Epoch  5866  G loss  0.054189093\n",
      "Epoch  5867  G loss  0.034843735\n",
      "Epoch  5868  G loss  0.13391826\n",
      "Epoch  5869  G loss  0.021921903\n",
      "Epoch  5870  G loss  0.028517935\n",
      "Epoch  5871  G loss  0.011917235\n",
      "Epoch  5872  G loss  0.13223885\n",
      "Epoch  5873  G loss  0.072450906\n",
      "Epoch  5874  G loss  0.013948478\n",
      "Epoch  5875  G loss  0.32726824\n",
      "Epoch  5876  G loss  0.011656765\n",
      "Epoch  5877  G loss  0.06155563\n",
      "Epoch  5878  G loss  0.10868318\n",
      "Epoch  5879  G loss  0.028345801\n",
      "Epoch  5880  G loss  0.010491505\n",
      "Epoch  5881  G loss  0.01022305\n",
      "Epoch  5882  G loss  0.12901941\n",
      "Epoch  5883  G loss  0.014552997\n",
      "Epoch  5884  G loss  0.039313424\n",
      "Epoch  5885  G loss  0.025095835\n",
      "Epoch  5886  G loss  0.10458194\n",
      "Epoch  5887  G loss  0.024220861\n",
      "Epoch  5888  G loss  0.06556015\n",
      "Epoch  5889  G loss  0.015728105\n",
      "Epoch  5890  G loss  0.21691126\n",
      "Epoch  5891  G loss  0.06738827\n",
      "Epoch  5892  G loss  0.01920979\n",
      "Epoch  5893  G loss  0.028103586\n",
      "Epoch  5894  G loss  0.057939805\n",
      "Epoch  5895  G loss  0.02843959\n",
      "Epoch  5896  G loss  0.011634295\n",
      "Epoch  5897  G loss  0.1270896\n",
      "Epoch  5898  G loss  0.010783978\n",
      "Epoch  5899  G loss  0.03518276\n",
      "Epoch  5900  G loss  0.008850969\n",
      "Epoch  5901  G loss  0.067471474\n",
      "Epoch  5902  G loss  0.008882141\n",
      "Epoch  5903  G loss  0.02787072\n",
      "Epoch  5904  G loss  0.020544719\n",
      "Epoch  5905  G loss  0.04074446\n",
      "Epoch  5906  G loss  0.030931553\n",
      "Epoch  5907  G loss  0.0827496\n",
      "Epoch  5908  G loss  0.115419105\n",
      "Epoch  5909  G loss  0.04354778\n",
      "Epoch  5910  G loss  0.011716204\n",
      "Epoch  5911  G loss  0.030420367\n",
      "Epoch  5912  G loss  0.07038089\n",
      "Epoch  5913  G loss  0.035510607\n",
      "Epoch  5914  G loss  0.03893958\n",
      "Epoch  5915  G loss  0.023140265\n",
      "Epoch  5916  G loss  0.085920714\n",
      "Epoch  5917  G loss  0.071894325\n",
      "Epoch  5918  G loss  0.010146127\n",
      "Epoch  5919  G loss  0.07953283\n",
      "Epoch  5920  G loss  0.15542316\n",
      "Epoch  5921  G loss  0.014974424\n",
      "Epoch  5922  G loss  0.05780387\n",
      "Epoch  5923  G loss  0.14251772\n",
      "Epoch  5924  G loss  0.022500232\n",
      "Epoch  5925  G loss  0.020237647\n",
      "Epoch  5926  G loss  0.011237048\n",
      "Epoch  5927  G loss  0.10474757\n",
      "Epoch  5928  G loss  0.007831693\n",
      "Epoch  5929  G loss  0.010233281\n",
      "Epoch  5930  G loss  0.013835875\n",
      "Epoch  5931  G loss  0.03350766\n",
      "Epoch  5932  G loss  0.03623991\n",
      "Epoch  5933  G loss  0.05989452\n",
      "Epoch  5934  G loss  0.18728653\n",
      "Epoch  5935  G loss  0.075078234\n",
      "Epoch  5936  G loss  0.011434554\n",
      "Epoch  5937  G loss  0.009915106\n",
      "Epoch  5938  G loss  0.18432872\n",
      "Epoch  5939  G loss  0.1028559\n",
      "Epoch  5940  G loss  0.018024392\n",
      "Epoch  5941  G loss  0.05403518\n",
      "Epoch  5942  G loss  0.01822556\n",
      "Epoch  5943  G loss  0.10178334\n",
      "Epoch  5944  G loss  0.13729566\n",
      "Epoch  5945  G loss  0.04280105\n",
      "Epoch  5946  G loss  0.03298594\n",
      "Epoch  5947  G loss  0.014554315\n",
      "Epoch  5948  G loss  0.027319914\n",
      "Epoch  5949  G loss  0.022097632\n",
      "Epoch  5950  G loss  0.15375975\n",
      "Epoch  5951  G loss  0.026253225\n",
      "Epoch  5952  G loss  0.16064711\n",
      "Epoch  5953  G loss  0.13177063\n",
      "Epoch  5954  G loss  0.035833947\n",
      "Epoch  5955  G loss  0.013341864\n",
      "Epoch  5956  G loss  0.004981815\n",
      "Epoch  5957  G loss  0.042331204\n",
      "Epoch  5958  G loss  0.0480196\n",
      "Epoch  5959  G loss  0.06666872\n",
      "Epoch  5960  G loss  0.03246521\n",
      "Epoch  5961  G loss  0.1766449\n",
      "Epoch  5962  G loss  0.07041013\n",
      "Epoch  5963  G loss  0.016709523\n",
      "Epoch  5964  G loss  0.012702441\n",
      "Epoch  5965  G loss  0.15122896\n",
      "Epoch  5966  G loss  0.01591639\n",
      "Epoch  5967  G loss  0.07116816\n",
      "Epoch  5968  G loss  0.011758996\n",
      "Epoch  5969  G loss  0.020183068\n",
      "Epoch  5970  G loss  0.013191731\n",
      "Epoch  5971  G loss  0.043875046\n",
      "Epoch  5972  G loss  0.019734345\n",
      "Epoch  5973  G loss  0.023525063\n",
      "Epoch  5974  G loss  0.11438482\n",
      "Epoch  5975  G loss  0.059925925\n",
      "Epoch  5976  G loss  0.0060249586\n",
      "Epoch  5977  G loss  0.01696309\n",
      "Epoch  5978  G loss  0.011236696\n",
      "Epoch  5979  G loss  0.012211889\n",
      "Epoch  5980  G loss  0.032143395\n",
      "Epoch  5981  G loss  0.04558127\n",
      "Epoch  5982  G loss  0.027160548\n",
      "Epoch  5983  G loss  0.085227236\n",
      "Epoch  5984  G loss  0.025490232\n",
      "Epoch  5985  G loss  0.03707443\n",
      "Epoch  5986  G loss  0.048334584\n",
      "Epoch  5987  G loss  0.01068436\n",
      "Epoch  5988  G loss  0.004205992\n",
      "Epoch  5989  G loss  0.03549234\n",
      "Epoch  5990  G loss  0.028349651\n",
      "Epoch  5991  G loss  0.052701503\n",
      "Epoch  5992  G loss  0.02926567\n",
      "Epoch  5993  G loss  0.08825066\n",
      "Epoch  5994  G loss  0.07241091\n",
      "Epoch  5995  G loss  0.04963329\n",
      "Epoch  5996  G loss  0.021120418\n",
      "Epoch  5997  G loss  0.00856705\n",
      "Epoch  5998  G loss  0.061507292\n",
      "Epoch  5999  G loss  0.013133478\n",
      "Epoch  6000  G loss  0.08201547\n",
      "Epoch  6001  G loss  0.18601349\n",
      "Epoch  6002  G loss  0.07094299\n",
      "Epoch  6003  G loss  0.072319314\n",
      "Epoch  6004  G loss  0.10746325\n",
      "Epoch  6005  G loss  0.017827284\n",
      "Epoch  6006  G loss  0.02599596\n",
      "Epoch  6007  G loss  0.13521579\n",
      "Epoch  6008  G loss  0.010923231\n",
      "Epoch  6009  G loss  0.016620167\n",
      "Epoch  6010  G loss  0.028359728\n",
      "Epoch  6011  G loss  0.03856363\n",
      "Epoch  6012  G loss  0.0035083196\n",
      "Epoch  6013  G loss  0.051140048\n",
      "Epoch  6014  G loss  0.049768418\n",
      "Epoch  6015  G loss  0.07189333\n",
      "Epoch  6016  G loss  0.23517804\n",
      "Epoch  6017  G loss  0.020261602\n",
      "Epoch  6018  G loss  0.025262602\n",
      "Epoch  6019  G loss  0.08050668\n",
      "Epoch  6020  G loss  0.08976169\n",
      "Epoch  6021  G loss  0.052821074\n",
      "Epoch  6022  G loss  0.034475222\n",
      "Epoch  6023  G loss  0.10151587\n",
      "Epoch  6024  G loss  0.016268808\n",
      "Epoch  6025  G loss  0.022694834\n",
      "Epoch  6026  G loss  0.00774681\n",
      "Epoch  6027  G loss  0.047687978\n",
      "Epoch  6028  G loss  0.02045926\n",
      "Epoch  6029  G loss  0.054906685\n",
      "Epoch  6030  G loss  0.21630073\n",
      "Epoch  6031  G loss  0.0050926367\n",
      "Epoch  6032  G loss  0.0147796655\n",
      "Epoch  6033  G loss  0.07070537\n",
      "Epoch  6034  G loss  0.11815092\n",
      "Epoch  6035  G loss  0.07182195\n",
      "Epoch  6036  G loss  0.0066914633\n",
      "Epoch  6037  G loss  0.039662607\n",
      "Epoch  6038  G loss  0.017670076\n",
      "Epoch  6039  G loss  0.019562636\n",
      "Epoch  6040  G loss  0.012099678\n",
      "Epoch  6041  G loss  0.009014862\n",
      "Epoch  6042  G loss  0.009377919\n",
      "Epoch  6043  G loss  0.016035603\n",
      "Epoch  6044  G loss  0.04753606\n",
      "Epoch  6045  G loss  0.23714513\n",
      "Epoch  6046  G loss  0.04460816\n",
      "Epoch  6047  G loss  0.009892603\n",
      "Epoch  6048  G loss  0.023565274\n",
      "Epoch  6049  G loss  0.046102542\n",
      "Epoch  6050  G loss  0.13124359\n",
      "Epoch  6051  G loss  0.021108204\n",
      "Epoch  6052  G loss  0.017864771\n",
      "Epoch  6053  G loss  0.10587765\n",
      "Epoch  6054  G loss  0.01593731\n",
      "Epoch  6055  G loss  0.03924936\n",
      "Epoch  6056  G loss  0.06680044\n",
      "Epoch  6057  G loss  0.024980104\n",
      "Epoch  6058  G loss  0.01609952\n",
      "Epoch  6059  G loss  0.053710654\n",
      "Epoch  6060  G loss  0.0050067273\n",
      "Epoch  6061  G loss  0.008174876\n",
      "Epoch  6062  G loss  0.047598027\n",
      "Epoch  6063  G loss  0.015798088\n",
      "Epoch  6064  G loss  0.02876778\n",
      "Epoch  6065  G loss  0.010105299\n",
      "Epoch  6066  G loss  0.031591695\n",
      "Epoch  6067  G loss  0.02670923\n",
      "Epoch  6068  G loss  0.01512751\n",
      "Epoch  6069  G loss  0.13494912\n",
      "Epoch  6070  G loss  0.07488365\n",
      "Epoch  6071  G loss  0.013319239\n",
      "Epoch  6072  G loss  0.025103942\n",
      "Epoch  6073  G loss  0.06660575\n",
      "Epoch  6074  G loss  0.010876699\n",
      "Epoch  6075  G loss  0.018702647\n",
      "Epoch  6076  G loss  0.044127747\n",
      "Epoch  6077  G loss  0.011144573\n",
      "Epoch  6078  G loss  0.032429654\n",
      "Epoch  6079  G loss  0.13041131\n",
      "Epoch  6080  G loss  0.15824232\n",
      "Epoch  6081  G loss  0.009681039\n",
      "Epoch  6082  G loss  0.07298835\n",
      "Epoch  6083  G loss  0.060589496\n",
      "Epoch  6084  G loss  0.0070870174\n",
      "Epoch  6085  G loss  0.08677608\n",
      "Epoch  6086  G loss  0.031675696\n",
      "Epoch  6087  G loss  0.009800276\n",
      "Epoch  6088  G loss  0.30642736\n",
      "Epoch  6089  G loss  0.008048831\n",
      "Epoch  6090  G loss  0.13913116\n",
      "Epoch  6091  G loss  0.011408796\n",
      "Epoch  6092  G loss  0.039053597\n",
      "Epoch  6093  G loss  0.01905011\n",
      "Epoch  6094  G loss  0.03259007\n",
      "Epoch  6095  G loss  0.057856753\n",
      "Epoch  6096  G loss  0.013386443\n",
      "Epoch  6097  G loss  0.05837966\n",
      "Epoch  6098  G loss  0.055420466\n",
      "Epoch  6099  G loss  0.03579525\n",
      "Epoch  6100  G loss  0.018695652\n",
      "Epoch  6101  G loss  0.12147416\n",
      "Epoch  6102  G loss  0.06856421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  6103  G loss  0.008012798\n",
      "Epoch  6104  G loss  0.008121924\n",
      "Epoch  6105  G loss  0.08332634\n",
      "Epoch  6106  G loss  0.064183146\n",
      "Epoch  6107  G loss  0.18079531\n",
      "Epoch  6108  G loss  0.041996777\n",
      "Epoch  6109  G loss  0.07786317\n",
      "Epoch  6110  G loss  0.02706523\n",
      "Epoch  6111  G loss  0.057679705\n",
      "Epoch  6112  G loss  0.010185627\n",
      "Epoch  6113  G loss  0.017474651\n",
      "Epoch  6114  G loss  0.019955035\n",
      "Epoch  6115  G loss  0.048595734\n",
      "Epoch  6116  G loss  0.04018545\n",
      "Epoch  6117  G loss  0.019761577\n",
      "Epoch  6118  G loss  0.08838743\n",
      "Epoch  6119  G loss  0.007514825\n",
      "Epoch  6120  G loss  0.05675228\n",
      "Epoch  6121  G loss  0.08618034\n",
      "Epoch  6122  G loss  0.1446702\n",
      "Epoch  6123  G loss  0.024783485\n",
      "Epoch  6124  G loss  0.022128066\n",
      "Epoch  6125  G loss  0.028584924\n",
      "Epoch  6126  G loss  0.007905916\n",
      "Epoch  6127  G loss  0.015398538\n",
      "Epoch  6128  G loss  0.09505109\n",
      "Epoch  6129  G loss  0.03813594\n",
      "Epoch  6130  G loss  0.04442825\n",
      "Epoch  6131  G loss  0.025461812\n",
      "Epoch  6132  G loss  0.0056261458\n",
      "Epoch  6133  G loss  0.0074281255\n",
      "Epoch  6134  G loss  0.005138339\n",
      "Epoch  6135  G loss  0.02988879\n",
      "Epoch  6136  G loss  0.024998754\n",
      "Epoch  6137  G loss  0.01568865\n",
      "Epoch  6138  G loss  0.03154072\n",
      "Epoch  6139  G loss  0.096325606\n",
      "Epoch  6140  G loss  0.16245084\n",
      "Epoch  6141  G loss  0.028344814\n",
      "Epoch  6142  G loss  0.07318235\n",
      "Epoch  6143  G loss  0.032812215\n",
      "Epoch  6144  G loss  0.08516741\n",
      "Epoch  6145  G loss  0.02622682\n",
      "Epoch  6146  G loss  0.011082327\n",
      "Epoch  6147  G loss  0.031472594\n",
      "Epoch  6148  G loss  0.18744937\n",
      "Epoch  6149  G loss  0.008430582\n",
      "Epoch  6150  G loss  0.119692564\n",
      "Epoch  6151  G loss  0.0142329065\n",
      "Epoch  6152  G loss  0.08739716\n",
      "Epoch  6153  G loss  0.023185728\n",
      "Epoch  6154  G loss  0.009568486\n",
      "Epoch  6155  G loss  0.08055506\n",
      "Epoch  6156  G loss  0.041894227\n",
      "Epoch  6157  G loss  0.016687892\n",
      "Epoch  6158  G loss  0.009600248\n",
      "Epoch  6159  G loss  0.15230711\n",
      "Epoch  6160  G loss  0.035958912\n",
      "Epoch  6161  G loss  0.041852567\n",
      "Epoch  6162  G loss  0.108014844\n",
      "Epoch  6163  G loss  0.025069399\n",
      "Epoch  6164  G loss  0.07028192\n",
      "Epoch  6165  G loss  0.009834967\n",
      "Epoch  6166  G loss  0.013076959\n",
      "Epoch  6167  G loss  0.043050174\n",
      "Epoch  6168  G loss  0.04242982\n",
      "Epoch  6169  G loss  0.019059023\n",
      "Epoch  6170  G loss  0.046267226\n",
      "Epoch  6171  G loss  0.049300134\n",
      "Epoch  6172  G loss  0.008779048\n",
      "Epoch  6173  G loss  0.019209873\n",
      "Epoch  6174  G loss  0.011239405\n",
      "Epoch  6175  G loss  0.016014215\n",
      "Epoch  6176  G loss  0.047739044\n",
      "Epoch  6177  G loss  0.029576834\n",
      "Epoch  6178  G loss  0.022667073\n",
      "Epoch  6179  G loss  0.018887002\n",
      "Epoch  6180  G loss  0.0046701836\n",
      "Epoch  6181  G loss  0.05892071\n",
      "Epoch  6182  G loss  0.07934812\n",
      "Epoch  6183  G loss  0.022875872\n",
      "Epoch  6184  G loss  0.019160079\n",
      "Epoch  6185  G loss  0.030999172\n",
      "Epoch  6186  G loss  0.021293929\n",
      "Epoch  6187  G loss  0.020572957\n",
      "Epoch  6188  G loss  0.027802128\n",
      "Epoch  6189  G loss  0.033087794\n",
      "Epoch  6190  G loss  0.008272407\n",
      "Epoch  6191  G loss  0.008818063\n",
      "Epoch  6192  G loss  0.042773884\n",
      "Epoch  6193  G loss  0.015207236\n",
      "Epoch  6194  G loss  0.07523598\n",
      "Epoch  6195  G loss  0.12012854\n",
      "Epoch  6196  G loss  0.030377198\n",
      "Epoch  6197  G loss  0.0075448495\n",
      "Epoch  6198  G loss  0.017672013\n",
      "Epoch  6199  G loss  0.018265005\n",
      "Epoch  6200  G loss  0.01164577\n",
      "Epoch  6201  G loss  0.10894548\n",
      "Epoch  6202  G loss  0.070094526\n",
      "Epoch  6203  G loss  0.009201347\n",
      "Epoch  6204  G loss  0.033618532\n",
      "Epoch  6205  G loss  0.12075044\n",
      "Epoch  6206  G loss  0.030432098\n",
      "Epoch  6207  G loss  0.02651215\n",
      "Epoch  6208  G loss  0.05589742\n",
      "Epoch  6209  G loss  0.15785661\n",
      "Epoch  6210  G loss  0.07955836\n",
      "Epoch  6211  G loss  0.010722133\n",
      "Epoch  6212  G loss  0.12330857\n",
      "Epoch  6213  G loss  0.017457228\n",
      "Epoch  6214  G loss  0.016429864\n",
      "Epoch  6215  G loss  0.0395316\n",
      "Epoch  6216  G loss  0.072862715\n",
      "Epoch  6217  G loss  0.023992106\n",
      "Epoch  6218  G loss  0.020054623\n",
      "Epoch  6219  G loss  0.0150604015\n",
      "Epoch  6220  G loss  0.023343686\n",
      "Epoch  6221  G loss  0.02687095\n",
      "Epoch  6222  G loss  0.17138207\n",
      "Epoch  6223  G loss  0.048260786\n",
      "Epoch  6224  G loss  0.073127866\n",
      "Epoch  6225  G loss  0.018739425\n",
      "Epoch  6226  G loss  0.25507778\n",
      "Epoch  6227  G loss  0.015136085\n",
      "Epoch  6228  G loss  0.21372396\n",
      "Epoch  6229  G loss  0.019264733\n",
      "Epoch  6230  G loss  0.0417214\n",
      "Epoch  6231  G loss  0.04831989\n",
      "Epoch  6232  G loss  0.16770144\n",
      "Epoch  6233  G loss  0.17768003\n",
      "Epoch  6234  G loss  0.009999394\n",
      "Epoch  6235  G loss  0.015963811\n",
      "Epoch  6236  G loss  0.01158187\n",
      "Epoch  6237  G loss  0.048674814\n",
      "Epoch  6238  G loss  0.017808426\n",
      "Epoch  6239  G loss  0.04797154\n",
      "Epoch  6240  G loss  0.08100565\n",
      "Epoch  6241  G loss  0.0035027652\n",
      "Epoch  6242  G loss  0.08501603\n",
      "Epoch  6243  G loss  0.017868238\n",
      "Epoch  6244  G loss  0.032667972\n",
      "Epoch  6245  G loss  0.016720569\n",
      "Epoch  6246  G loss  0.01357873\n",
      "Epoch  6247  G loss  0.01773432\n",
      "Epoch  6248  G loss  0.12156314\n",
      "Epoch  6249  G loss  0.0773317\n",
      "Epoch  6250  G loss  0.013175352\n",
      "Epoch  6251  G loss  0.023462575\n",
      "Epoch  6252  G loss  0.046675038\n",
      "Epoch  6253  G loss  0.016770529\n",
      "Epoch  6254  G loss  0.0241693\n",
      "Epoch  6255  G loss  0.03870544\n",
      "Epoch  6256  G loss  0.026805457\n",
      "Epoch  6257  G loss  0.008341642\n",
      "Epoch  6258  G loss  0.089612275\n",
      "Epoch  6259  G loss  0.006750174\n",
      "Epoch  6260  G loss  0.007125158\n",
      "Epoch  6261  G loss  0.017052542\n",
      "Epoch  6262  G loss  0.0271599\n",
      "Epoch  6263  G loss  0.022746425\n",
      "Epoch  6264  G loss  0.048835397\n",
      "Epoch  6265  G loss  0.0211569\n",
      "Epoch  6266  G loss  0.08589449\n",
      "Epoch  6267  G loss  0.007612097\n",
      "Epoch  6268  G loss  0.016294364\n",
      "Epoch  6269  G loss  0.071626626\n",
      "Epoch  6270  G loss  0.036074795\n",
      "Epoch  6271  G loss  0.008791747\n",
      "Epoch  6272  G loss  0.039452396\n",
      "Epoch  6273  G loss  0.011710083\n",
      "Epoch  6274  G loss  0.084905684\n",
      "Epoch  6275  G loss  0.17485115\n",
      "Epoch  6276  G loss  0.046796534\n",
      "Epoch  6277  G loss  0.17576832\n",
      "Epoch  6278  G loss  0.045850508\n",
      "Epoch  6279  G loss  0.011961379\n",
      "Epoch  6280  G loss  0.036367703\n",
      "Epoch  6281  G loss  0.10407536\n",
      "Epoch  6282  G loss  0.086692765\n",
      "Epoch  6283  G loss  0.007967258\n",
      "Epoch  6284  G loss  0.078370884\n",
      "Epoch  6285  G loss  0.0095145\n",
      "Epoch  6286  G loss  0.060443074\n",
      "Epoch  6287  G loss  0.055926584\n",
      "Epoch  6288  G loss  0.024362613\n",
      "Epoch  6289  G loss  0.05112917\n",
      "Epoch  6290  G loss  0.0053842375\n",
      "Epoch  6291  G loss  0.04099726\n",
      "Epoch  6292  G loss  0.01947539\n",
      "Epoch  6293  G loss  0.051528335\n",
      "Epoch  6294  G loss  0.0912813\n",
      "Epoch  6295  G loss  0.077792704\n",
      "Epoch  6296  G loss  0.02584161\n",
      "Epoch  6297  G loss  0.008594224\n",
      "Epoch  6298  G loss  0.011813279\n",
      "Epoch  6299  G loss  0.040769186\n",
      "Epoch  6300  G loss  0.006221669\n",
      "Epoch  6301  G loss  0.004583245\n",
      "Epoch  6302  G loss  0.01744324\n",
      "Epoch  6303  G loss  0.045027673\n",
      "Epoch  6304  G loss  0.019655596\n",
      "Epoch  6305  G loss  0.02947519\n",
      "Epoch  6306  G loss  0.019563615\n",
      "Epoch  6307  G loss  0.008803177\n",
      "Epoch  6308  G loss  0.080272816\n",
      "Epoch  6309  G loss  0.025750848\n",
      "Epoch  6310  G loss  0.0171981\n",
      "Epoch  6311  G loss  0.06313658\n",
      "Epoch  6312  G loss  0.005497598\n",
      "Epoch  6313  G loss  0.06107721\n",
      "Epoch  6314  G loss  0.07762068\n",
      "Epoch  6315  G loss  0.016182484\n",
      "Epoch  6316  G loss  0.0068442174\n",
      "Epoch  6317  G loss  0.1256851\n",
      "Epoch  6318  G loss  0.035835683\n",
      "Epoch  6319  G loss  0.0229754\n",
      "Epoch  6320  G loss  0.007940721\n",
      "Epoch  6321  G loss  0.023334378\n",
      "Epoch  6322  G loss  0.023485731\n",
      "Epoch  6323  G loss  0.020643855\n",
      "Epoch  6324  G loss  0.31979278\n",
      "Epoch  6325  G loss  0.011912817\n",
      "Epoch  6326  G loss  0.10157494\n",
      "Epoch  6327  G loss  0.02570457\n",
      "Epoch  6328  G loss  0.023617677\n",
      "Epoch  6329  G loss  0.013805801\n",
      "Epoch  6330  G loss  0.05130495\n",
      "Epoch  6331  G loss  0.49762666\n",
      "Epoch  6332  G loss  0.10221438\n",
      "Epoch  6333  G loss  0.029902505\n",
      "Epoch  6334  G loss  0.080999196\n",
      "Epoch  6335  G loss  0.09817177\n",
      "Epoch  6336  G loss  0.014180461\n",
      "Epoch  6337  G loss  0.14662932\n",
      "Epoch  6338  G loss  0.055717178\n",
      "Epoch  6339  G loss  0.08991818\n",
      "Epoch  6340  G loss  0.035987675\n",
      "Epoch  6341  G loss  0.008208378\n",
      "Epoch  6342  G loss  0.03046498\n",
      "Epoch  6343  G loss  0.017108824\n",
      "Epoch  6344  G loss  0.012674992\n",
      "Epoch  6345  G loss  0.21968734\n",
      "Epoch  6346  G loss  0.16995528\n",
      "Epoch  6347  G loss  0.057185303\n",
      "Epoch  6348  G loss  0.03625951\n",
      "Epoch  6349  G loss  0.18008846\n",
      "Epoch  6350  G loss  0.035433777\n",
      "Epoch  6351  G loss  0.0063977167\n",
      "Epoch  6352  G loss  0.06123498\n",
      "Epoch  6353  G loss  0.08337492\n",
      "Epoch  6354  G loss  0.44146872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  6355  G loss  0.0077282237\n",
      "Epoch  6356  G loss  0.026949741\n",
      "Epoch  6357  G loss  0.061234936\n",
      "Epoch  6358  G loss  0.017047398\n",
      "Epoch  6359  G loss  0.01977857\n",
      "Epoch  6360  G loss  0.070512354\n",
      "Epoch  6361  G loss  0.019168563\n",
      "Epoch  6362  G loss  0.007831999\n",
      "Epoch  6363  G loss  0.12422423\n",
      "Epoch  6364  G loss  0.027624685\n",
      "Epoch  6365  G loss  0.010609599\n",
      "Epoch  6366  G loss  0.018981336\n",
      "Epoch  6367  G loss  0.04749933\n",
      "Epoch  6368  G loss  0.024536476\n",
      "Epoch  6369  G loss  0.022476336\n",
      "Epoch  6370  G loss  0.0083682835\n",
      "Epoch  6371  G loss  0.012718333\n",
      "Epoch  6372  G loss  0.014689071\n",
      "Epoch  6373  G loss  0.04222388\n",
      "Epoch  6374  G loss  0.027310867\n",
      "Epoch  6375  G loss  0.04845524\n",
      "Epoch  6376  G loss  0.028806154\n",
      "Epoch  6377  G loss  0.042968623\n",
      "Epoch  6378  G loss  0.012307091\n",
      "Epoch  6379  G loss  0.06337659\n",
      "Epoch  6380  G loss  0.032046434\n",
      "Epoch  6381  G loss  0.015509374\n",
      "Epoch  6382  G loss  0.27031606\n",
      "Epoch  6383  G loss  0.058879882\n",
      "Epoch  6384  G loss  0.079012506\n",
      "Epoch  6385  G loss  0.025198206\n",
      "Epoch  6386  G loss  0.007738323\n",
      "Epoch  6387  G loss  0.031196624\n",
      "Epoch  6388  G loss  0.034988653\n",
      "Epoch  6389  G loss  0.12757823\n",
      "Epoch  6390  G loss  0.030956164\n",
      "Epoch  6391  G loss  0.07409349\n",
      "Epoch  6392  G loss  0.026118264\n",
      "Epoch  6393  G loss  0.08067289\n",
      "Epoch  6394  G loss  0.07200929\n",
      "Epoch  6395  G loss  0.01266299\n",
      "Epoch  6396  G loss  0.012796703\n",
      "Epoch  6397  G loss  0.0296905\n",
      "Epoch  6398  G loss  0.01982458\n",
      "Epoch  6399  G loss  0.026346918\n",
      "Epoch  6400  G loss  0.18998735\n",
      "Epoch  6401  G loss  0.02539035\n",
      "Epoch  6402  G loss  0.008642501\n",
      "Epoch  6403  G loss  0.06477199\n",
      "Epoch  6404  G loss  0.027170496\n",
      "Epoch  6405  G loss  0.0075693657\n",
      "Epoch  6406  G loss  0.04540365\n",
      "Epoch  6407  G loss  0.023787215\n",
      "Epoch  6408  G loss  0.023734063\n",
      "Epoch  6409  G loss  0.008837548\n",
      "Epoch  6410  G loss  0.011546609\n",
      "Epoch  6411  G loss  0.028727997\n",
      "Epoch  6412  G loss  0.025939176\n",
      "Epoch  6413  G loss  0.15560511\n",
      "Epoch  6414  G loss  0.016363716\n",
      "Epoch  6415  G loss  0.014209574\n",
      "Epoch  6416  G loss  0.00998717\n",
      "Epoch  6417  G loss  0.055043913\n",
      "Epoch  6418  G loss  0.040232945\n",
      "Epoch  6419  G loss  0.0068388395\n",
      "Epoch  6420  G loss  0.06567333\n",
      "Epoch  6421  G loss  0.036611874\n",
      "Epoch  6422  G loss  0.057637367\n",
      "Epoch  6423  G loss  0.009785144\n",
      "Epoch  6424  G loss  0.1349234\n",
      "Epoch  6425  G loss  0.35684156\n",
      "Epoch  6426  G loss  0.11598269\n",
      "Epoch  6427  G loss  0.018718354\n",
      "Epoch  6428  G loss  0.008392859\n",
      "Epoch  6429  G loss  0.056919295\n",
      "Epoch  6430  G loss  0.04680837\n",
      "Epoch  6431  G loss  0.0129012065\n",
      "Epoch  6432  G loss  0.01041244\n",
      "Epoch  6433  G loss  0.058580384\n",
      "Epoch  6434  G loss  0.05031863\n",
      "Epoch  6435  G loss  0.01875516\n",
      "Epoch  6436  G loss  0.06913598\n",
      "Epoch  6437  G loss  0.0050101657\n",
      "Epoch  6438  G loss  0.007815689\n",
      "Epoch  6439  G loss  0.01307573\n",
      "Epoch  6440  G loss  0.16901365\n",
      "Epoch  6441  G loss  0.056167595\n",
      "Epoch  6442  G loss  0.0765931\n",
      "Epoch  6443  G loss  0.02305996\n",
      "Epoch  6444  G loss  0.01203473\n",
      "Epoch  6445  G loss  0.27180198\n",
      "Epoch  6446  G loss  0.046686657\n",
      "Epoch  6447  G loss  0.11658546\n",
      "Epoch  6448  G loss  0.14582077\n",
      "Epoch  6449  G loss  0.11352946\n",
      "Epoch  6450  G loss  0.029550686\n",
      "Epoch  6451  G loss  0.10696254\n",
      "Epoch  6452  G loss  0.028230473\n",
      "Epoch  6453  G loss  0.021420669\n",
      "Epoch  6454  G loss  0.033381633\n",
      "Epoch  6455  G loss  0.094630025\n",
      "Epoch  6456  G loss  0.07253878\n",
      "Epoch  6457  G loss  0.017344862\n",
      "Epoch  6458  G loss  0.026667748\n",
      "Epoch  6459  G loss  0.02060737\n",
      "Epoch  6460  G loss  0.0118646845\n",
      "Epoch  6461  G loss  0.010519683\n",
      "Epoch  6462  G loss  0.025934923\n",
      "Epoch  6463  G loss  0.022690585\n",
      "Epoch  6464  G loss  0.031973563\n",
      "Epoch  6465  G loss  0.035176903\n",
      "Epoch  6466  G loss  0.06725998\n",
      "Epoch  6467  G loss  0.03735194\n",
      "Epoch  6468  G loss  0.012719264\n",
      "Epoch  6469  G loss  0.0044596912\n",
      "Epoch  6470  G loss  0.07001557\n",
      "Epoch  6471  G loss  0.010582799\n",
      "Epoch  6472  G loss  0.05024558\n",
      "Epoch  6473  G loss  0.0055799778\n",
      "Epoch  6474  G loss  0.0073131835\n",
      "Epoch  6475  G loss  0.09216102\n",
      "Epoch  6476  G loss  0.013165224\n",
      "Epoch  6477  G loss  0.010799862\n",
      "Epoch  6478  G loss  0.02347075\n",
      "Epoch  6479  G loss  0.01044949\n",
      "Epoch  6480  G loss  0.09723021\n",
      "Epoch  6481  G loss  0.09412044\n",
      "Epoch  6482  G loss  0.010976157\n",
      "Epoch  6483  G loss  0.01548822\n",
      "Epoch  6484  G loss  0.010805514\n",
      "Epoch  6485  G loss  0.031618103\n",
      "Epoch  6486  G loss  0.039512575\n",
      "Epoch  6487  G loss  0.02258208\n",
      "Epoch  6488  G loss  0.025546048\n",
      "Epoch  6489  G loss  0.024202552\n",
      "Epoch  6490  G loss  0.07923411\n",
      "Epoch  6491  G loss  0.044219553\n",
      "Epoch  6492  G loss  0.02135352\n",
      "Epoch  6493  G loss  0.0048780027\n",
      "Epoch  6494  G loss  0.008576599\n",
      "Epoch  6495  G loss  0.03453\n",
      "Epoch  6496  G loss  0.010313292\n",
      "Epoch  6497  G loss  0.03117714\n",
      "Epoch  6498  G loss  0.024976525\n",
      "Epoch  6499  G loss  0.22938552\n",
      "Epoch  6500  G loss  0.11173135\n",
      "Epoch  6501  G loss  0.1394221\n",
      "Epoch  6502  G loss  0.014537351\n",
      "Epoch  6503  G loss  0.08495452\n",
      "Epoch  6504  G loss  0.04252273\n",
      "Epoch  6505  G loss  0.10961945\n",
      "Epoch  6506  G loss  0.0060123736\n",
      "Epoch  6507  G loss  0.018889442\n",
      "Epoch  6508  G loss  0.00869103\n",
      "Epoch  6509  G loss  0.051388294\n",
      "Epoch  6510  G loss  0.021304399\n",
      "Epoch  6511  G loss  0.067113854\n",
      "Epoch  6512  G loss  0.07439339\n",
      "Epoch  6513  G loss  0.07419674\n",
      "Epoch  6514  G loss  0.021297056\n",
      "Epoch  6515  G loss  0.011407319\n",
      "Epoch  6516  G loss  0.016483203\n",
      "Epoch  6517  G loss  0.015956506\n",
      "Epoch  6518  G loss  0.11382568\n",
      "Epoch  6519  G loss  0.052236177\n",
      "Epoch  6520  G loss  0.0406413\n",
      "Epoch  6521  G loss  0.0061308956\n",
      "Epoch  6522  G loss  0.03944851\n",
      "Epoch  6523  G loss  0.013733871\n",
      "Epoch  6524  G loss  0.04924294\n",
      "Epoch  6525  G loss  0.22579445\n",
      "Epoch  6526  G loss  0.007662202\n",
      "Epoch  6527  G loss  0.0110870665\n",
      "Epoch  6528  G loss  0.0083130235\n",
      "Epoch  6529  G loss  0.015500057\n",
      "Epoch  6530  G loss  0.019234132\n",
      "Epoch  6531  G loss  0.120902844\n",
      "Epoch  6532  G loss  0.104044355\n",
      "Epoch  6533  G loss  0.023109302\n",
      "Epoch  6534  G loss  0.037994724\n",
      "Epoch  6535  G loss  0.020195266\n",
      "Epoch  6536  G loss  0.014255344\n",
      "Epoch  6537  G loss  0.0075040422\n",
      "Epoch  6538  G loss  0.009078879\n",
      "Epoch  6539  G loss  0.15002503\n",
      "Epoch  6540  G loss  0.05822684\n",
      "Epoch  6541  G loss  0.07469419\n",
      "Epoch  6542  G loss  0.03716258\n",
      "Epoch  6543  G loss  0.12903109\n",
      "Epoch  6544  G loss  0.006213623\n",
      "Epoch  6545  G loss  0.015893597\n",
      "Epoch  6546  G loss  0.09922081\n",
      "Epoch  6547  G loss  0.007857855\n",
      "Epoch  6548  G loss  0.009133475\n",
      "Epoch  6549  G loss  0.0102269165\n",
      "Epoch  6550  G loss  0.12756641\n",
      "Epoch  6551  G loss  0.039947044\n",
      "Epoch  6552  G loss  0.019170942\n",
      "Epoch  6553  G loss  0.13430515\n",
      "Epoch  6554  G loss  0.012595002\n",
      "Epoch  6555  G loss  0.017874114\n",
      "Epoch  6556  G loss  0.020011477\n",
      "Epoch  6557  G loss  0.02752195\n",
      "Epoch  6558  G loss  0.08189739\n",
      "Epoch  6559  G loss  0.24889912\n",
      "Epoch  6560  G loss  0.06955601\n",
      "Epoch  6561  G loss  0.033862345\n",
      "Epoch  6562  G loss  0.119577274\n",
      "Epoch  6563  G loss  0.10717283\n",
      "Epoch  6564  G loss  0.09883412\n",
      "Epoch  6565  G loss  0.021563385\n",
      "Epoch  6566  G loss  0.039536208\n",
      "Epoch  6567  G loss  0.01483666\n",
      "Epoch  6568  G loss  0.02720679\n",
      "Epoch  6569  G loss  0.007591255\n",
      "Epoch  6570  G loss  0.024494272\n",
      "Epoch  6571  G loss  0.020744028\n",
      "Epoch  6572  G loss  0.0589298\n",
      "Epoch  6573  G loss  0.012573591\n",
      "Epoch  6574  G loss  0.0369394\n",
      "Epoch  6575  G loss  0.010503046\n",
      "Epoch  6576  G loss  0.019608561\n",
      "Epoch  6577  G loss  0.017159916\n",
      "Epoch  6578  G loss  0.094824195\n",
      "Epoch  6579  G loss  0.047749028\n",
      "Epoch  6580  G loss  0.013325318\n",
      "Epoch  6581  G loss  0.009846485\n",
      "Epoch  6582  G loss  0.022245402\n",
      "Epoch  6583  G loss  0.3087843\n",
      "Epoch  6584  G loss  0.039507367\n",
      "Epoch  6585  G loss  0.13155806\n",
      "Epoch  6586  G loss  0.017138362\n",
      "Epoch  6587  G loss  0.017873023\n",
      "Epoch  6588  G loss  0.09364163\n",
      "Epoch  6589  G loss  0.011759004\n",
      "Epoch  6590  G loss  0.11508624\n",
      "Epoch  6591  G loss  0.031049784\n",
      "Epoch  6592  G loss  0.048965935\n",
      "Epoch  6593  G loss  0.09031688\n",
      "Epoch  6594  G loss  0.0061359312\n",
      "Epoch  6595  G loss  0.00446566\n",
      "Epoch  6596  G loss  0.017597973\n",
      "Epoch  6597  G loss  0.013539515\n",
      "Epoch  6598  G loss  0.008106526\n",
      "Epoch  6599  G loss  0.016833369\n",
      "Epoch  6600  G loss  0.24554259\n",
      "Epoch  6601  G loss  0.07065962\n",
      "Epoch  6602  G loss  0.08321305\n",
      "Epoch  6603  G loss  0.029516017\n",
      "Epoch  6604  G loss  0.023752876\n",
      "Epoch  6605  G loss  0.02709404\n",
      "Epoch  6606  G loss  0.014256509\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  6607  G loss  0.035802722\n",
      "Epoch  6608  G loss  0.04970243\n",
      "Epoch  6609  G loss  0.09644157\n",
      "Epoch  6610  G loss  0.047287636\n",
      "Epoch  6611  G loss  0.024793264\n",
      "Epoch  6612  G loss  0.045881424\n",
      "Epoch  6613  G loss  0.055810325\n",
      "Epoch  6614  G loss  0.07199508\n",
      "Epoch  6615  G loss  0.09644349\n",
      "Epoch  6616  G loss  0.015146238\n",
      "Epoch  6617  G loss  0.037721913\n",
      "Epoch  6618  G loss  0.030662104\n",
      "Epoch  6619  G loss  0.03000173\n",
      "Epoch  6620  G loss  0.019865476\n",
      "Epoch  6621  G loss  0.008558335\n",
      "Epoch  6622  G loss  0.03431727\n",
      "Epoch  6623  G loss  0.041104816\n",
      "Epoch  6624  G loss  0.238111\n",
      "Epoch  6625  G loss  0.063479125\n",
      "Epoch  6626  G loss  0.049525026\n",
      "Epoch  6627  G loss  0.019358866\n",
      "Epoch  6628  G loss  0.005390004\n",
      "Epoch  6629  G loss  0.042145826\n",
      "Epoch  6630  G loss  0.11336289\n",
      "Epoch  6631  G loss  0.008989755\n",
      "Epoch  6632  G loss  0.21394145\n",
      "Epoch  6633  G loss  0.09350562\n",
      "Epoch  6634  G loss  0.018516112\n",
      "Epoch  6635  G loss  0.037183776\n",
      "Epoch  6636  G loss  0.015351675\n",
      "Epoch  6637  G loss  0.044230297\n",
      "Epoch  6638  G loss  0.019085474\n",
      "Epoch  6639  G loss  0.015681908\n",
      "Epoch  6640  G loss  0.056465313\n",
      "Epoch  6641  G loss  0.051135406\n",
      "Epoch  6642  G loss  0.07736151\n",
      "Epoch  6643  G loss  0.011274749\n",
      "Epoch  6644  G loss  0.0131303305\n",
      "Epoch  6645  G loss  0.01675755\n",
      "Epoch  6646  G loss  0.04467393\n",
      "Epoch  6647  G loss  0.054778546\n",
      "Epoch  6648  G loss  0.015450825\n",
      "Epoch  6649  G loss  0.055822477\n",
      "Epoch  6650  G loss  0.11722098\n",
      "Epoch  6651  G loss  0.017135378\n",
      "Epoch  6652  G loss  0.012096697\n",
      "Epoch  6653  G loss  0.092839666\n",
      "Epoch  6654  G loss  0.02391148\n",
      "Epoch  6655  G loss  0.01732758\n",
      "Epoch  6656  G loss  0.0071487003\n",
      "Epoch  6657  G loss  0.1764062\n",
      "Epoch  6658  G loss  0.08411701\n",
      "Epoch  6659  G loss  0.14570116\n",
      "Epoch  6660  G loss  0.049609937\n",
      "Epoch  6661  G loss  0.044549584\n",
      "Epoch  6662  G loss  0.0040324307\n",
      "Epoch  6663  G loss  0.0028941361\n",
      "Epoch  6664  G loss  0.0031675058\n",
      "Epoch  6665  G loss  0.021126393\n",
      "Epoch  6666  G loss  0.0049736006\n",
      "Epoch  6667  G loss  0.22268297\n",
      "Epoch  6668  G loss  0.045083083\n",
      "Epoch  6669  G loss  0.032561265\n",
      "Epoch  6670  G loss  0.10439349\n",
      "Epoch  6671  G loss  0.11508685\n",
      "Epoch  6672  G loss  0.04096078\n",
      "Epoch  6673  G loss  0.1023591\n",
      "Epoch  6674  G loss  0.049350042\n",
      "Epoch  6675  G loss  0.46895203\n",
      "Epoch  6676  G loss  0.13484514\n",
      "Epoch  6677  G loss  0.018808203\n",
      "Epoch  6678  G loss  0.013902849\n",
      "Epoch  6679  G loss  0.02852295\n",
      "Epoch  6680  G loss  0.10847619\n",
      "Epoch  6681  G loss  0.082132\n",
      "Epoch  6682  G loss  0.01447253\n",
      "Epoch  6683  G loss  0.112399496\n",
      "Epoch  6684  G loss  0.008844422\n",
      "Epoch  6685  G loss  0.013595363\n",
      "Epoch  6686  G loss  0.013405163\n",
      "Epoch  6687  G loss  0.04596561\n",
      "Epoch  6688  G loss  0.007465134\n",
      "Epoch  6689  G loss  0.12759092\n",
      "Epoch  6690  G loss  0.042589556\n",
      "Epoch  6691  G loss  0.02669391\n",
      "Epoch  6692  G loss  0.016733821\n",
      "Epoch  6693  G loss  0.023181539\n",
      "Epoch  6694  G loss  0.017967159\n",
      "Epoch  6695  G loss  0.020661024\n",
      "Epoch  6696  G loss  0.013356739\n",
      "Epoch  6697  G loss  0.10368933\n",
      "Epoch  6698  G loss  0.05694439\n",
      "Epoch  6699  G loss  0.020754538\n",
      "Epoch  6700  G loss  0.019495277\n",
      "Epoch  6701  G loss  0.019984532\n",
      "Epoch  6702  G loss  0.013914244\n",
      "Epoch  6703  G loss  0.01369799\n",
      "Epoch  6704  G loss  0.09187892\n",
      "Epoch  6705  G loss  0.05885397\n",
      "Epoch  6706  G loss  0.005633026\n",
      "Epoch  6707  G loss  0.03197066\n",
      "Epoch  6708  G loss  0.15116316\n",
      "Epoch  6709  G loss  0.028476918\n",
      "Epoch  6710  G loss  0.04745444\n",
      "Epoch  6711  G loss  0.010954445\n",
      "Epoch  6712  G loss  0.23505977\n",
      "Epoch  6713  G loss  0.10768929\n",
      "Epoch  6714  G loss  0.10017604\n",
      "Epoch  6715  G loss  0.018018074\n",
      "Epoch  6716  G loss  0.024637032\n",
      "Epoch  6717  G loss  0.073833056\n",
      "Epoch  6718  G loss  0.013723083\n",
      "Epoch  6719  G loss  0.0128612295\n",
      "Epoch  6720  G loss  0.022427537\n",
      "Epoch  6721  G loss  0.013934347\n",
      "Epoch  6722  G loss  0.027446194\n",
      "Epoch  6723  G loss  0.045427054\n",
      "Epoch  6724  G loss  0.039039053\n",
      "Epoch  6725  G loss  0.15558949\n",
      "Epoch  6726  G loss  0.04747455\n",
      "Epoch  6727  G loss  0.02239236\n",
      "Epoch  6728  G loss  0.023472438\n",
      "Epoch  6729  G loss  0.07097925\n",
      "Epoch  6730  G loss  0.02605503\n",
      "Epoch  6731  G loss  0.06856552\n",
      "Epoch  6732  G loss  0.01132556\n",
      "Epoch  6733  G loss  0.04684168\n",
      "Epoch  6734  G loss  0.16387111\n",
      "Epoch  6735  G loss  0.008525308\n",
      "Epoch  6736  G loss  0.008095326\n",
      "Epoch  6737  G loss  0.07047523\n",
      "Epoch  6738  G loss  0.09103218\n",
      "Epoch  6739  G loss  0.051531456\n",
      "Epoch  6740  G loss  0.0139080435\n",
      "Epoch  6741  G loss  0.2353766\n",
      "Epoch  6742  G loss  0.006989393\n",
      "Epoch  6743  G loss  0.013536627\n",
      "Epoch  6744  G loss  0.017928878\n",
      "Epoch  6745  G loss  0.15604043\n",
      "Epoch  6746  G loss  0.19365112\n",
      "Epoch  6747  G loss  0.037972063\n",
      "Epoch  6748  G loss  0.009591974\n",
      "Epoch  6749  G loss  0.015292422\n",
      "Epoch  6750  G loss  0.032092307\n",
      "Epoch  6751  G loss  0.017071482\n",
      "Epoch  6752  G loss  0.29660392\n",
      "Epoch  6753  G loss  0.13334748\n",
      "Epoch  6754  G loss  0.06325139\n",
      "Epoch  6755  G loss  0.0049927803\n",
      "Epoch  6756  G loss  0.17957744\n",
      "Epoch  6757  G loss  0.13815472\n",
      "Epoch  6758  G loss  0.015421596\n",
      "Epoch  6759  G loss  0.024567917\n",
      "Epoch  6760  G loss  0.034959055\n",
      "Epoch  6761  G loss  0.048563473\n",
      "Epoch  6762  G loss  0.11798893\n",
      "Epoch  6763  G loss  0.167164\n",
      "Epoch  6764  G loss  0.026915256\n",
      "Epoch  6765  G loss  0.025580611\n",
      "Epoch  6766  G loss  0.14195348\n",
      "Epoch  6767  G loss  0.07824746\n",
      "Epoch  6768  G loss  0.016375441\n",
      "Epoch  6769  G loss  0.06802425\n",
      "Epoch  6770  G loss  0.23558637\n",
      "Epoch  6771  G loss  0.012704804\n",
      "Epoch  6772  G loss  0.07574096\n",
      "Epoch  6773  G loss  0.085072756\n",
      "Epoch  6774  G loss  0.06117343\n",
      "Epoch  6775  G loss  0.39825737\n",
      "Epoch  6776  G loss  0.06292884\n",
      "Epoch  6777  G loss  0.035742044\n",
      "Epoch  6778  G loss  0.04606257\n",
      "Epoch  6779  G loss  0.005689147\n",
      "Epoch  6780  G loss  0.038089804\n",
      "Epoch  6781  G loss  0.021643918\n",
      "Epoch  6782  G loss  0.0172948\n",
      "Epoch  6783  G loss  0.078189746\n",
      "Epoch  6784  G loss  0.010337906\n",
      "Epoch  6785  G loss  0.023514997\n",
      "Epoch  6786  G loss  0.0329658\n",
      "Epoch  6787  G loss  0.046313375\n",
      "Epoch  6788  G loss  0.043140166\n",
      "Epoch  6789  G loss  0.051467553\n",
      "Epoch  6790  G loss  0.01767827\n",
      "Epoch  6791  G loss  0.15630588\n",
      "Epoch  6792  G loss  0.031877197\n",
      "Epoch  6793  G loss  0.05878772\n",
      "Epoch  6794  G loss  0.0054929005\n",
      "Epoch  6795  G loss  0.026637368\n",
      "Epoch  6796  G loss  0.05749614\n",
      "Epoch  6797  G loss  0.0043044235\n",
      "Epoch  6798  G loss  0.016512731\n",
      "Epoch  6799  G loss  0.0088174455\n",
      "Epoch  6800  G loss  0.020063087\n",
      "Epoch  6801  G loss  0.09767215\n",
      "Epoch  6802  G loss  0.08423045\n",
      "Epoch  6803  G loss  0.008217442\n",
      "Epoch  6804  G loss  0.00322634\n",
      "Epoch  6805  G loss  0.006977753\n",
      "Epoch  6806  G loss  0.011875063\n",
      "Epoch  6807  G loss  0.1285488\n",
      "Epoch  6808  G loss  0.16201982\n",
      "Epoch  6809  G loss  0.014014106\n",
      "Epoch  6810  G loss  0.024022965\n",
      "Epoch  6811  G loss  0.02436798\n",
      "Epoch  6812  G loss  0.11825065\n",
      "Epoch  6813  G loss  0.018538691\n",
      "Epoch  6814  G loss  0.013744812\n",
      "Epoch  6815  G loss  0.075160876\n",
      "Epoch  6816  G loss  0.005809795\n",
      "Epoch  6817  G loss  0.101064466\n",
      "Epoch  6818  G loss  0.15151507\n",
      "Epoch  6819  G loss  0.016423002\n",
      "Epoch  6820  G loss  0.035031974\n",
      "Epoch  6821  G loss  0.027421061\n",
      "Epoch  6822  G loss  0.0058140494\n",
      "Epoch  6823  G loss  0.081557035\n",
      "Epoch  6824  G loss  0.08150162\n",
      "Epoch  6825  G loss  0.04712549\n",
      "Epoch  6826  G loss  0.017195508\n",
      "Epoch  6827  G loss  0.14290978\n",
      "Epoch  6828  G loss  0.014789019\n",
      "Epoch  6829  G loss  0.051638413\n",
      "Epoch  6830  G loss  0.021041937\n",
      "Epoch  6831  G loss  0.05916793\n",
      "Epoch  6832  G loss  0.008389035\n",
      "Epoch  6833  G loss  0.036831476\n",
      "Epoch  6834  G loss  0.025090745\n",
      "Epoch  6835  G loss  0.029149164\n",
      "Epoch  6836  G loss  0.011519069\n",
      "Epoch  6837  G loss  0.026502226\n",
      "Epoch  6838  G loss  0.009703898\n",
      "Epoch  6839  G loss  0.017237067\n",
      "Epoch  6840  G loss  0.009974444\n",
      "Epoch  6841  G loss  0.0068925857\n",
      "Epoch  6842  G loss  0.02756435\n",
      "Epoch  6843  G loss  0.00894152\n",
      "Epoch  6844  G loss  0.010660946\n",
      "Epoch  6845  G loss  0.015511842\n",
      "Epoch  6846  G loss  0.0519367\n",
      "Epoch  6847  G loss  0.041618403\n",
      "Epoch  6848  G loss  0.098038964\n",
      "Epoch  6849  G loss  0.019533364\n",
      "Epoch  6850  G loss  0.03459132\n",
      "Epoch  6851  G loss  0.021575185\n",
      "Epoch  6852  G loss  0.016402094\n",
      "Epoch  6853  G loss  0.060852874\n",
      "Epoch  6854  G loss  0.03689355\n",
      "Epoch  6855  G loss  0.016546404\n",
      "Epoch  6856  G loss  0.08670293\n",
      "Epoch  6857  G loss  0.017503515\n",
      "Epoch  6858  G loss  0.08705762\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  6859  G loss  0.01792521\n",
      "Epoch  6860  G loss  0.10414584\n",
      "Epoch  6861  G loss  0.04056172\n",
      "Epoch  6862  G loss  0.10459168\n",
      "Epoch  6863  G loss  0.018865142\n",
      "Epoch  6864  G loss  0.0119749885\n",
      "Epoch  6865  G loss  0.09254844\n",
      "Epoch  6866  G loss  0.009856138\n",
      "Epoch  6867  G loss  0.19428064\n",
      "Epoch  6868  G loss  0.0068066265\n",
      "Epoch  6869  G loss  0.18582691\n",
      "Epoch  6870  G loss  0.085480586\n",
      "Epoch  6871  G loss  0.014372123\n",
      "Epoch  6872  G loss  0.019529946\n",
      "Epoch  6873  G loss  0.2668866\n",
      "Epoch  6874  G loss  0.03664403\n",
      "Epoch  6875  G loss  0.02536869\n",
      "Epoch  6876  G loss  0.034524158\n",
      "Epoch  6877  G loss  0.013456145\n",
      "Epoch  6878  G loss  0.016321214\n",
      "Epoch  6879  G loss  0.010088163\n",
      "Epoch  6880  G loss  0.030065777\n",
      "Epoch  6881  G loss  0.006477955\n",
      "Epoch  6882  G loss  0.015587809\n",
      "Epoch  6883  G loss  0.114940315\n",
      "Epoch  6884  G loss  0.015120421\n",
      "Epoch  6885  G loss  0.0073666894\n",
      "Epoch  6886  G loss  0.085519694\n",
      "Epoch  6887  G loss  0.02119318\n",
      "Epoch  6888  G loss  0.09187262\n",
      "Epoch  6889  G loss  0.05527009\n",
      "Epoch  6890  G loss  0.011521105\n",
      "Epoch  6891  G loss  0.022841226\n",
      "Epoch  6892  G loss  0.032960143\n",
      "Epoch  6893  G loss  0.019836875\n",
      "Epoch  6894  G loss  0.04705091\n",
      "Epoch  6895  G loss  0.018555135\n",
      "Epoch  6896  G loss  0.0342852\n",
      "Epoch  6897  G loss  0.13411018\n",
      "Epoch  6898  G loss  0.022414608\n",
      "Epoch  6899  G loss  0.046050534\n",
      "Epoch  6900  G loss  0.06294614\n",
      "Epoch  6901  G loss  0.039234914\n",
      "Epoch  6902  G loss  0.013788082\n",
      "Epoch  6903  G loss  0.12996373\n",
      "Epoch  6904  G loss  0.019106314\n",
      "Epoch  6905  G loss  0.02044086\n",
      "Epoch  6906  G loss  0.036996804\n",
      "Epoch  6907  G loss  0.007668947\n",
      "Epoch  6908  G loss  0.10978477\n",
      "Epoch  6909  G loss  0.090914115\n",
      "Epoch  6910  G loss  0.05064305\n",
      "Epoch  6911  G loss  0.062281124\n",
      "Epoch  6912  G loss  0.12370338\n",
      "Epoch  6913  G loss  0.0407711\n",
      "Epoch  6914  G loss  0.012579023\n",
      "Epoch  6915  G loss  0.0759606\n",
      "Epoch  6916  G loss  0.062430933\n",
      "Epoch  6917  G loss  0.019521393\n",
      "Epoch  6918  G loss  0.047784735\n",
      "Epoch  6919  G loss  0.019106636\n",
      "Epoch  6920  G loss  0.010022857\n",
      "Epoch  6921  G loss  0.009958553\n",
      "Epoch  6922  G loss  0.0223009\n",
      "Epoch  6923  G loss  0.010288342\n",
      "Epoch  6924  G loss  0.08191891\n",
      "Epoch  6925  G loss  0.047960676\n",
      "Epoch  6926  G loss  0.020009294\n",
      "Epoch  6927  G loss  0.015908904\n",
      "Epoch  6928  G loss  0.01568308\n",
      "Epoch  6929  G loss  0.16512266\n",
      "Epoch  6930  G loss  0.12705809\n",
      "Epoch  6931  G loss  0.017879859\n",
      "Epoch  6932  G loss  0.041747227\n",
      "Epoch  6933  G loss  0.041678086\n",
      "Epoch  6934  G loss  0.06542842\n",
      "Epoch  6935  G loss  0.03956416\n",
      "Epoch  6936  G loss  0.11209185\n",
      "Epoch  6937  G loss  0.027352259\n",
      "Epoch  6938  G loss  0.035171993\n",
      "Epoch  6939  G loss  0.023368452\n",
      "Epoch  6940  G loss  0.05125682\n",
      "Epoch  6941  G loss  0.11570211\n",
      "Epoch  6942  G loss  0.13060269\n",
      "Epoch  6943  G loss  0.028410602\n",
      "Epoch  6944  G loss  0.055490114\n",
      "Epoch  6945  G loss  0.08917232\n",
      "Epoch  6946  G loss  0.013194686\n",
      "Epoch  6947  G loss  0.0067661405\n",
      "Epoch  6948  G loss  0.08616714\n",
      "Epoch  6949  G loss  0.003543676\n",
      "Epoch  6950  G loss  0.22885555\n",
      "Epoch  6951  G loss  0.03830394\n",
      "Epoch  6952  G loss  0.03944324\n",
      "Epoch  6953  G loss  0.013192467\n",
      "Epoch  6954  G loss  0.010670299\n",
      "Epoch  6955  G loss  0.26812845\n",
      "Epoch  6956  G loss  0.018364834\n",
      "Epoch  6957  G loss  0.0057171434\n",
      "Epoch  6958  G loss  0.012656786\n",
      "Epoch  6959  G loss  0.0073569072\n",
      "Epoch  6960  G loss  0.022243794\n",
      "Epoch  6961  G loss  0.03918334\n",
      "Epoch  6962  G loss  0.02237232\n",
      "Epoch  6963  G loss  0.01553485\n",
      "Epoch  6964  G loss  0.07832171\n",
      "Epoch  6965  G loss  0.031177782\n",
      "Epoch  6966  G loss  0.09150566\n",
      "Epoch  6967  G loss  0.079895556\n",
      "Epoch  6968  G loss  0.08794762\n",
      "Epoch  6969  G loss  0.07517931\n",
      "Epoch  6970  G loss  0.058395434\n",
      "Epoch  6971  G loss  0.18838388\n",
      "Epoch  6972  G loss  0.12982264\n",
      "Epoch  6973  G loss  0.00806427\n",
      "Epoch  6974  G loss  0.016452\n",
      "Epoch  6975  G loss  0.0064573707\n",
      "Epoch  6976  G loss  0.005301645\n",
      "Epoch  6977  G loss  0.04778735\n",
      "Epoch  6978  G loss  0.048785508\n",
      "Epoch  6979  G loss  0.063605644\n",
      "Epoch  6980  G loss  0.0052000126\n",
      "Epoch  6981  G loss  0.028771643\n",
      "Epoch  6982  G loss  0.24900882\n",
      "Epoch  6983  G loss  0.070388146\n",
      "Epoch  6984  G loss  0.0102076875\n",
      "Epoch  6985  G loss  0.047953136\n",
      "Epoch  6986  G loss  0.009633567\n",
      "Epoch  6987  G loss  0.12194479\n",
      "Epoch  6988  G loss  0.021314837\n",
      "Epoch  6989  G loss  0.0059756814\n",
      "Epoch  6990  G loss  0.034898393\n",
      "Epoch  6991  G loss  0.03488189\n",
      "Epoch  6992  G loss  0.011496136\n",
      "Epoch  6993  G loss  0.016170945\n",
      "Epoch  6994  G loss  0.006405535\n",
      "Epoch  6995  G loss  0.018945143\n",
      "Epoch  6996  G loss  0.09411221\n",
      "Epoch  6997  G loss  0.008361621\n",
      "Epoch  6998  G loss  0.032353796\n",
      "Epoch  6999  G loss  0.016091362\n",
      "Epoch  7000  G loss  0.019038241\n",
      "Epoch  7001  G loss  0.029793816\n",
      "Epoch  7002  G loss  0.1250974\n",
      "Epoch  7003  G loss  0.0062424103\n",
      "Epoch  7004  G loss  0.13300517\n",
      "Epoch  7005  G loss  0.087774105\n",
      "Epoch  7006  G loss  0.02036151\n",
      "Epoch  7007  G loss  0.0123863155\n",
      "Epoch  7008  G loss  0.038553596\n",
      "Epoch  7009  G loss  0.008432311\n",
      "Epoch  7010  G loss  0.08737482\n",
      "Epoch  7011  G loss  0.012105728\n",
      "Epoch  7012  G loss  0.008756584\n",
      "Epoch  7013  G loss  0.012046584\n",
      "Epoch  7014  G loss  0.055120174\n",
      "Epoch  7015  G loss  0.023823358\n",
      "Epoch  7016  G loss  0.14938976\n",
      "Epoch  7017  G loss  0.019074436\n",
      "Epoch  7018  G loss  0.010372759\n",
      "Epoch  7019  G loss  0.055578623\n",
      "Epoch  7020  G loss  0.035320245\n",
      "Epoch  7021  G loss  0.014252706\n",
      "Epoch  7022  G loss  0.0051711276\n",
      "Epoch  7023  G loss  0.020988263\n",
      "Epoch  7024  G loss  0.010590351\n",
      "Epoch  7025  G loss  0.048851132\n",
      "Epoch  7026  G loss  0.074442\n",
      "Epoch  7027  G loss  0.014512397\n",
      "Epoch  7028  G loss  0.02718011\n",
      "Epoch  7029  G loss  0.011755614\n",
      "Epoch  7030  G loss  0.11417328\n",
      "Epoch  7031  G loss  0.07019209\n",
      "Epoch  7032  G loss  0.011702738\n",
      "Epoch  7033  G loss  0.029587083\n",
      "Epoch  7034  G loss  0.034192115\n",
      "Epoch  7035  G loss  0.05084228\n",
      "Epoch  7036  G loss  0.033096075\n",
      "Epoch  7037  G loss  0.103799805\n",
      "Epoch  7038  G loss  0.097657405\n",
      "Epoch  7039  G loss  0.03443461\n",
      "Epoch  7040  G loss  0.021988243\n",
      "Epoch  7041  G loss  0.06589581\n",
      "Epoch  7042  G loss  0.08073959\n",
      "Epoch  7043  G loss  0.013543565\n",
      "Epoch  7044  G loss  0.071450844\n",
      "Epoch  7045  G loss  0.021833824\n",
      "Epoch  7046  G loss  0.022838546\n",
      "Epoch  7047  G loss  0.060733598\n",
      "Epoch  7048  G loss  0.030386802\n",
      "Epoch  7049  G loss  0.13525373\n",
      "Epoch  7050  G loss  0.013740677\n",
      "Epoch  7051  G loss  0.07058084\n",
      "Epoch  7052  G loss  0.16506034\n",
      "Epoch  7053  G loss  0.024518104\n",
      "Epoch  7054  G loss  0.49040624\n",
      "Epoch  7055  G loss  0.0050787106\n",
      "Epoch  7056  G loss  0.2655839\n",
      "Epoch  7057  G loss  0.008953229\n",
      "Epoch  7058  G loss  0.008712189\n",
      "Epoch  7059  G loss  0.006560559\n",
      "Epoch  7060  G loss  0.018095825\n",
      "Epoch  7061  G loss  0.060787708\n",
      "Epoch  7062  G loss  0.020956641\n",
      "Epoch  7063  G loss  0.083668046\n",
      "Epoch  7064  G loss  0.013596668\n",
      "Epoch  7065  G loss  0.031426076\n",
      "Epoch  7066  G loss  0.016413927\n",
      "Epoch  7067  G loss  0.016381567\n",
      "Epoch  7068  G loss  0.06147229\n",
      "Epoch  7069  G loss  0.097210124\n",
      "Epoch  7070  G loss  0.044060983\n",
      "Epoch  7071  G loss  0.019230556\n",
      "Epoch  7072  G loss  0.034454316\n",
      "Epoch  7073  G loss  0.06687886\n",
      "Epoch  7074  G loss  0.012570857\n",
      "Epoch  7075  G loss  0.040689386\n",
      "Epoch  7076  G loss  0.029644096\n",
      "Epoch  7077  G loss  0.014469767\n",
      "Epoch  7078  G loss  0.15252022\n",
      "Epoch  7079  G loss  0.017933471\n",
      "Epoch  7080  G loss  0.14422831\n",
      "Epoch  7081  G loss  0.036181726\n",
      "Epoch  7082  G loss  0.01818619\n",
      "Epoch  7083  G loss  0.05220279\n",
      "Epoch  7084  G loss  0.10047294\n",
      "Epoch  7085  G loss  0.05056957\n",
      "Epoch  7086  G loss  0.030037714\n",
      "Epoch  7087  G loss  0.008067368\n",
      "Epoch  7088  G loss  0.023080079\n",
      "Epoch  7089  G loss  0.04668329\n",
      "Epoch  7090  G loss  0.02324472\n",
      "Epoch  7091  G loss  0.01583597\n",
      "Epoch  7092  G loss  0.016618093\n",
      "Epoch  7093  G loss  0.014507702\n",
      "Epoch  7094  G loss  0.011802558\n",
      "Epoch  7095  G loss  0.0620773\n",
      "Epoch  7096  G loss  0.03870341\n",
      "Epoch  7097  G loss  0.048203956\n",
      "Epoch  7098  G loss  0.010304294\n",
      "Epoch  7099  G loss  0.10067129\n",
      "Epoch  7100  G loss  0.04097028\n",
      "Epoch  7101  G loss  0.018908972\n",
      "Epoch  7102  G loss  0.0063469494\n",
      "Epoch  7103  G loss  0.027632903\n",
      "Epoch  7104  G loss  0.04353787\n",
      "Epoch  7105  G loss  0.15016219\n",
      "Epoch  7106  G loss  0.0063742176\n",
      "Epoch  7107  G loss  0.050698508\n",
      "Epoch  7108  G loss  0.008980954\n",
      "Epoch  7109  G loss  0.18693236\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  7110  G loss  0.06313454\n",
      "Epoch  7111  G loss  0.22822586\n",
      "Epoch  7112  G loss  0.01406862\n",
      "Epoch  7113  G loss  0.01763168\n",
      "Epoch  7114  G loss  0.05867987\n",
      "Epoch  7115  G loss  0.036141418\n",
      "Epoch  7116  G loss  0.0068654823\n",
      "Epoch  7117  G loss  0.011304079\n",
      "Epoch  7118  G loss  0.009330043\n",
      "Epoch  7119  G loss  0.038911857\n",
      "Epoch  7120  G loss  0.053807847\n",
      "Epoch  7121  G loss  0.01923839\n",
      "Epoch  7122  G loss  0.34362465\n",
      "Epoch  7123  G loss  0.05239904\n",
      "Epoch  7124  G loss  0.052186616\n",
      "Epoch  7125  G loss  0.0045139734\n",
      "Epoch  7126  G loss  0.056343846\n",
      "Epoch  7127  G loss  0.0114522185\n",
      "Epoch  7128  G loss  0.037236556\n",
      "Epoch  7129  G loss  0.015695654\n",
      "Epoch  7130  G loss  0.0213011\n",
      "Epoch  7131  G loss  0.006603114\n",
      "Epoch  7132  G loss  0.02863803\n",
      "Epoch  7133  G loss  0.024805771\n",
      "Epoch  7134  G loss  0.017483197\n",
      "Epoch  7135  G loss  0.029761596\n",
      "Epoch  7136  G loss  0.022497261\n",
      "Epoch  7137  G loss  0.01015739\n",
      "Epoch  7138  G loss  0.00842604\n",
      "Epoch  7139  G loss  0.011895501\n",
      "Epoch  7140  G loss  0.048246883\n",
      "Epoch  7141  G loss  0.025283651\n",
      "Epoch  7142  G loss  0.021869052\n",
      "Epoch  7143  G loss  0.022907767\n",
      "Epoch  7144  G loss  0.014927067\n",
      "Epoch  7145  G loss  0.006858254\n",
      "Epoch  7146  G loss  0.010113706\n",
      "Epoch  7147  G loss  0.04902924\n",
      "Epoch  7148  G loss  0.0039123083\n",
      "Epoch  7149  G loss  0.26599413\n",
      "Epoch  7150  G loss  0.03501255\n",
      "Epoch  7151  G loss  0.066452205\n",
      "Epoch  7152  G loss  0.08062063\n",
      "Epoch  7153  G loss  0.0061454535\n",
      "Epoch  7154  G loss  0.02674191\n",
      "Epoch  7155  G loss  0.030024163\n",
      "Epoch  7156  G loss  0.012656646\n",
      "Epoch  7157  G loss  0.14229316\n",
      "Epoch  7158  G loss  0.0046660416\n",
      "Epoch  7159  G loss  0.10215305\n",
      "Epoch  7160  G loss  0.33868992\n",
      "Epoch  7161  G loss  0.009668194\n",
      "Epoch  7162  G loss  0.033200048\n",
      "Epoch  7163  G loss  0.012922036\n",
      "Epoch  7164  G loss  0.07166472\n",
      "Epoch  7165  G loss  0.044609413\n",
      "Epoch  7166  G loss  0.0058343764\n",
      "Epoch  7167  G loss  0.008001863\n",
      "Epoch  7168  G loss  0.006510265\n",
      "Epoch  7169  G loss  0.12984318\n",
      "Epoch  7170  G loss  0.0352939\n",
      "Epoch  7171  G loss  0.0071918536\n",
      "Epoch  7172  G loss  0.20781621\n",
      "Epoch  7173  G loss  0.035974048\n",
      "Epoch  7174  G loss  0.017693201\n",
      "Epoch  7175  G loss  0.012491391\n",
      "Epoch  7176  G loss  0.03724912\n",
      "Epoch  7177  G loss  0.012043586\n",
      "Epoch  7178  G loss  0.053194366\n",
      "Epoch  7179  G loss  0.05387209\n",
      "Epoch  7180  G loss  0.02322115\n",
      "Epoch  7181  G loss  0.06404823\n",
      "Epoch  7182  G loss  0.009305674\n",
      "Epoch  7183  G loss  0.028178005\n",
      "Epoch  7184  G loss  0.00999332\n",
      "Epoch  7185  G loss  0.04182781\n",
      "Epoch  7186  G loss  0.046137013\n",
      "Epoch  7187  G loss  0.042482942\n",
      "Epoch  7188  G loss  0.06293526\n",
      "Epoch  7189  G loss  0.008284836\n",
      "Epoch  7190  G loss  0.051820196\n",
      "Epoch  7191  G loss  0.026751395\n",
      "Epoch  7192  G loss  0.006364666\n",
      "Epoch  7193  G loss  0.017596418\n",
      "Epoch  7194  G loss  0.0440797\n",
      "Epoch  7195  G loss  0.014111141\n",
      "Epoch  7196  G loss  0.08982587\n",
      "Epoch  7197  G loss  0.016521215\n",
      "Epoch  7198  G loss  0.013334996\n",
      "Epoch  7199  G loss  0.014730314\n",
      "Epoch  7200  G loss  0.019029226\n",
      "Epoch  7201  G loss  0.024962038\n",
      "Epoch  7202  G loss  0.07191896\n",
      "Epoch  7203  G loss  0.19093966\n",
      "Epoch  7204  G loss  0.085677415\n",
      "Epoch  7205  G loss  0.031875152\n",
      "Epoch  7206  G loss  0.011188235\n",
      "Epoch  7207  G loss  0.046123512\n",
      "Epoch  7208  G loss  0.14043228\n",
      "Epoch  7209  G loss  0.03746917\n",
      "Epoch  7210  G loss  0.058559403\n",
      "Epoch  7211  G loss  0.041591235\n",
      "Epoch  7212  G loss  0.02004736\n",
      "Epoch  7213  G loss  0.0052560465\n",
      "Epoch  7214  G loss  0.16424018\n",
      "Epoch  7215  G loss  0.30129376\n",
      "Epoch  7216  G loss  0.11331832\n",
      "Epoch  7217  G loss  0.030813383\n",
      "Epoch  7218  G loss  0.06607761\n",
      "Epoch  7219  G loss  0.036741514\n",
      "Epoch  7220  G loss  0.007966751\n",
      "Epoch  7221  G loss  0.005829575\n",
      "Epoch  7222  G loss  0.060979024\n",
      "Epoch  7223  G loss  0.055084556\n",
      "Epoch  7224  G loss  0.006676789\n",
      "Epoch  7225  G loss  0.04147798\n",
      "Epoch  7226  G loss  0.018382244\n",
      "Epoch  7227  G loss  0.007183685\n",
      "Epoch  7228  G loss  0.008084642\n",
      "Epoch  7229  G loss  0.025507344\n",
      "Epoch  7230  G loss  0.027093526\n",
      "Epoch  7231  G loss  0.0151435565\n",
      "Epoch  7232  G loss  0.008731836\n",
      "Epoch  7233  G loss  0.029379051\n",
      "Epoch  7234  G loss  0.0795491\n",
      "Epoch  7235  G loss  0.020513788\n",
      "Epoch  7236  G loss  0.022915378\n",
      "Epoch  7237  G loss  0.013881415\n",
      "Epoch  7238  G loss  0.07655159\n",
      "Epoch  7239  G loss  0.14880736\n",
      "Epoch  7240  G loss  0.007695836\n",
      "Epoch  7241  G loss  0.032329597\n",
      "Epoch  7242  G loss  0.026841402\n",
      "Epoch  7243  G loss  0.03986751\n",
      "Epoch  7244  G loss  0.023211338\n",
      "Epoch  7245  G loss  0.0051874695\n",
      "Epoch  7246  G loss  0.06616528\n",
      "Epoch  7247  G loss  0.03684392\n",
      "Epoch  7248  G loss  0.0149447685\n",
      "Epoch  7249  G loss  0.039506003\n",
      "Epoch  7250  G loss  0.014979913\n",
      "Epoch  7251  G loss  0.06460671\n",
      "Epoch  7252  G loss  0.006991643\n",
      "Epoch  7253  G loss  0.030080661\n",
      "Epoch  7254  G loss  0.012337776\n",
      "Epoch  7255  G loss  0.07646351\n",
      "Epoch  7256  G loss  0.12122154\n",
      "Epoch  7257  G loss  0.1836656\n",
      "Epoch  7258  G loss  0.09333576\n",
      "Epoch  7259  G loss  0.014597105\n",
      "Epoch  7260  G loss  0.023952367\n",
      "Epoch  7261  G loss  0.032297708\n",
      "Epoch  7262  G loss  0.04951338\n",
      "Epoch  7263  G loss  0.044583756\n",
      "Epoch  7264  G loss  0.046557207\n",
      "Epoch  7265  G loss  0.021266073\n",
      "Epoch  7266  G loss  0.03772874\n",
      "Epoch  7267  G loss  0.2891417\n",
      "Epoch  7268  G loss  0.007592246\n",
      "Epoch  7269  G loss  0.076137304\n",
      "Epoch  7270  G loss  0.17819975\n",
      "Epoch  7271  G loss  0.008542011\n",
      "Epoch  7272  G loss  0.027389282\n",
      "Epoch  7273  G loss  0.014327176\n",
      "Epoch  7274  G loss  0.024268568\n",
      "Epoch  7275  G loss  0.008272323\n",
      "Epoch  7276  G loss  0.073292725\n",
      "Epoch  7277  G loss  0.0064594746\n",
      "Epoch  7278  G loss  0.011185829\n",
      "Epoch  7279  G loss  0.022345752\n",
      "Epoch  7280  G loss  0.013109155\n",
      "Epoch  7281  G loss  0.09767243\n",
      "Epoch  7282  G loss  0.023842225\n",
      "Epoch  7283  G loss  0.0049282867\n",
      "Epoch  7284  G loss  0.09142482\n",
      "Epoch  7285  G loss  0.016195487\n",
      "Epoch  7286  G loss  0.010408029\n",
      "Epoch  7287  G loss  0.0646285\n",
      "Epoch  7288  G loss  0.11550565\n",
      "Epoch  7289  G loss  0.0038207863\n",
      "Epoch  7290  G loss  0.013143777\n",
      "Epoch  7291  G loss  0.021455664\n",
      "Epoch  7292  G loss  0.048479743\n",
      "Epoch  7293  G loss  0.024415102\n",
      "Epoch  7294  G loss  0.06337845\n",
      "Epoch  7295  G loss  0.021243306\n",
      "Epoch  7296  G loss  0.18884456\n",
      "Epoch  7297  G loss  0.020715605\n",
      "Epoch  7298  G loss  0.09096426\n",
      "Epoch  7299  G loss  0.02307142\n",
      "Epoch  7300  G loss  0.008007905\n",
      "Epoch  7301  G loss  0.0065711387\n",
      "Epoch  7302  G loss  0.028847152\n",
      "Epoch  7303  G loss  0.042687584\n",
      "Epoch  7304  G loss  0.029719725\n",
      "Epoch  7305  G loss  0.21758735\n",
      "Epoch  7306  G loss  0.115757585\n",
      "Epoch  7307  G loss  0.25501597\n",
      "Epoch  7308  G loss  0.023807324\n",
      "Epoch  7309  G loss  0.03150531\n",
      "Epoch  7310  G loss  0.015515009\n",
      "Epoch  7311  G loss  0.025241606\n",
      "Epoch  7312  G loss  0.033889454\n",
      "Epoch  7313  G loss  0.084942415\n",
      "Epoch  7314  G loss  0.09225612\n",
      "Epoch  7315  G loss  0.081543654\n",
      "Epoch  7316  G loss  0.040518805\n",
      "Epoch  7317  G loss  0.048897766\n",
      "Epoch  7318  G loss  0.029656937\n",
      "Epoch  7319  G loss  0.016834175\n",
      "Epoch  7320  G loss  0.042543728\n",
      "Epoch  7321  G loss  0.12502256\n",
      "Epoch  7322  G loss  0.01523836\n",
      "Epoch  7323  G loss  0.15156548\n",
      "Epoch  7324  G loss  0.0088613555\n",
      "Epoch  7325  G loss  0.0063074436\n",
      "Epoch  7326  G loss  0.021223687\n",
      "Epoch  7327  G loss  0.05150634\n",
      "Epoch  7328  G loss  0.011859137\n",
      "Epoch  7329  G loss  0.01971913\n",
      "Epoch  7330  G loss  0.009377232\n",
      "Epoch  7331  G loss  0.010139914\n",
      "Epoch  7332  G loss  0.030312274\n",
      "Epoch  7333  G loss  0.079982415\n",
      "Epoch  7334  G loss  0.12144197\n",
      "Epoch  7335  G loss  0.035322957\n",
      "Epoch  7336  G loss  0.025546081\n",
      "Epoch  7337  G loss  0.08280395\n",
      "Epoch  7338  G loss  0.07569109\n",
      "Epoch  7339  G loss  0.022197155\n",
      "Epoch  7340  G loss  0.0128310835\n",
      "Epoch  7341  G loss  0.0049294485\n",
      "Epoch  7342  G loss  0.019222943\n",
      "Epoch  7343  G loss  0.11765193\n",
      "Epoch  7344  G loss  0.007397699\n",
      "Epoch  7345  G loss  0.044441223\n",
      "Epoch  7346  G loss  0.0774679\n",
      "Epoch  7347  G loss  0.004576885\n",
      "Epoch  7348  G loss  0.18039915\n",
      "Epoch  7349  G loss  0.017130043\n",
      "Epoch  7350  G loss  0.019160444\n",
      "Epoch  7351  G loss  0.02579701\n",
      "Epoch  7352  G loss  0.019266857\n",
      "Epoch  7353  G loss  0.0868839\n",
      "Epoch  7354  G loss  0.03379075\n",
      "Epoch  7355  G loss  0.11603574\n",
      "Epoch  7356  G loss  0.012671546\n",
      "Epoch  7357  G loss  0.028528905\n",
      "Epoch  7358  G loss  0.084358126\n",
      "Epoch  7359  G loss  0.08359121\n",
      "Epoch  7360  G loss  0.017387211\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  7361  G loss  0.26544598\n",
      "Epoch  7362  G loss  0.06193658\n",
      "Epoch  7363  G loss  0.018655773\n",
      "Epoch  7364  G loss  0.020177167\n",
      "Epoch  7365  G loss  0.028398976\n",
      "Epoch  7366  G loss  0.027993077\n",
      "Epoch  7367  G loss  0.019683098\n",
      "Epoch  7368  G loss  0.026565004\n",
      "Epoch  7369  G loss  0.080289066\n",
      "Epoch  7370  G loss  0.02495471\n",
      "Epoch  7371  G loss  0.012497223\n",
      "Epoch  7372  G loss  0.028109133\n",
      "Epoch  7373  G loss  0.18312234\n",
      "Epoch  7374  G loss  0.1452783\n",
      "Epoch  7375  G loss  0.04787594\n",
      "Epoch  7376  G loss  0.005726956\n",
      "Epoch  7377  G loss  0.029847289\n",
      "Epoch  7378  G loss  0.012213143\n",
      "Epoch  7379  G loss  0.09651546\n",
      "Epoch  7380  G loss  0.07921023\n",
      "Epoch  7381  G loss  0.020994145\n",
      "Epoch  7382  G loss  0.011351647\n",
      "Epoch  7383  G loss  0.009618076\n",
      "Epoch  7384  G loss  0.019272443\n",
      "Epoch  7385  G loss  0.014154756\n",
      "Epoch  7386  G loss  0.043619245\n",
      "Epoch  7387  G loss  0.096750565\n",
      "Epoch  7388  G loss  0.08220175\n",
      "Epoch  7389  G loss  0.013362508\n",
      "Epoch  7390  G loss  0.017506944\n",
      "Epoch  7391  G loss  0.023306046\n",
      "Epoch  7392  G loss  0.003534943\n",
      "Epoch  7393  G loss  0.005936222\n",
      "Epoch  7394  G loss  0.0069633173\n",
      "Epoch  7395  G loss  0.01086851\n",
      "Epoch  7396  G loss  0.012418121\n",
      "Epoch  7397  G loss  0.032400448\n",
      "Epoch  7398  G loss  0.024332622\n",
      "Epoch  7399  G loss  0.03164325\n",
      "Epoch  7400  G loss  0.04163794\n",
      "Epoch  7401  G loss  0.059799485\n",
      "Epoch  7402  G loss  0.053711224\n",
      "Epoch  7403  G loss  0.0045569628\n",
      "Epoch  7404  G loss  0.022043249\n",
      "Epoch  7405  G loss  0.027140532\n",
      "Epoch  7406  G loss  0.25546357\n",
      "Epoch  7407  G loss  0.008522619\n",
      "Epoch  7408  G loss  0.027723726\n",
      "Epoch  7409  G loss  0.014685068\n",
      "Epoch  7410  G loss  0.03882049\n",
      "Epoch  7411  G loss  0.025398396\n",
      "Epoch  7412  G loss  0.01680015\n",
      "Epoch  7413  G loss  0.008738282\n",
      "Epoch  7414  G loss  0.014980311\n",
      "Epoch  7415  G loss  0.040336754\n",
      "Epoch  7416  G loss  0.053576287\n",
      "Epoch  7417  G loss  0.024958307\n",
      "Epoch  7418  G loss  0.12338417\n",
      "Epoch  7419  G loss  0.011217809\n",
      "Epoch  7420  G loss  0.049816426\n",
      "Epoch  7421  G loss  0.026230562\n",
      "Epoch  7422  G loss  0.16529632\n",
      "Epoch  7423  G loss  0.4042337\n",
      "Epoch  7424  G loss  0.009747165\n",
      "Epoch  7425  G loss  0.33706367\n",
      "Epoch  7426  G loss  0.30936253\n",
      "Epoch  7427  G loss  0.0069236066\n",
      "Epoch  7428  G loss  0.04932184\n",
      "Epoch  7429  G loss  0.085792094\n",
      "Epoch  7430  G loss  0.03589502\n",
      "Epoch  7431  G loss  0.03743439\n",
      "Epoch  7432  G loss  0.019049061\n",
      "Epoch  7433  G loss  0.03242816\n",
      "Epoch  7434  G loss  0.03139852\n",
      "Epoch  7435  G loss  0.06804599\n",
      "Epoch  7436  G loss  0.036913134\n",
      "Epoch  7437  G loss  0.0039405446\n",
      "Epoch  7438  G loss  0.034405157\n",
      "Epoch  7439  G loss  0.046578553\n",
      "Epoch  7440  G loss  0.07685141\n",
      "Epoch  7441  G loss  0.018471608\n",
      "Epoch  7442  G loss  0.08242491\n",
      "Epoch  7443  G loss  0.04305486\n",
      "Epoch  7444  G loss  0.009041785\n",
      "Epoch  7445  G loss  0.17676274\n",
      "Epoch  7446  G loss  0.017090624\n",
      "Epoch  7447  G loss  0.059829064\n",
      "Epoch  7448  G loss  0.00807097\n",
      "Epoch  7449  G loss  0.011809055\n",
      "Epoch  7450  G loss  0.051134072\n",
      "Epoch  7451  G loss  0.0030578573\n",
      "Epoch  7452  G loss  0.008223884\n",
      "Epoch  7453  G loss  0.0025664065\n",
      "Epoch  7454  G loss  0.030920165\n",
      "Epoch  7455  G loss  0.0072464766\n",
      "Epoch  7456  G loss  0.04245185\n",
      "Epoch  7457  G loss  0.011801353\n",
      "Epoch  7458  G loss  0.02625553\n",
      "Epoch  7459  G loss  0.016026635\n",
      "Epoch  7460  G loss  0.06757063\n",
      "Epoch  7461  G loss  0.022615999\n",
      "Epoch  7462  G loss  0.23521549\n",
      "Epoch  7463  G loss  0.02142838\n",
      "Epoch  7464  G loss  0.013698444\n",
      "Epoch  7465  G loss  0.025075985\n",
      "Epoch  7466  G loss  0.024521966\n",
      "Epoch  7467  G loss  0.19569167\n",
      "Epoch  7468  G loss  0.025393862\n",
      "Epoch  7469  G loss  0.028516382\n",
      "Epoch  7470  G loss  0.035772096\n",
      "Epoch  7471  G loss  0.060644418\n",
      "Epoch  7472  G loss  0.05380878\n",
      "Epoch  7473  G loss  0.022819212\n",
      "Epoch  7474  G loss  0.1370762\n",
      "Epoch  7475  G loss  0.015316565\n",
      "Epoch  7476  G loss  0.015642107\n",
      "Epoch  7477  G loss  0.053520784\n",
      "Epoch  7478  G loss  0.028622087\n",
      "Epoch  7479  G loss  0.14744383\n",
      "Epoch  7480  G loss  0.12827541\n",
      "Epoch  7481  G loss  0.046632186\n",
      "Epoch  7482  G loss  0.04228677\n",
      "Epoch  7483  G loss  0.014744755\n",
      "Epoch  7484  G loss  0.015777688\n",
      "Epoch  7485  G loss  0.053134695\n",
      "Epoch  7486  G loss  0.0889782\n",
      "Epoch  7487  G loss  0.019245617\n",
      "Epoch  7488  G loss  0.0548997\n",
      "Epoch  7489  G loss  0.09703521\n",
      "Epoch  7490  G loss  0.0075831725\n",
      "Epoch  7491  G loss  0.0034507893\n",
      "Epoch  7492  G loss  0.053090163\n",
      "Epoch  7493  G loss  0.03776283\n",
      "Epoch  7494  G loss  0.0061620083\n",
      "Epoch  7495  G loss  0.10217394\n",
      "Epoch  7496  G loss  0.015384469\n",
      "Epoch  7497  G loss  0.059644848\n",
      "Epoch  7498  G loss  0.057807513\n",
      "Epoch  7499  G loss  0.028244525\n",
      "Epoch  7500  G loss  0.07666917\n",
      "Epoch  7501  G loss  0.08072118\n",
      "Epoch  7502  G loss  0.03214038\n",
      "Epoch  7503  G loss  0.05507398\n",
      "Epoch  7504  G loss  0.02884157\n",
      "Epoch  7505  G loss  0.044282027\n",
      "Epoch  7506  G loss  0.026558042\n",
      "Epoch  7507  G loss  0.13369761\n",
      "Epoch  7508  G loss  0.07289806\n",
      "Epoch  7509  G loss  0.24542417\n",
      "Epoch  7510  G loss  0.010136902\n",
      "Epoch  7511  G loss  0.018787079\n",
      "Epoch  7512  G loss  0.0116414195\n",
      "Epoch  7513  G loss  0.014296373\n",
      "Epoch  7514  G loss  0.063449696\n",
      "Epoch  7515  G loss  0.024182219\n",
      "Epoch  7516  G loss  0.042084582\n",
      "Epoch  7517  G loss  0.059630144\n",
      "Epoch  7518  G loss  0.005085281\n",
      "Epoch  7519  G loss  0.010182463\n",
      "Epoch  7520  G loss  0.07069315\n",
      "Epoch  7521  G loss  0.023528133\n",
      "Epoch  7522  G loss  0.06709516\n",
      "Epoch  7523  G loss  0.023891713\n",
      "Epoch  7524  G loss  0.020689946\n",
      "Epoch  7525  G loss  0.017711362\n",
      "Epoch  7526  G loss  0.005286806\n",
      "Epoch  7527  G loss  0.007738959\n",
      "Epoch  7528  G loss  0.06877404\n",
      "Epoch  7529  G loss  0.083238214\n",
      "Epoch  7530  G loss  0.019254837\n",
      "Epoch  7531  G loss  0.012732968\n",
      "Epoch  7532  G loss  0.0062377015\n",
      "Epoch  7533  G loss  0.00549761\n",
      "Epoch  7534  G loss  0.10377799\n",
      "Epoch  7535  G loss  0.005632961\n",
      "Epoch  7536  G loss  0.039656643\n",
      "Epoch  7537  G loss  0.19527319\n",
      "Epoch  7538  G loss  0.026181303\n",
      "Epoch  7539  G loss  0.074886814\n",
      "Epoch  7540  G loss  0.16701597\n",
      "Epoch  7541  G loss  0.10372155\n",
      "Epoch  7542  G loss  0.0036186425\n",
      "Epoch  7543  G loss  0.014349988\n",
      "Epoch  7544  G loss  0.014925847\n",
      "Epoch  7545  G loss  0.050006196\n",
      "Epoch  7546  G loss  0.1721899\n",
      "Epoch  7547  G loss  0.04160311\n",
      "Epoch  7548  G loss  0.004635619\n",
      "Epoch  7549  G loss  0.03993435\n",
      "Epoch  7550  G loss  0.040579315\n",
      "Epoch  7551  G loss  0.0072725\n",
      "Epoch  7552  G loss  0.012974692\n",
      "Epoch  7553  G loss  0.007605185\n",
      "Epoch  7554  G loss  0.013327443\n",
      "Epoch  7555  G loss  0.1079258\n",
      "Epoch  7556  G loss  0.01177337\n",
      "Epoch  7557  G loss  0.07217355\n",
      "Epoch  7558  G loss  0.20233308\n",
      "Epoch  7559  G loss  0.008019963\n",
      "Epoch  7560  G loss  0.14787364\n",
      "Epoch  7561  G loss  0.023063686\n",
      "Epoch  7562  G loss  0.0038255171\n",
      "Epoch  7563  G loss  0.029987222\n",
      "Epoch  7564  G loss  0.005382568\n",
      "Epoch  7565  G loss  0.017634092\n",
      "Epoch  7566  G loss  0.05699902\n",
      "Epoch  7567  G loss  0.018734964\n",
      "Epoch  7568  G loss  0.016446033\n",
      "Epoch  7569  G loss  0.010740634\n",
      "Epoch  7570  G loss  0.01732422\n",
      "Epoch  7571  G loss  0.10814274\n",
      "Epoch  7572  G loss  0.14913996\n",
      "Epoch  7573  G loss  0.010070967\n",
      "Epoch  7574  G loss  0.059370954\n",
      "Epoch  7575  G loss  0.030068977\n",
      "Epoch  7576  G loss  0.041755777\n",
      "Epoch  7577  G loss  0.029713798\n",
      "Epoch  7578  G loss  0.26094514\n",
      "Epoch  7579  G loss  0.0984621\n",
      "Epoch  7580  G loss  0.0810474\n",
      "Epoch  7581  G loss  0.010718174\n",
      "Epoch  7582  G loss  0.08594583\n",
      "Epoch  7583  G loss  0.04470972\n",
      "Epoch  7584  G loss  0.009669298\n",
      "Epoch  7585  G loss  0.07319491\n",
      "Epoch  7586  G loss  0.10574448\n",
      "Epoch  7587  G loss  0.02184756\n",
      "Epoch  7588  G loss  0.059809886\n",
      "Epoch  7589  G loss  0.07558505\n",
      "Epoch  7590  G loss  0.023790946\n",
      "Epoch  7591  G loss  0.01153696\n",
      "Epoch  7592  G loss  0.10551131\n",
      "Epoch  7593  G loss  0.08785242\n",
      "Epoch  7594  G loss  0.011646826\n",
      "Epoch  7595  G loss  0.031506903\n",
      "Epoch  7596  G loss  0.055101335\n",
      "Epoch  7597  G loss  0.020642072\n",
      "Epoch  7598  G loss  0.004644052\n",
      "Epoch  7599  G loss  0.00806202\n",
      "Epoch  7600  G loss  0.025502535\n",
      "Epoch  7601  G loss  0.0046981564\n",
      "Epoch  7602  G loss  0.034024473\n",
      "Epoch  7603  G loss  0.5051281\n",
      "Epoch  7604  G loss  0.041419625\n",
      "Epoch  7605  G loss  0.006329739\n",
      "Epoch  7606  G loss  0.022342885\n",
      "Epoch  7607  G loss  0.08894722\n",
      "Epoch  7608  G loss  0.07550121\n",
      "Epoch  7609  G loss  0.09041223\n",
      "Epoch  7610  G loss  0.06401868\n",
      "Epoch  7611  G loss  0.110707745\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  7612  G loss  0.095888376\n",
      "Epoch  7613  G loss  0.00653781\n",
      "Epoch  7614  G loss  0.048484106\n",
      "Epoch  7615  G loss  0.025863685\n",
      "Epoch  7616  G loss  0.025614321\n",
      "Epoch  7617  G loss  0.060314957\n",
      "Epoch  7618  G loss  0.103598565\n",
      "Epoch  7619  G loss  0.015851017\n",
      "Epoch  7620  G loss  0.013485189\n",
      "Epoch  7621  G loss  0.012783848\n",
      "Epoch  7622  G loss  0.022468755\n",
      "Epoch  7623  G loss  0.022456324\n",
      "Epoch  7624  G loss  0.019460343\n",
      "Epoch  7625  G loss  0.014520619\n",
      "Epoch  7626  G loss  0.009726306\n",
      "Epoch  7627  G loss  0.021562526\n",
      "Epoch  7628  G loss  0.051388606\n",
      "Epoch  7629  G loss  0.023702156\n",
      "Epoch  7630  G loss  0.071000785\n",
      "Epoch  7631  G loss  0.09871968\n",
      "Epoch  7632  G loss  0.017991556\n",
      "Epoch  7633  G loss  0.007196421\n",
      "Epoch  7634  G loss  0.041607022\n",
      "Epoch  7635  G loss  0.013974449\n",
      "Epoch  7636  G loss  0.006247133\n",
      "Epoch  7637  G loss  0.07985482\n",
      "Epoch  7638  G loss  0.09622076\n",
      "Epoch  7639  G loss  0.005215852\n",
      "Epoch  7640  G loss  0.047456942\n",
      "Epoch  7641  G loss  0.0754629\n",
      "Epoch  7642  G loss  0.0080636125\n",
      "Epoch  7643  G loss  0.010963023\n",
      "Epoch  7644  G loss  0.010032454\n",
      "Epoch  7645  G loss  0.017540665\n",
      "Epoch  7646  G loss  0.016613694\n",
      "Epoch  7647  G loss  0.021670813\n",
      "Epoch  7648  G loss  0.018998723\n",
      "Epoch  7649  G loss  0.2971955\n",
      "Epoch  7650  G loss  0.015245017\n",
      "Epoch  7651  G loss  0.0038895612\n",
      "Epoch  7652  G loss  0.014754619\n",
      "Epoch  7653  G loss  0.056982554\n",
      "Epoch  7654  G loss  0.017770253\n",
      "Epoch  7655  G loss  0.005328419\n",
      "Epoch  7656  G loss  0.14769864\n",
      "Epoch  7657  G loss  0.027039431\n",
      "Epoch  7658  G loss  0.017810607\n",
      "Epoch  7659  G loss  0.021854546\n",
      "Epoch  7660  G loss  0.020320456\n",
      "Epoch  7661  G loss  0.0313299\n",
      "Epoch  7662  G loss  0.15690121\n",
      "Epoch  7663  G loss  0.06334699\n",
      "Epoch  7664  G loss  0.004988889\n",
      "Epoch  7665  G loss  0.020247862\n",
      "Epoch  7666  G loss  0.006830814\n",
      "Epoch  7667  G loss  0.034849487\n",
      "Epoch  7668  G loss  0.057779867\n",
      "Epoch  7669  G loss  0.009155399\n",
      "Epoch  7670  G loss  0.04391036\n",
      "Epoch  7671  G loss  0.07112688\n",
      "Epoch  7672  G loss  0.036842365\n",
      "Epoch  7673  G loss  0.053712666\n",
      "Epoch  7674  G loss  0.033589564\n",
      "Epoch  7675  G loss  0.09438511\n",
      "Epoch  7676  G loss  0.022145031\n",
      "Epoch  7677  G loss  0.08093965\n",
      "Epoch  7678  G loss  0.15666795\n",
      "Epoch  7679  G loss  0.07538129\n",
      "Epoch  7680  G loss  0.07418917\n",
      "Epoch  7681  G loss  0.009561164\n",
      "Epoch  7682  G loss  0.039711937\n",
      "Epoch  7683  G loss  0.009624571\n",
      "Epoch  7684  G loss  0.048634835\n",
      "Epoch  7685  G loss  0.12313919\n",
      "Epoch  7686  G loss  0.013967263\n",
      "Epoch  7687  G loss  0.0049991943\n",
      "Epoch  7688  G loss  0.012488483\n",
      "Epoch  7689  G loss  0.014722111\n",
      "Epoch  7690  G loss  0.006011483\n",
      "Epoch  7691  G loss  0.06958807\n",
      "Epoch  7692  G loss  0.01985874\n",
      "Epoch  7693  G loss  0.0067140115\n",
      "Epoch  7694  G loss  0.06070333\n",
      "Epoch  7695  G loss  0.012211543\n",
      "Epoch  7696  G loss  0.0441498\n",
      "Epoch  7697  G loss  0.007803592\n",
      "Epoch  7698  G loss  0.007140699\n",
      "Epoch  7699  G loss  0.15093313\n",
      "Epoch  7700  G loss  0.014314441\n",
      "Epoch  7701  G loss  0.010653801\n",
      "Epoch  7702  G loss  0.030760646\n",
      "Epoch  7703  G loss  0.029961469\n",
      "Epoch  7704  G loss  0.03866952\n",
      "Epoch  7705  G loss  0.0140665835\n",
      "Epoch  7706  G loss  0.08452959\n",
      "Epoch  7707  G loss  0.15367836\n",
      "Epoch  7708  G loss  0.047279403\n",
      "Epoch  7709  G loss  0.06982653\n",
      "Epoch  7710  G loss  0.012287743\n",
      "Epoch  7711  G loss  0.007512756\n",
      "Epoch  7712  G loss  0.06410087\n",
      "Epoch  7713  G loss  0.053165913\n",
      "Epoch  7714  G loss  0.03199207\n",
      "Epoch  7715  G loss  0.11735462\n",
      "Epoch  7716  G loss  0.29051852\n",
      "Epoch  7717  G loss  0.020625785\n",
      "Epoch  7718  G loss  0.019795481\n",
      "Epoch  7719  G loss  0.12984508\n",
      "Epoch  7720  G loss  0.015859757\n",
      "Epoch  7721  G loss  0.010704514\n",
      "Epoch  7722  G loss  0.027767984\n",
      "Epoch  7723  G loss  0.004967555\n",
      "Epoch  7724  G loss  0.00390008\n",
      "Epoch  7725  G loss  0.013247116\n",
      "Epoch  7726  G loss  0.030263564\n",
      "Epoch  7727  G loss  0.02960165\n",
      "Epoch  7728  G loss  0.059842348\n",
      "Epoch  7729  G loss  0.14135253\n",
      "Epoch  7730  G loss  0.069323465\n",
      "Epoch  7731  G loss  0.019064607\n",
      "Epoch  7732  G loss  0.018665157\n",
      "Epoch  7733  G loss  0.04025021\n",
      "Epoch  7734  G loss  0.112939805\n",
      "Epoch  7735  G loss  0.047491893\n",
      "Epoch  7736  G loss  0.07550026\n",
      "Epoch  7737  G loss  0.0042676236\n",
      "Epoch  7738  G loss  0.064668626\n",
      "Epoch  7739  G loss  0.039060485\n",
      "Epoch  7740  G loss  0.004522513\n",
      "Epoch  7741  G loss  0.03119226\n",
      "Epoch  7742  G loss  0.07251678\n",
      "Epoch  7743  G loss  0.051116593\n",
      "Epoch  7744  G loss  0.01364788\n",
      "Epoch  7745  G loss  0.07738003\n",
      "Epoch  7746  G loss  0.10673024\n",
      "Epoch  7747  G loss  0.0046980986\n",
      "Epoch  7748  G loss  0.038444042\n",
      "Epoch  7749  G loss  0.10385621\n",
      "Epoch  7750  G loss  0.018976774\n",
      "Epoch  7751  G loss  0.0139823\n",
      "Epoch  7752  G loss  0.012069876\n",
      "Epoch  7753  G loss  0.011027709\n",
      "Epoch  7754  G loss  0.012478118\n",
      "Epoch  7755  G loss  0.044134118\n",
      "Epoch  7756  G loss  0.055905394\n",
      "Epoch  7757  G loss  0.009836907\n",
      "Epoch  7758  G loss  0.2304824\n",
      "Epoch  7759  G loss  0.068136685\n",
      "Epoch  7760  G loss  0.025960155\n",
      "Epoch  7761  G loss  0.023037044\n",
      "Epoch  7762  G loss  0.011423059\n",
      "Epoch  7763  G loss  0.057786178\n",
      "Epoch  7764  G loss  0.019538064\n",
      "Epoch  7765  G loss  0.08625141\n",
      "Epoch  7766  G loss  0.21470267\n",
      "Epoch  7767  G loss  0.024640188\n",
      "Epoch  7768  G loss  0.038301706\n",
      "Epoch  7769  G loss  0.009739705\n",
      "Epoch  7770  G loss  0.03485103\n",
      "Epoch  7771  G loss  0.017852819\n",
      "Epoch  7772  G loss  0.23105831\n",
      "Epoch  7773  G loss  0.02668484\n",
      "Epoch  7774  G loss  0.020157315\n",
      "Epoch  7775  G loss  0.014939165\n",
      "Epoch  7776  G loss  0.019294553\n",
      "Epoch  7777  G loss  0.024260355\n",
      "Epoch  7778  G loss  0.061736085\n",
      "Epoch  7779  G loss  0.011138145\n",
      "Epoch  7780  G loss  0.019990286\n",
      "Epoch  7781  G loss  0.0050185015\n",
      "Epoch  7782  G loss  0.028961439\n",
      "Epoch  7783  G loss  0.0521442\n",
      "Epoch  7784  G loss  0.03211585\n",
      "Epoch  7785  G loss  0.06669132\n",
      "Epoch  7786  G loss  0.03662228\n",
      "Epoch  7787  G loss  0.033246543\n",
      "Epoch  7788  G loss  0.048258785\n",
      "Epoch  7789  G loss  0.022461193\n",
      "Epoch  7790  G loss  0.019070234\n",
      "Epoch  7791  G loss  0.07246435\n",
      "Epoch  7792  G loss  0.029563606\n",
      "Epoch  7793  G loss  0.030670747\n",
      "Epoch  7794  G loss  0.0105104735\n",
      "Epoch  7795  G loss  0.0116002\n",
      "Epoch  7796  G loss  0.007808958\n",
      "Epoch  7797  G loss  0.011982463\n",
      "Epoch  7798  G loss  0.02005887\n",
      "Epoch  7799  G loss  0.056230277\n",
      "Epoch  7800  G loss  0.074266955\n",
      "Epoch  7801  G loss  0.044631474\n",
      "Epoch  7802  G loss  0.060983025\n",
      "Epoch  7803  G loss  0.0051294826\n",
      "Epoch  7804  G loss  0.009375045\n",
      "Epoch  7805  G loss  0.061788768\n",
      "Epoch  7806  G loss  0.17455713\n",
      "Epoch  7807  G loss  0.09281719\n",
      "Epoch  7808  G loss  0.0105763525\n",
      "Epoch  7809  G loss  0.0090464335\n",
      "Epoch  7810  G loss  0.121397614\n",
      "Epoch  7811  G loss  0.15499923\n",
      "Epoch  7812  G loss  0.005932117\n",
      "Epoch  7813  G loss  0.039980434\n",
      "Epoch  7814  G loss  0.022953741\n",
      "Epoch  7815  G loss  0.018107533\n",
      "Epoch  7816  G loss  0.015830344\n",
      "Epoch  7817  G loss  0.027885275\n",
      "Epoch  7818  G loss  0.033904098\n",
      "Epoch  7819  G loss  0.02514844\n",
      "Epoch  7820  G loss  0.02953496\n",
      "Epoch  7821  G loss  0.040964022\n",
      "Epoch  7822  G loss  0.0066322885\n",
      "Epoch  7823  G loss  0.057836477\n",
      "Epoch  7824  G loss  0.026845414\n",
      "Epoch  7825  G loss  0.016185155\n",
      "Epoch  7826  G loss  0.039605334\n",
      "Epoch  7827  G loss  0.111725956\n",
      "Epoch  7828  G loss  0.010449474\n",
      "Epoch  7829  G loss  0.24079691\n",
      "Epoch  7830  G loss  0.067361385\n",
      "Epoch  7831  G loss  0.011964459\n",
      "Epoch  7832  G loss  0.05878473\n",
      "Epoch  7833  G loss  0.05577474\n",
      "Epoch  7834  G loss  0.060213827\n",
      "Epoch  7835  G loss  0.038233116\n",
      "Epoch  7836  G loss  0.012701024\n",
      "Epoch  7837  G loss  0.013499189\n",
      "Epoch  7838  G loss  0.103544876\n",
      "Epoch  7839  G loss  0.024005886\n",
      "Epoch  7840  G loss  0.010524217\n",
      "Epoch  7841  G loss  0.044211723\n",
      "Epoch  7842  G loss  0.013315657\n",
      "Epoch  7843  G loss  0.007643507\n",
      "Epoch  7844  G loss  0.012386558\n",
      "Epoch  7845  G loss  0.043013472\n",
      "Epoch  7846  G loss  0.007918179\n",
      "Epoch  7847  G loss  0.003680685\n",
      "Epoch  7848  G loss  0.016358927\n",
      "Epoch  7849  G loss  0.030070454\n",
      "Epoch  7850  G loss  0.16217986\n",
      "Epoch  7851  G loss  0.015796535\n",
      "Epoch  7852  G loss  0.019861592\n",
      "Epoch  7853  G loss  0.01224984\n",
      "Epoch  7854  G loss  0.1549386\n",
      "Epoch  7855  G loss  0.16099909\n",
      "Epoch  7856  G loss  0.019285658\n",
      "Epoch  7857  G loss  0.10595882\n",
      "Epoch  7858  G loss  0.04764434\n",
      "Epoch  7859  G loss  0.1758193\n",
      "Epoch  7860  G loss  0.007509595\n",
      "Epoch  7861  G loss  0.031829014\n",
      "Epoch  7862  G loss  0.13508895\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  7863  G loss  0.022361208\n",
      "Epoch  7864  G loss  0.012069538\n",
      "Epoch  7865  G loss  0.01379128\n",
      "Epoch  7866  G loss  0.040392417\n",
      "Epoch  7867  G loss  0.07042405\n",
      "Epoch  7868  G loss  0.007191901\n",
      "Epoch  7869  G loss  0.029894711\n",
      "Epoch  7870  G loss  0.07481465\n",
      "Epoch  7871  G loss  0.016840953\n",
      "Epoch  7872  G loss  0.15856755\n",
      "Epoch  7873  G loss  0.021991476\n",
      "Epoch  7874  G loss  0.03270772\n",
      "Epoch  7875  G loss  0.019436944\n",
      "Epoch  7876  G loss  0.0911474\n",
      "Epoch  7877  G loss  0.009452285\n",
      "Epoch  7878  G loss  0.004375975\n",
      "Epoch  7879  G loss  0.008516898\n",
      "Epoch  7880  G loss  0.026081003\n",
      "Epoch  7881  G loss  0.04555186\n",
      "Epoch  7882  G loss  0.011252375\n",
      "Epoch  7883  G loss  0.0659786\n",
      "Epoch  7884  G loss  0.028269343\n",
      "Epoch  7885  G loss  0.06786883\n",
      "Epoch  7886  G loss  0.014274483\n",
      "Epoch  7887  G loss  0.066235214\n",
      "Epoch  7888  G loss  0.019187063\n",
      "Epoch  7889  G loss  0.006855651\n",
      "Epoch  7890  G loss  0.032483753\n",
      "Epoch  7891  G loss  0.0514711\n",
      "Epoch  7892  G loss  0.08612448\n",
      "Epoch  7893  G loss  0.048943266\n",
      "Epoch  7894  G loss  0.16720755\n",
      "Epoch  7895  G loss  0.028009236\n",
      "Epoch  7896  G loss  0.10260534\n",
      "Epoch  7897  G loss  0.10225038\n",
      "Epoch  7898  G loss  0.01763404\n",
      "Epoch  7899  G loss  0.016115725\n",
      "Epoch  7900  G loss  0.034738462\n",
      "Epoch  7901  G loss  0.019194935\n",
      "Epoch  7902  G loss  0.021666083\n",
      "Epoch  7903  G loss  0.27607805\n",
      "Epoch  7904  G loss  0.006198652\n",
      "Epoch  7905  G loss  0.024739994\n",
      "Epoch  7906  G loss  0.0069045476\n",
      "Epoch  7907  G loss  0.03867892\n",
      "Epoch  7908  G loss  0.07544176\n",
      "Epoch  7909  G loss  0.031175938\n",
      "Epoch  7910  G loss  0.022708591\n",
      "Epoch  7911  G loss  0.02052832\n",
      "Epoch  7912  G loss  0.05022783\n",
      "Epoch  7913  G loss  0.018572196\n",
      "Epoch  7914  G loss  0.20109177\n",
      "Epoch  7915  G loss  0.0108032245\n",
      "Epoch  7916  G loss  0.07462524\n",
      "Epoch  7917  G loss  0.00893817\n",
      "Epoch  7918  G loss  0.09262578\n",
      "Epoch  7919  G loss  0.03456402\n",
      "Epoch  7920  G loss  0.118120745\n",
      "Epoch  7921  G loss  0.12565553\n",
      "Epoch  7922  G loss  0.029351799\n",
      "Epoch  7923  G loss  0.029838493\n",
      "Epoch  7924  G loss  0.061040364\n",
      "Epoch  7925  G loss  0.04316198\n",
      "Epoch  7926  G loss  0.064425446\n",
      "Epoch  7927  G loss  0.04037152\n",
      "Epoch  7928  G loss  0.07801913\n",
      "Epoch  7929  G loss  0.12037429\n",
      "Epoch  7930  G loss  0.10809719\n",
      "Epoch  7931  G loss  0.02521132\n",
      "Epoch  7932  G loss  0.008344907\n",
      "Epoch  7933  G loss  0.06683195\n",
      "Epoch  7934  G loss  0.029666312\n",
      "Epoch  7935  G loss  0.03840376\n",
      "Epoch  7936  G loss  0.050771207\n",
      "Epoch  7937  G loss  0.025552826\n",
      "Epoch  7938  G loss  0.03714378\n",
      "Epoch  7939  G loss  0.004737581\n",
      "Epoch  7940  G loss  0.016313635\n",
      "Epoch  7941  G loss  0.0064910967\n",
      "Epoch  7942  G loss  0.011250159\n",
      "Epoch  7943  G loss  0.0040557417\n",
      "Epoch  7944  G loss  0.020133791\n",
      "Epoch  7945  G loss  0.0054131416\n",
      "Epoch  7946  G loss  0.1073093\n",
      "Epoch  7947  G loss  0.011305593\n",
      "Epoch  7948  G loss  0.08833382\n",
      "Epoch  7949  G loss  0.032452404\n",
      "Epoch  7950  G loss  0.004947517\n",
      "Epoch  7951  G loss  0.061369047\n",
      "Epoch  7952  G loss  0.064166024\n",
      "Epoch  7953  G loss  0.023527615\n",
      "Epoch  7954  G loss  0.2760024\n",
      "Epoch  7955  G loss  0.023761863\n",
      "Epoch  7956  G loss  0.009743234\n",
      "Epoch  7957  G loss  0.07655022\n",
      "Epoch  7958  G loss  0.02426679\n",
      "Epoch  7959  G loss  0.0075255446\n",
      "Epoch  7960  G loss  0.013342129\n",
      "Epoch  7961  G loss  0.0072990987\n",
      "Epoch  7962  G loss  0.008866938\n",
      "Epoch  7963  G loss  0.020766854\n",
      "Epoch  7964  G loss  0.028339231\n",
      "Epoch  7965  G loss  0.11950796\n",
      "Epoch  7966  G loss  0.008016612\n",
      "Epoch  7967  G loss  0.10126506\n",
      "Epoch  7968  G loss  0.06482774\n",
      "Epoch  7969  G loss  0.0030718432\n",
      "Epoch  7970  G loss  0.020703375\n",
      "Epoch  7971  G loss  0.031847287\n",
      "Epoch  7972  G loss  0.007161823\n",
      "Epoch  7973  G loss  0.025052503\n",
      "Epoch  7974  G loss  0.006979822\n",
      "Epoch  7975  G loss  0.011984933\n",
      "Epoch  7976  G loss  0.015917923\n",
      "Epoch  7977  G loss  0.014226342\n",
      "Epoch  7978  G loss  0.009845279\n",
      "Epoch  7979  G loss  0.06964925\n",
      "Epoch  7980  G loss  0.022907235\n",
      "Epoch  7981  G loss  0.030832496\n",
      "Epoch  7982  G loss  0.011632036\n",
      "Epoch  7983  G loss  0.009639757\n",
      "Epoch  7984  G loss  0.010899112\n",
      "Epoch  7985  G loss  0.00938261\n",
      "Epoch  7986  G loss  0.03416606\n",
      "Epoch  7987  G loss  0.025902139\n",
      "Epoch  7988  G loss  0.2786299\n",
      "Epoch  7989  G loss  0.04114011\n",
      "Epoch  7990  G loss  0.020083042\n",
      "Epoch  7991  G loss  0.006131989\n",
      "Epoch  7992  G loss  0.0048487955\n",
      "Epoch  7993  G loss  0.05568178\n",
      "Epoch  7994  G loss  0.07157935\n",
      "Epoch  7995  G loss  0.025006844\n",
      "Epoch  7996  G loss  0.08116189\n",
      "Epoch  7997  G loss  0.07538849\n",
      "Epoch  7998  G loss  0.034056045\n",
      "Epoch  7999  G loss  0.10410732\n",
      "Epoch  8000  G loss  0.039298136\n",
      "Epoch  8001  G loss  0.04345949\n",
      "Epoch  8002  G loss  0.19185428\n",
      "Epoch  8003  G loss  0.010394671\n",
      "Epoch  8004  G loss  0.018834861\n",
      "Epoch  8005  G loss  0.032881487\n",
      "Epoch  8006  G loss  0.0134138595\n",
      "Epoch  8007  G loss  0.019630523\n",
      "Epoch  8008  G loss  0.012847763\n",
      "Epoch  8009  G loss  0.32032156\n",
      "Epoch  8010  G loss  0.038210012\n",
      "Epoch  8011  G loss  0.05420902\n",
      "Epoch  8012  G loss  0.029168664\n",
      "Epoch  8013  G loss  0.011809174\n",
      "Epoch  8014  G loss  0.013388818\n",
      "Epoch  8015  G loss  0.07527137\n",
      "Epoch  8016  G loss  0.04158988\n",
      "Epoch  8017  G loss  0.026767835\n",
      "Epoch  8018  G loss  0.016577091\n",
      "Epoch  8019  G loss  0.09853623\n",
      "Epoch  8020  G loss  0.02587535\n",
      "Epoch  8021  G loss  0.043767355\n",
      "Epoch  8022  G loss  0.036078893\n",
      "Epoch  8023  G loss  0.0029979525\n",
      "Epoch  8024  G loss  0.056413345\n",
      "Epoch  8025  G loss  0.20634584\n",
      "Epoch  8026  G loss  0.018809993\n",
      "Epoch  8027  G loss  0.010515509\n",
      "Epoch  8028  G loss  0.08709982\n",
      "Epoch  8029  G loss  0.022307519\n",
      "Epoch  8030  G loss  0.059921056\n",
      "Epoch  8031  G loss  0.030360378\n",
      "Epoch  8032  G loss  0.015454571\n",
      "Epoch  8033  G loss  0.013866298\n",
      "Epoch  8034  G loss  0.013313293\n",
      "Epoch  8035  G loss  0.01276813\n",
      "Epoch  8036  G loss  0.09392221\n",
      "Epoch  8037  G loss  0.035921887\n",
      "Epoch  8038  G loss  0.2293941\n",
      "Epoch  8039  G loss  0.048756383\n",
      "Epoch  8040  G loss  0.051456936\n",
      "Epoch  8041  G loss  0.02311555\n",
      "Epoch  8042  G loss  0.014845319\n",
      "Epoch  8043  G loss  0.055713825\n",
      "Epoch  8044  G loss  0.03746914\n",
      "Epoch  8045  G loss  0.03510746\n",
      "Epoch  8046  G loss  0.0069042034\n",
      "Epoch  8047  G loss  0.013297051\n",
      "Epoch  8048  G loss  0.031918935\n",
      "Epoch  8049  G loss  0.03480258\n",
      "Epoch  8050  G loss  0.053176872\n",
      "Epoch  8051  G loss  0.052408956\n",
      "Epoch  8052  G loss  0.012849273\n",
      "Epoch  8053  G loss  0.101149544\n",
      "Epoch  8054  G loss  0.016132597\n",
      "Epoch  8055  G loss  0.008797049\n",
      "Epoch  8056  G loss  0.016525459\n",
      "Epoch  8057  G loss  0.019414116\n",
      "Epoch  8058  G loss  0.010860311\n",
      "Epoch  8059  G loss  0.019537678\n",
      "Epoch  8060  G loss  0.02739812\n",
      "Epoch  8061  G loss  0.078335494\n",
      "Epoch  8062  G loss  0.012110993\n",
      "Epoch  8063  G loss  0.02973283\n",
      "Epoch  8064  G loss  0.017548863\n",
      "Epoch  8065  G loss  0.007923787\n",
      "Epoch  8066  G loss  0.0031622662\n",
      "Epoch  8067  G loss  0.033709608\n",
      "Epoch  8068  G loss  0.0163016\n",
      "Epoch  8069  G loss  0.0117749665\n",
      "Epoch  8070  G loss  0.34340483\n",
      "Epoch  8071  G loss  0.10210539\n",
      "Epoch  8072  G loss  0.025331244\n",
      "Epoch  8073  G loss  0.0053409943\n",
      "Epoch  8074  G loss  0.06952482\n",
      "Epoch  8075  G loss  0.14570035\n",
      "Epoch  8076  G loss  0.019311875\n",
      "Epoch  8077  G loss  0.057643496\n",
      "Epoch  8078  G loss  0.008062695\n",
      "Epoch  8079  G loss  0.07004325\n",
      "Epoch  8080  G loss  0.0643705\n",
      "Epoch  8081  G loss  0.06751367\n",
      "Epoch  8082  G loss  0.022547055\n",
      "Epoch  8083  G loss  0.09907295\n",
      "Epoch  8084  G loss  0.029287186\n",
      "Epoch  8085  G loss  0.009822257\n",
      "Epoch  8086  G loss  0.029961176\n",
      "Epoch  8087  G loss  0.15664372\n",
      "Epoch  8088  G loss  0.045673035\n",
      "Epoch  8089  G loss  0.009261806\n",
      "Epoch  8090  G loss  0.04648322\n",
      "Epoch  8091  G loss  0.015018005\n",
      "Epoch  8092  G loss  0.009275537\n",
      "Epoch  8093  G loss  0.07774097\n",
      "Epoch  8094  G loss  0.21475378\n",
      "Epoch  8095  G loss  0.014292138\n",
      "Epoch  8096  G loss  0.026104365\n",
      "Epoch  8097  G loss  0.018660981\n",
      "Epoch  8098  G loss  0.031353597\n",
      "Epoch  8099  G loss  0.022499315\n",
      "Epoch  8100  G loss  0.0043100053\n",
      "Epoch  8101  G loss  0.022226622\n",
      "Epoch  8102  G loss  0.027887514\n",
      "Epoch  8103  G loss  0.037530094\n",
      "Epoch  8104  G loss  0.03392805\n",
      "Epoch  8105  G loss  0.013569789\n",
      "Epoch  8106  G loss  0.006314228\n",
      "Epoch  8107  G loss  0.060398795\n",
      "Epoch  8108  G loss  0.056930996\n",
      "Epoch  8109  G loss  0.06008357\n",
      "Epoch  8110  G loss  0.021330383\n",
      "Epoch  8111  G loss  0.01568574\n",
      "Epoch  8112  G loss  0.066804595\n",
      "Epoch  8113  G loss  0.02691494\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  8114  G loss  0.049923714\n",
      "Epoch  8115  G loss  0.1239717\n",
      "Epoch  8116  G loss  0.21643285\n",
      "Epoch  8117  G loss  0.027313627\n",
      "Epoch  8118  G loss  0.0111171575\n",
      "Epoch  8119  G loss  0.027256528\n",
      "Epoch  8120  G loss  0.006023973\n",
      "Epoch  8121  G loss  0.012656698\n",
      "Epoch  8122  G loss  0.09152144\n",
      "Epoch  8123  G loss  0.090642616\n",
      "Epoch  8124  G loss  0.05574359\n",
      "Epoch  8125  G loss  0.043730468\n",
      "Epoch  8126  G loss  0.015134902\n",
      "Epoch  8127  G loss  0.10887839\n",
      "Epoch  8128  G loss  0.028918408\n",
      "Epoch  8129  G loss  0.016382981\n",
      "Epoch  8130  G loss  0.020831704\n",
      "Epoch  8131  G loss  0.006430329\n",
      "Epoch  8132  G loss  0.008844417\n",
      "Epoch  8133  G loss  0.07250528\n",
      "Epoch  8134  G loss  0.0052889185\n",
      "Epoch  8135  G loss  0.16334571\n",
      "Epoch  8136  G loss  0.014337242\n",
      "Epoch  8137  G loss  0.07608378\n",
      "Epoch  8138  G loss  0.1413321\n",
      "Epoch  8139  G loss  0.0842934\n",
      "Epoch  8140  G loss  0.027770529\n",
      "Epoch  8141  G loss  0.019163534\n",
      "Epoch  8142  G loss  0.034654886\n",
      "Epoch  8143  G loss  0.008469616\n",
      "Epoch  8144  G loss  0.015996356\n",
      "Epoch  8145  G loss  0.013176983\n",
      "Epoch  8146  G loss  0.02101568\n",
      "Epoch  8147  G loss  0.018717268\n",
      "Epoch  8148  G loss  0.048215218\n",
      "Epoch  8149  G loss  0.0045964974\n",
      "Epoch  8150  G loss  0.07815195\n",
      "Epoch  8151  G loss  0.030081538\n",
      "Epoch  8152  G loss  0.037997052\n",
      "Epoch  8153  G loss  0.017839966\n",
      "Epoch  8154  G loss  0.03788621\n",
      "Epoch  8155  G loss  0.016588364\n",
      "Epoch  8156  G loss  0.006166257\n",
      "Epoch  8157  G loss  0.04112591\n",
      "Epoch  8158  G loss  0.09492037\n",
      "Epoch  8159  G loss  0.033085566\n",
      "Epoch  8160  G loss  0.050641682\n",
      "Epoch  8161  G loss  0.106249504\n",
      "Epoch  8162  G loss  0.006686575\n",
      "Epoch  8163  G loss  0.08828287\n",
      "Epoch  8164  G loss  0.0056821364\n",
      "Epoch  8165  G loss  0.16003865\n",
      "Epoch  8166  G loss  0.07914555\n",
      "Epoch  8167  G loss  0.10596256\n",
      "Epoch  8168  G loss  0.07127963\n",
      "Epoch  8169  G loss  0.005941393\n",
      "Epoch  8170  G loss  0.008059445\n",
      "Epoch  8171  G loss  0.016124077\n",
      "Epoch  8172  G loss  0.023652742\n",
      "Epoch  8173  G loss  0.055637382\n",
      "Epoch  8174  G loss  0.009618157\n",
      "Epoch  8175  G loss  0.007976912\n",
      "Epoch  8176  G loss  0.042607877\n",
      "Epoch  8177  G loss  0.011650752\n",
      "Epoch  8178  G loss  0.00398599\n",
      "Epoch  8179  G loss  0.028714906\n",
      "Epoch  8180  G loss  0.09514062\n",
      "Epoch  8181  G loss  0.021238754\n",
      "Epoch  8182  G loss  0.023650788\n",
      "Epoch  8183  G loss  0.010510623\n",
      "Epoch  8184  G loss  0.00511326\n",
      "Epoch  8185  G loss  0.014402658\n",
      "Epoch  8186  G loss  0.051213905\n",
      "Epoch  8187  G loss  0.08290596\n",
      "Epoch  8188  G loss  0.004834011\n",
      "Epoch  8189  G loss  0.016470134\n",
      "Epoch  8190  G loss  0.088551044\n",
      "Epoch  8191  G loss  0.19595098\n",
      "Epoch  8192  G loss  0.0118891345\n",
      "Epoch  8193  G loss  0.15188631\n",
      "Epoch  8194  G loss  0.0065651005\n",
      "Epoch  8195  G loss  0.00940107\n",
      "Epoch  8196  G loss  0.03532632\n",
      "Epoch  8197  G loss  0.019055197\n",
      "Epoch  8198  G loss  0.050834313\n",
      "Epoch  8199  G loss  0.09820491\n",
      "Epoch  8200  G loss  0.031003056\n",
      "Epoch  8201  G loss  0.0075383103\n",
      "Epoch  8202  G loss  0.021847855\n",
      "Epoch  8203  G loss  0.14409435\n",
      "Epoch  8204  G loss  0.04755545\n",
      "Epoch  8205  G loss  0.04530825\n",
      "Epoch  8206  G loss  0.015060434\n",
      "Epoch  8207  G loss  0.02545844\n",
      "Epoch  8208  G loss  0.012645557\n",
      "Epoch  8209  G loss  0.013585003\n",
      "Epoch  8210  G loss  0.027814837\n",
      "Epoch  8211  G loss  0.012169414\n",
      "Epoch  8212  G loss  0.018677687\n",
      "Epoch  8213  G loss  0.11195342\n",
      "Epoch  8214  G loss  0.00768454\n",
      "Epoch  8215  G loss  0.036829412\n",
      "Epoch  8216  G loss  0.013467255\n",
      "Epoch  8217  G loss  0.020046208\n",
      "Epoch  8218  G loss  0.014134089\n",
      "Epoch  8219  G loss  0.020721432\n",
      "Epoch  8220  G loss  0.11965285\n",
      "Epoch  8221  G loss  0.0073201787\n",
      "Epoch  8222  G loss  0.058638953\n",
      "Epoch  8223  G loss  0.020229645\n",
      "Epoch  8224  G loss  0.008314112\n",
      "Epoch  8225  G loss  0.14011191\n",
      "Epoch  8226  G loss  0.008594534\n",
      "Epoch  8227  G loss  0.025374563\n",
      "Epoch  8228  G loss  0.10947\n",
      "Epoch  8229  G loss  0.030006923\n",
      "Epoch  8230  G loss  0.041751117\n",
      "Epoch  8231  G loss  0.021869149\n",
      "Epoch  8232  G loss  0.029901806\n",
      "Epoch  8233  G loss  0.008494899\n",
      "Epoch  8234  G loss  0.042880975\n",
      "Epoch  8235  G loss  0.02608073\n",
      "Epoch  8236  G loss  0.067322925\n",
      "Epoch  8237  G loss  0.0988921\n",
      "Epoch  8238  G loss  0.07058963\n",
      "Epoch  8239  G loss  0.015248997\n",
      "Epoch  8240  G loss  0.1344865\n",
      "Epoch  8241  G loss  0.014743541\n",
      "Epoch  8242  G loss  0.13598129\n",
      "Epoch  8243  G loss  0.013071741\n",
      "Epoch  8244  G loss  0.017190725\n",
      "Epoch  8245  G loss  0.010755852\n",
      "Epoch  8246  G loss  0.042213522\n",
      "Epoch  8247  G loss  0.040255755\n",
      "Epoch  8248  G loss  0.007087059\n",
      "Epoch  8249  G loss  0.034764692\n",
      "Epoch  8250  G loss  0.020078775\n",
      "Epoch  8251  G loss  0.030046597\n",
      "Epoch  8252  G loss  0.015104268\n",
      "Epoch  8253  G loss  0.031120747\n",
      "Epoch  8254  G loss  0.0157739\n",
      "Epoch  8255  G loss  0.005776029\n",
      "Epoch  8256  G loss  0.030020028\n",
      "Epoch  8257  G loss  0.033041436\n",
      "Epoch  8258  G loss  0.02944102\n",
      "Epoch  8259  G loss  0.025258813\n",
      "Epoch  8260  G loss  0.0141458195\n",
      "Epoch  8261  G loss  0.12798235\n",
      "Epoch  8262  G loss  0.014377568\n",
      "Epoch  8263  G loss  0.14082195\n",
      "Epoch  8264  G loss  0.098752126\n",
      "Epoch  8265  G loss  0.37537438\n",
      "Epoch  8266  G loss  0.028616732\n",
      "Epoch  8267  G loss  0.03549126\n",
      "Epoch  8268  G loss  0.044586472\n",
      "Epoch  8269  G loss  0.016506074\n",
      "Epoch  8270  G loss  0.115121275\n",
      "Epoch  8271  G loss  0.04261493\n",
      "Epoch  8272  G loss  0.093459606\n",
      "Epoch  8273  G loss  0.0046998765\n",
      "Epoch  8274  G loss  0.008122045\n",
      "Epoch  8275  G loss  0.012275019\n",
      "Epoch  8276  G loss  0.07735303\n",
      "Epoch  8277  G loss  0.07523009\n",
      "Epoch  8278  G loss  0.011996059\n",
      "Epoch  8279  G loss  0.54060894\n",
      "Epoch  8280  G loss  0.20661187\n",
      "Epoch  8281  G loss  0.054215766\n",
      "Epoch  8282  G loss  0.038656667\n",
      "Epoch  8283  G loss  0.02761557\n",
      "Epoch  8284  G loss  0.017994259\n",
      "Epoch  8285  G loss  0.0069907857\n",
      "Epoch  8286  G loss  0.037972808\n",
      "Epoch  8287  G loss  0.023632482\n",
      "Epoch  8288  G loss  0.08192855\n",
      "Epoch  8289  G loss  0.008312006\n",
      "Epoch  8290  G loss  0.10776486\n",
      "Epoch  8291  G loss  0.02108235\n",
      "Epoch  8292  G loss  0.19147864\n",
      "Epoch  8293  G loss  0.007212613\n",
      "Epoch  8294  G loss  0.39301068\n",
      "Epoch  8295  G loss  0.013179974\n",
      "Epoch  8296  G loss  0.011047285\n",
      "Epoch  8297  G loss  0.075798064\n",
      "Epoch  8298  G loss  0.022151813\n",
      "Epoch  8299  G loss  0.030905318\n",
      "Epoch  8300  G loss  0.009585217\n",
      "Epoch  8301  G loss  0.019544683\n",
      "Epoch  8302  G loss  0.008804542\n",
      "Epoch  8303  G loss  0.011185877\n",
      "Epoch  8304  G loss  0.15402286\n",
      "Epoch  8305  G loss  0.01872021\n",
      "Epoch  8306  G loss  0.067546636\n",
      "Epoch  8307  G loss  0.015004462\n",
      "Epoch  8308  G loss  0.0044473214\n",
      "Epoch  8309  G loss  0.076913804\n",
      "Epoch  8310  G loss  0.053179897\n",
      "Epoch  8311  G loss  0.0288694\n",
      "Epoch  8312  G loss  0.068381295\n",
      "Epoch  8313  G loss  0.012518216\n",
      "Epoch  8314  G loss  0.008880964\n",
      "Epoch  8315  G loss  0.06975125\n",
      "Epoch  8316  G loss  0.008823508\n",
      "Epoch  8317  G loss  0.0037294177\n",
      "Epoch  8318  G loss  0.024816286\n",
      "Epoch  8319  G loss  0.034170203\n",
      "Epoch  8320  G loss  0.02417764\n",
      "Epoch  8321  G loss  0.005619165\n",
      "Epoch  8322  G loss  0.2518428\n",
      "Epoch  8323  G loss  0.013560073\n",
      "Epoch  8324  G loss  0.017014636\n",
      "Epoch  8325  G loss  0.056334525\n",
      "Epoch  8326  G loss  0.01879509\n",
      "Epoch  8327  G loss  0.05178501\n",
      "Epoch  8328  G loss  0.0025481961\n",
      "Epoch  8329  G loss  0.042019427\n",
      "Epoch  8330  G loss  0.025018964\n",
      "Epoch  8331  G loss  0.045291744\n",
      "Epoch  8332  G loss  0.035851665\n",
      "Epoch  8333  G loss  0.046170987\n",
      "Epoch  8334  G loss  0.03982061\n",
      "Epoch  8335  G loss  0.012730677\n",
      "Epoch  8336  G loss  0.04282691\n",
      "Epoch  8337  G loss  0.042151522\n",
      "Epoch  8338  G loss  0.06446196\n",
      "Epoch  8339  G loss  0.011477821\n",
      "Epoch  8340  G loss  0.050755993\n",
      "Epoch  8341  G loss  0.032171823\n",
      "Epoch  8342  G loss  0.009385816\n",
      "Epoch  8343  G loss  0.027583074\n",
      "Epoch  8344  G loss  0.02068802\n",
      "Epoch  8345  G loss  0.0070802784\n",
      "Epoch  8346  G loss  0.07760002\n",
      "Epoch  8347  G loss  0.007301833\n",
      "Epoch  8348  G loss  0.021694725\n",
      "Epoch  8349  G loss  0.13135703\n",
      "Epoch  8350  G loss  0.019722708\n",
      "Epoch  8351  G loss  0.043901533\n",
      "Epoch  8352  G loss  0.0657785\n",
      "Epoch  8353  G loss  0.039399434\n",
      "Epoch  8354  G loss  0.016498152\n",
      "Epoch  8355  G loss  0.022258233\n",
      "Epoch  8356  G loss  0.00977019\n",
      "Epoch  8357  G loss  0.019072348\n",
      "Epoch  8358  G loss  0.019025303\n",
      "Epoch  8359  G loss  0.013695218\n",
      "Epoch  8360  G loss  0.0490236\n",
      "Epoch  8361  G loss  0.041640908\n",
      "Epoch  8362  G loss  0.009714429\n",
      "Epoch  8363  G loss  0.009183339\n",
      "Epoch  8364  G loss  0.023789886\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  8365  G loss  0.012300181\n",
      "Epoch  8366  G loss  0.31963113\n",
      "Epoch  8367  G loss  0.010127225\n",
      "Epoch  8368  G loss  0.014139717\n",
      "Epoch  8369  G loss  0.0525662\n",
      "Epoch  8370  G loss  0.2052539\n",
      "Epoch  8371  G loss  0.019518157\n",
      "Epoch  8372  G loss  0.13261563\n",
      "Epoch  8373  G loss  0.008836188\n",
      "Epoch  8374  G loss  0.0130429845\n",
      "Epoch  8375  G loss  0.030751197\n",
      "Epoch  8376  G loss  0.027813291\n",
      "Epoch  8377  G loss  0.046611816\n",
      "Epoch  8378  G loss  0.048355702\n",
      "Epoch  8379  G loss  0.113937266\n",
      "Epoch  8380  G loss  0.009923635\n",
      "Epoch  8381  G loss  0.024870634\n",
      "Epoch  8382  G loss  0.052922007\n",
      "Epoch  8383  G loss  0.047576495\n",
      "Epoch  8384  G loss  0.01209851\n",
      "Epoch  8385  G loss  0.068121776\n",
      "Epoch  8386  G loss  0.059806306\n",
      "Epoch  8387  G loss  0.006122021\n",
      "Epoch  8388  G loss  0.010795288\n",
      "Epoch  8389  G loss  0.12973663\n",
      "Epoch  8390  G loss  0.027929103\n",
      "Epoch  8391  G loss  0.021969862\n",
      "Epoch  8392  G loss  0.021757932\n",
      "Epoch  8393  G loss  0.02414859\n",
      "Epoch  8394  G loss  0.027379919\n",
      "Epoch  8395  G loss  0.02252381\n",
      "Epoch  8396  G loss  0.118984535\n",
      "Epoch  8397  G loss  0.052409224\n",
      "Epoch  8398  G loss  0.017986737\n",
      "Epoch  8399  G loss  0.064516574\n",
      "Epoch  8400  G loss  0.06757586\n",
      "Epoch  8401  G loss  0.011830048\n",
      "Epoch  8402  G loss  0.032388665\n",
      "Epoch  8403  G loss  0.06063578\n",
      "Epoch  8404  G loss  0.0061707534\n",
      "Epoch  8405  G loss  0.005379281\n",
      "Epoch  8406  G loss  0.013374915\n",
      "Epoch  8407  G loss  0.039402064\n",
      "Epoch  8408  G loss  0.023666847\n",
      "Epoch  8409  G loss  0.020086156\n",
      "Epoch  8410  G loss  0.020644654\n",
      "Epoch  8411  G loss  0.0081451945\n",
      "Epoch  8412  G loss  0.09811147\n",
      "Epoch  8413  G loss  0.034075204\n",
      "Epoch  8414  G loss  0.014434453\n",
      "Epoch  8415  G loss  0.08322698\n",
      "Epoch  8416  G loss  0.00994745\n",
      "Epoch  8417  G loss  0.15950984\n",
      "Epoch  8418  G loss  0.031366084\n",
      "Epoch  8419  G loss  0.065123394\n",
      "Epoch  8420  G loss  0.011631265\n",
      "Epoch  8421  G loss  0.14773683\n",
      "Epoch  8422  G loss  0.06999913\n",
      "Epoch  8423  G loss  0.022694323\n",
      "Epoch  8424  G loss  0.020888101\n",
      "Epoch  8425  G loss  0.09739681\n",
      "Epoch  8426  G loss  0.0118660545\n",
      "Epoch  8427  G loss  0.027989\n",
      "Epoch  8428  G loss  0.59355044\n",
      "Epoch  8429  G loss  0.07356976\n",
      "Epoch  8430  G loss  0.009142374\n",
      "Epoch  8431  G loss  0.024649005\n",
      "Epoch  8432  G loss  0.08523963\n",
      "Epoch  8433  G loss  0.09204231\n",
      "Epoch  8434  G loss  0.012028686\n",
      "Epoch  8435  G loss  0.012222287\n",
      "Epoch  8436  G loss  0.042416297\n",
      "Epoch  8437  G loss  0.020811202\n",
      "Epoch  8438  G loss  0.013755146\n",
      "Epoch  8439  G loss  0.02390847\n",
      "Epoch  8440  G loss  0.020491041\n",
      "Epoch  8441  G loss  0.013447536\n",
      "Epoch  8442  G loss  0.11815764\n",
      "Epoch  8443  G loss  0.21014878\n",
      "Epoch  8444  G loss  0.060179368\n",
      "Epoch  8445  G loss  0.012714516\n",
      "Epoch  8446  G loss  0.031230671\n",
      "Epoch  8447  G loss  0.13840972\n",
      "Epoch  8448  G loss  0.067492284\n",
      "Epoch  8449  G loss  0.012595694\n",
      "Epoch  8450  G loss  0.03008946\n",
      "Epoch  8451  G loss  0.26294374\n",
      "Epoch  8452  G loss  0.016488802\n",
      "Epoch  8453  G loss  0.07101434\n",
      "Epoch  8454  G loss  0.097466744\n",
      "Epoch  8455  G loss  0.009391852\n",
      "Epoch  8456  G loss  0.09221869\n",
      "Epoch  8457  G loss  0.040801853\n",
      "Epoch  8458  G loss  0.007957907\n",
      "Epoch  8459  G loss  0.0059414105\n",
      "Epoch  8460  G loss  0.14226471\n",
      "Epoch  8461  G loss  0.03188602\n",
      "Epoch  8462  G loss  0.045160465\n",
      "Epoch  8463  G loss  0.008152704\n",
      "Epoch  8464  G loss  0.031618066\n",
      "Epoch  8465  G loss  0.09374477\n",
      "Epoch  8466  G loss  0.007731872\n",
      "Epoch  8467  G loss  0.084442385\n",
      "Epoch  8468  G loss  0.0060676895\n",
      "Epoch  8469  G loss  0.027507469\n",
      "Epoch  8470  G loss  0.02345981\n",
      "Epoch  8471  G loss  0.0051204464\n",
      "Epoch  8472  G loss  0.04707877\n",
      "Epoch  8473  G loss  0.053211167\n",
      "Epoch  8474  G loss  0.09668447\n",
      "Epoch  8475  G loss  0.116695836\n",
      "Epoch  8476  G loss  0.0063084974\n",
      "Epoch  8477  G loss  0.015556747\n",
      "Epoch  8478  G loss  0.015627641\n",
      "Epoch  8479  G loss  0.012495583\n",
      "Epoch  8480  G loss  0.03159014\n",
      "Epoch  8481  G loss  0.0059156837\n",
      "Epoch  8482  G loss  0.009808743\n",
      "Epoch  8483  G loss  0.006542843\n",
      "Epoch  8484  G loss  0.010769878\n",
      "Epoch  8485  G loss  0.021406133\n",
      "Epoch  8486  G loss  0.17401153\n",
      "Epoch  8487  G loss  0.025832368\n",
      "Epoch  8488  G loss  0.02020474\n",
      "Epoch  8489  G loss  0.06813914\n",
      "Epoch  8490  G loss  0.054961257\n",
      "Epoch  8491  G loss  0.018685523\n",
      "Epoch  8492  G loss  0.007854006\n",
      "Epoch  8493  G loss  0.008099529\n",
      "Epoch  8494  G loss  0.028280472\n",
      "Epoch  8495  G loss  0.17928222\n",
      "Epoch  8496  G loss  0.059124403\n",
      "Epoch  8497  G loss  0.0377473\n",
      "Epoch  8498  G loss  0.1406522\n",
      "Epoch  8499  G loss  0.082744986\n",
      "Epoch  8500  G loss  0.115707256\n",
      "Epoch  8501  G loss  0.21883506\n",
      "Epoch  8502  G loss  0.02088799\n",
      "Epoch  8503  G loss  0.17819077\n",
      "Epoch  8504  G loss  0.20170704\n",
      "Epoch  8505  G loss  0.081079155\n",
      "Epoch  8506  G loss  0.15342763\n",
      "Epoch  8507  G loss  0.012467386\n",
      "Epoch  8508  G loss  0.012453352\n",
      "Epoch  8509  G loss  0.090156406\n",
      "Epoch  8510  G loss  0.18730843\n",
      "Epoch  8511  G loss  0.023502233\n",
      "Epoch  8512  G loss  0.05600468\n",
      "Epoch  8513  G loss  0.05650793\n",
      "Epoch  8514  G loss  0.08841637\n",
      "Epoch  8515  G loss  0.05940686\n",
      "Epoch  8516  G loss  0.02856528\n",
      "Epoch  8517  G loss  0.10517168\n",
      "Epoch  8518  G loss  0.047347892\n",
      "Epoch  8519  G loss  0.044291105\n",
      "Epoch  8520  G loss  0.0073997173\n",
      "Epoch  8521  G loss  0.011982923\n",
      "Epoch  8522  G loss  0.007886745\n",
      "Epoch  8523  G loss  0.13361457\n",
      "Epoch  8524  G loss  0.13242596\n",
      "Epoch  8525  G loss  0.19795053\n",
      "Epoch  8526  G loss  0.012104316\n",
      "Epoch  8527  G loss  0.09864104\n",
      "Epoch  8528  G loss  0.06660421\n",
      "Epoch  8529  G loss  0.029140757\n",
      "Epoch  8530  G loss  0.011327689\n",
      "Epoch  8531  G loss  0.008259947\n",
      "Epoch  8532  G loss  0.11464001\n",
      "Epoch  8533  G loss  0.016134448\n",
      "Epoch  8534  G loss  0.1664184\n",
      "Epoch  8535  G loss  0.015057048\n",
      "Epoch  8536  G loss  0.007311423\n",
      "Epoch  8537  G loss  0.005238102\n",
      "Epoch  8538  G loss  0.025127921\n",
      "Epoch  8539  G loss  0.027199848\n",
      "Epoch  8540  G loss  0.04752744\n",
      "Epoch  8541  G loss  0.061050065\n",
      "Epoch  8542  G loss  0.019502597\n",
      "Epoch  8543  G loss  0.04164668\n",
      "Epoch  8544  G loss  0.021444406\n",
      "Epoch  8545  G loss  0.023292873\n",
      "Epoch  8546  G loss  0.015059533\n",
      "Epoch  8547  G loss  0.22725415\n",
      "Epoch  8548  G loss  0.08442791\n",
      "Epoch  8549  G loss  0.050293226\n",
      "Epoch  8550  G loss  0.12777692\n",
      "Epoch  8551  G loss  0.02134328\n",
      "Epoch  8552  G loss  0.04362926\n",
      "Epoch  8553  G loss  0.1406042\n",
      "Epoch  8554  G loss  0.015465634\n",
      "Epoch  8555  G loss  0.019060133\n",
      "Epoch  8556  G loss  0.020647153\n",
      "Epoch  8557  G loss  0.0077919858\n",
      "Epoch  8558  G loss  0.02653366\n",
      "Epoch  8559  G loss  0.10583724\n",
      "Epoch  8560  G loss  0.025742963\n",
      "Epoch  8561  G loss  0.056872256\n",
      "Epoch  8562  G loss  0.008703515\n",
      "Epoch  8563  G loss  0.03837298\n",
      "Epoch  8564  G loss  0.21026534\n",
      "Epoch  8565  G loss  0.11126295\n",
      "Epoch  8566  G loss  0.039682977\n",
      "Epoch  8567  G loss  0.07928459\n",
      "Epoch  8568  G loss  0.038412623\n",
      "Epoch  8569  G loss  0.00862053\n",
      "Epoch  8570  G loss  0.048186917\n",
      "Epoch  8571  G loss  0.006780806\n",
      "Epoch  8572  G loss  0.017964585\n",
      "Epoch  8573  G loss  0.05256895\n",
      "Epoch  8574  G loss  0.03506013\n",
      "Epoch  8575  G loss  0.23193613\n",
      "Epoch  8576  G loss  0.105520576\n",
      "Epoch  8577  G loss  0.05778413\n",
      "Epoch  8578  G loss  0.018704167\n",
      "Epoch  8579  G loss  0.08619442\n",
      "Epoch  8580  G loss  0.03460686\n",
      "Epoch  8581  G loss  0.0127807865\n",
      "Epoch  8582  G loss  0.061613634\n",
      "Epoch  8583  G loss  0.004626181\n",
      "Epoch  8584  G loss  0.05886952\n",
      "Epoch  8585  G loss  0.027445894\n",
      "Epoch  8586  G loss  0.0064917104\n",
      "Epoch  8587  G loss  0.008195126\n",
      "Epoch  8588  G loss  0.07634604\n",
      "Epoch  8589  G loss  0.017353592\n",
      "Epoch  8590  G loss  0.06470935\n",
      "Epoch  8591  G loss  0.040563535\n",
      "Epoch  8592  G loss  0.045895837\n",
      "Epoch  8593  G loss  0.043301478\n",
      "Epoch  8594  G loss  0.0072728144\n",
      "Epoch  8595  G loss  0.081922226\n",
      "Epoch  8596  G loss  0.0075231194\n",
      "Epoch  8597  G loss  0.03353928\n",
      "Epoch  8598  G loss  0.011563176\n",
      "Epoch  8599  G loss  0.031707175\n",
      "Epoch  8600  G loss  0.046773102\n",
      "Epoch  8601  G loss  0.109984025\n",
      "Epoch  8602  G loss  0.013938658\n",
      "Epoch  8603  G loss  0.026147027\n",
      "Epoch  8604  G loss  0.024064653\n",
      "Epoch  8605  G loss  0.09840989\n",
      "Epoch  8606  G loss  0.031331547\n",
      "Epoch  8607  G loss  0.010622645\n",
      "Epoch  8608  G loss  0.019390982\n",
      "Epoch  8609  G loss  0.009871838\n",
      "Epoch  8610  G loss  0.09023519\n",
      "Epoch  8611  G loss  0.010617912\n",
      "Epoch  8612  G loss  0.007313289\n",
      "Epoch  8613  G loss  0.0090505285\n",
      "Epoch  8614  G loss  0.07891071\n",
      "Epoch  8615  G loss  0.008942571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  8616  G loss  0.0147103295\n",
      "Epoch  8617  G loss  0.05848077\n",
      "Epoch  8618  G loss  0.032665268\n",
      "Epoch  8619  G loss  0.004821433\n",
      "Epoch  8620  G loss  0.0094867125\n",
      "Epoch  8621  G loss  0.0048285145\n",
      "Epoch  8622  G loss  0.019946678\n",
      "Epoch  8623  G loss  0.024303824\n",
      "Epoch  8624  G loss  0.010763185\n",
      "Epoch  8625  G loss  0.068682246\n",
      "Epoch  8626  G loss  0.25379115\n",
      "Epoch  8627  G loss  0.15299256\n",
      "Epoch  8628  G loss  0.016179027\n",
      "Epoch  8629  G loss  0.008170653\n",
      "Epoch  8630  G loss  0.021587893\n",
      "Epoch  8631  G loss  0.048362866\n",
      "Epoch  8632  G loss  0.050580338\n",
      "Epoch  8633  G loss  0.022570979\n",
      "Epoch  8634  G loss  0.0076016043\n",
      "Epoch  8635  G loss  0.021706428\n",
      "Epoch  8636  G loss  0.06838988\n",
      "Epoch  8637  G loss  0.029738342\n",
      "Epoch  8638  G loss  0.018872824\n",
      "Epoch  8639  G loss  0.047372285\n",
      "Epoch  8640  G loss  0.053155944\n",
      "Epoch  8641  G loss  0.022703087\n",
      "Epoch  8642  G loss  0.034319207\n",
      "Epoch  8643  G loss  0.034201235\n",
      "Epoch  8644  G loss  0.06938418\n",
      "Epoch  8645  G loss  0.036565468\n",
      "Epoch  8646  G loss  0.039165445\n",
      "Epoch  8647  G loss  0.057056963\n",
      "Epoch  8648  G loss  0.10708392\n",
      "Epoch  8649  G loss  0.046433177\n",
      "Epoch  8650  G loss  0.02693509\n",
      "Epoch  8651  G loss  0.036202125\n",
      "Epoch  8652  G loss  0.029627096\n",
      "Epoch  8653  G loss  0.03174526\n",
      "Epoch  8654  G loss  0.009846106\n",
      "Epoch  8655  G loss  0.08656712\n",
      "Epoch  8656  G loss  0.046915412\n",
      "Epoch  8657  G loss  0.009604674\n",
      "Epoch  8658  G loss  0.076092646\n",
      "Epoch  8659  G loss  0.03881069\n",
      "Epoch  8660  G loss  0.082308814\n",
      "Epoch  8661  G loss  0.021112079\n",
      "Epoch  8662  G loss  0.036652125\n",
      "Epoch  8663  G loss  0.0058896155\n",
      "Epoch  8664  G loss  0.112854645\n",
      "Epoch  8665  G loss  0.03057942\n",
      "Epoch  8666  G loss  0.009232089\n",
      "Epoch  8667  G loss  0.08182944\n",
      "Epoch  8668  G loss  0.024634317\n",
      "Epoch  8669  G loss  0.029456204\n",
      "Epoch  8670  G loss  0.070515096\n",
      "Epoch  8671  G loss  0.0097980425\n",
      "Epoch  8672  G loss  0.023456601\n",
      "Epoch  8673  G loss  0.010242817\n",
      "Epoch  8674  G loss  0.09663153\n",
      "Epoch  8675  G loss  0.007900448\n",
      "Epoch  8676  G loss  0.054361142\n",
      "Epoch  8677  G loss  0.023010874\n",
      "Epoch  8678  G loss  0.08279856\n",
      "Epoch  8679  G loss  0.010931125\n",
      "Epoch  8680  G loss  0.0086337\n",
      "Epoch  8681  G loss  0.01938433\n",
      "Epoch  8682  G loss  0.031969994\n",
      "Epoch  8683  G loss  0.01589043\n",
      "Epoch  8684  G loss  0.009867182\n",
      "Epoch  8685  G loss  0.041336533\n",
      "Epoch  8686  G loss  0.005613145\n",
      "Epoch  8687  G loss  0.017295398\n",
      "Epoch  8688  G loss  0.01568371\n",
      "Epoch  8689  G loss  0.120498054\n",
      "Epoch  8690  G loss  0.0080740545\n",
      "Epoch  8691  G loss  0.033437576\n",
      "Epoch  8692  G loss  0.080361046\n",
      "Epoch  8693  G loss  0.014513761\n",
      "Epoch  8694  G loss  0.027263686\n",
      "Epoch  8695  G loss  0.029214803\n",
      "Epoch  8696  G loss  0.17251498\n",
      "Epoch  8697  G loss  0.02426654\n",
      "Epoch  8698  G loss  0.13242114\n",
      "Epoch  8699  G loss  0.23616336\n",
      "Epoch  8700  G loss  0.027750157\n",
      "Epoch  8701  G loss  0.0033423284\n",
      "Epoch  8702  G loss  0.06438883\n",
      "Epoch  8703  G loss  0.035131663\n",
      "Epoch  8704  G loss  0.065816164\n",
      "Epoch  8705  G loss  0.0065939906\n",
      "Epoch  8706  G loss  0.029453266\n",
      "Epoch  8707  G loss  0.105724484\n",
      "Epoch  8708  G loss  0.012701731\n",
      "Epoch  8709  G loss  0.25279206\n",
      "Epoch  8710  G loss  0.03731901\n",
      "Epoch  8711  G loss  0.01663303\n",
      "Epoch  8712  G loss  0.017785756\n",
      "Epoch  8713  G loss  0.0029953066\n",
      "Epoch  8714  G loss  0.21978337\n",
      "Epoch  8715  G loss  0.0062788734\n",
      "Epoch  8716  G loss  0.013694684\n",
      "Epoch  8717  G loss  0.01686554\n",
      "Epoch  8718  G loss  0.0033734452\n",
      "Epoch  8719  G loss  0.007063398\n",
      "Epoch  8720  G loss  0.050358593\n",
      "Epoch  8721  G loss  0.016433276\n",
      "Epoch  8722  G loss  0.1918914\n",
      "Epoch  8723  G loss  0.0058412175\n",
      "Epoch  8724  G loss  0.054148614\n",
      "Epoch  8725  G loss  0.021174807\n",
      "Epoch  8726  G loss  0.038150124\n",
      "Epoch  8727  G loss  0.0061870394\n",
      "Epoch  8728  G loss  0.09177398\n",
      "Epoch  8729  G loss  0.019612692\n",
      "Epoch  8730  G loss  0.030132875\n",
      "Epoch  8731  G loss  0.009445241\n",
      "Epoch  8732  G loss  0.03241136\n",
      "Epoch  8733  G loss  0.03571883\n",
      "Epoch  8734  G loss  0.16755593\n",
      "Epoch  8735  G loss  0.0077202553\n",
      "Epoch  8736  G loss  0.022043467\n",
      "Epoch  8737  G loss  0.005007197\n",
      "Epoch  8738  G loss  0.035561748\n",
      "Epoch  8739  G loss  0.04279919\n",
      "Epoch  8740  G loss  0.106272876\n",
      "Epoch  8741  G loss  0.1772586\n",
      "Epoch  8742  G loss  0.014396293\n",
      "Epoch  8743  G loss  0.031146806\n",
      "Epoch  8744  G loss  0.061915606\n",
      "Epoch  8745  G loss  0.013588417\n",
      "Epoch  8746  G loss  0.005700108\n",
      "Epoch  8747  G loss  0.11595099\n",
      "Epoch  8748  G loss  0.04040117\n",
      "Epoch  8749  G loss  0.00580597\n",
      "Epoch  8750  G loss  0.020921718\n",
      "Epoch  8751  G loss  0.0134630315\n",
      "Epoch  8752  G loss  0.06052538\n",
      "Epoch  8753  G loss  0.04250836\n",
      "Epoch  8754  G loss  0.043489218\n",
      "Epoch  8755  G loss  0.20512909\n",
      "Epoch  8756  G loss  0.105497375\n",
      "Epoch  8757  G loss  0.013454813\n",
      "Epoch  8758  G loss  0.045587678\n",
      "Epoch  8759  G loss  0.0058386456\n",
      "Epoch  8760  G loss  0.018318098\n",
      "Epoch  8761  G loss  0.020391144\n",
      "Epoch  8762  G loss  0.053406443\n",
      "Epoch  8763  G loss  0.047617756\n",
      "Epoch  8764  G loss  0.02464797\n",
      "Epoch  8765  G loss  0.02133556\n",
      "Epoch  8766  G loss  0.06645945\n",
      "Epoch  8767  G loss  0.078917444\n",
      "Epoch  8768  G loss  0.01864177\n",
      "Epoch  8769  G loss  0.017740814\n",
      "Epoch  8770  G loss  0.0068891784\n",
      "Epoch  8771  G loss  0.021499187\n",
      "Epoch  8772  G loss  0.02275347\n",
      "Epoch  8773  G loss  0.0077827824\n",
      "Epoch  8774  G loss  0.0129945595\n",
      "Epoch  8775  G loss  0.04345736\n",
      "Epoch  8776  G loss  0.031395964\n",
      "Epoch  8777  G loss  0.038418427\n",
      "Epoch  8778  G loss  0.09800615\n",
      "Epoch  8779  G loss  0.013512486\n",
      "Epoch  8780  G loss  0.017422646\n",
      "Epoch  8781  G loss  0.031747274\n",
      "Epoch  8782  G loss  0.029435309\n",
      "Epoch  8783  G loss  0.019705102\n",
      "Epoch  8784  G loss  0.19222733\n",
      "Epoch  8785  G loss  0.082694486\n",
      "Epoch  8786  G loss  0.0048050424\n",
      "Epoch  8787  G loss  0.017760623\n",
      "Epoch  8788  G loss  0.011320784\n",
      "Epoch  8789  G loss  0.016212257\n",
      "Epoch  8790  G loss  0.007854337\n",
      "Epoch  8791  G loss  0.011185391\n",
      "Epoch  8792  G loss  0.06573705\n",
      "Epoch  8793  G loss  0.12854972\n",
      "Epoch  8794  G loss  0.020188643\n",
      "Epoch  8795  G loss  0.014778616\n",
      "Epoch  8796  G loss  0.017169103\n",
      "Epoch  8797  G loss  0.07351639\n",
      "Epoch  8798  G loss  0.05877362\n",
      "Epoch  8799  G loss  0.10761074\n",
      "Epoch  8800  G loss  0.013431505\n",
      "Epoch  8801  G loss  0.04122194\n",
      "Epoch  8802  G loss  0.03355758\n",
      "Epoch  8803  G loss  0.040920872\n",
      "Epoch  8804  G loss  0.007835399\n",
      "Epoch  8805  G loss  0.08905648\n",
      "Epoch  8806  G loss  0.0239989\n",
      "Epoch  8807  G loss  0.11566941\n",
      "Epoch  8808  G loss  0.14408839\n",
      "Epoch  8809  G loss  0.0187792\n",
      "Epoch  8810  G loss  0.03420455\n",
      "Epoch  8811  G loss  0.011057546\n",
      "Epoch  8812  G loss  0.0074458807\n",
      "Epoch  8813  G loss  0.007044608\n",
      "Epoch  8814  G loss  0.07995661\n",
      "Epoch  8815  G loss  0.059102118\n",
      "Epoch  8816  G loss  0.019181427\n",
      "Epoch  8817  G loss  0.014122864\n",
      "Epoch  8818  G loss  0.043944113\n",
      "Epoch  8819  G loss  0.014349928\n",
      "Epoch  8820  G loss  0.09912978\n",
      "Epoch  8821  G loss  0.05649791\n",
      "Epoch  8822  G loss  0.12842341\n",
      "Epoch  8823  G loss  0.095316246\n",
      "Epoch  8824  G loss  0.0079698\n",
      "Epoch  8825  G loss  0.039557096\n",
      "Epoch  8826  G loss  0.007633053\n",
      "Epoch  8827  G loss  0.010798281\n",
      "Epoch  8828  G loss  0.3681931\n",
      "Epoch  8829  G loss  0.013605347\n",
      "Epoch  8830  G loss  0.033090744\n",
      "Epoch  8831  G loss  0.009870428\n",
      "Epoch  8832  G loss  0.057612963\n",
      "Epoch  8833  G loss  0.027574746\n",
      "Epoch  8834  G loss  0.015082228\n",
      "Epoch  8835  G loss  0.01332222\n",
      "Epoch  8836  G loss  0.01554383\n",
      "Epoch  8837  G loss  0.012210062\n",
      "Epoch  8838  G loss  0.364285\n",
      "Epoch  8839  G loss  0.0062869242\n",
      "Epoch  8840  G loss  0.056334846\n",
      "Epoch  8841  G loss  0.17166725\n",
      "Epoch  8842  G loss  0.0899427\n",
      "Epoch  8843  G loss  0.018068345\n",
      "Epoch  8844  G loss  0.046963774\n",
      "Epoch  8845  G loss  0.009012235\n",
      "Epoch  8846  G loss  0.008467404\n",
      "Epoch  8847  G loss  0.046991017\n",
      "Epoch  8848  G loss  0.00877827\n",
      "Epoch  8849  G loss  0.055387445\n",
      "Epoch  8850  G loss  0.0650149\n",
      "Epoch  8851  G loss  0.011714317\n",
      "Epoch  8852  G loss  0.023843298\n",
      "Epoch  8853  G loss  0.08638395\n",
      "Epoch  8854  G loss  0.009020228\n",
      "Epoch  8855  G loss  0.029133495\n",
      "Epoch  8856  G loss  0.028992452\n",
      "Epoch  8857  G loss  0.027461063\n",
      "Epoch  8858  G loss  0.015311638\n",
      "Epoch  8859  G loss  0.016352598\n",
      "Epoch  8860  G loss  0.14220217\n",
      "Epoch  8861  G loss  0.096287936\n",
      "Epoch  8862  G loss  0.02706521\n",
      "Epoch  8863  G loss  0.019351158\n",
      "Epoch  8864  G loss  0.018091578\n",
      "Epoch  8865  G loss  0.015916863\n",
      "Epoch  8866  G loss  0.014481897\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  8867  G loss  0.006888942\n",
      "Epoch  8868  G loss  0.026418373\n",
      "Epoch  8869  G loss  0.07032487\n",
      "Epoch  8870  G loss  0.11433238\n",
      "Epoch  8871  G loss  0.017112983\n",
      "Epoch  8872  G loss  0.1585221\n",
      "Epoch  8873  G loss  0.031882476\n",
      "Epoch  8874  G loss  0.17382646\n",
      "Epoch  8875  G loss  0.0367798\n",
      "Epoch  8876  G loss  0.22639936\n",
      "Epoch  8877  G loss  0.00553557\n",
      "Epoch  8878  G loss  0.060144737\n",
      "Epoch  8879  G loss  0.2426562\n",
      "Epoch  8880  G loss  0.2151297\n",
      "Epoch  8881  G loss  0.16498819\n",
      "Epoch  8882  G loss  0.008717664\n",
      "Epoch  8883  G loss  0.031577475\n",
      "Epoch  8884  G loss  0.012814087\n",
      "Epoch  8885  G loss  0.012562772\n",
      "Epoch  8886  G loss  0.0065403674\n",
      "Epoch  8887  G loss  0.025006358\n",
      "Epoch  8888  G loss  0.005015043\n",
      "Epoch  8889  G loss  0.016189858\n",
      "Epoch  8890  G loss  0.10220365\n",
      "Epoch  8891  G loss  0.04676636\n",
      "Epoch  8892  G loss  0.022526525\n",
      "Epoch  8893  G loss  0.03200993\n",
      "Epoch  8894  G loss  0.024017442\n",
      "Epoch  8895  G loss  0.015270136\n",
      "Epoch  8896  G loss  0.22578\n",
      "Epoch  8897  G loss  0.09337706\n",
      "Epoch  8898  G loss  0.078879625\n",
      "Epoch  8899  G loss  0.11176064\n",
      "Epoch  8900  G loss  0.066228114\n",
      "Epoch  8901  G loss  0.06630303\n",
      "Epoch  8902  G loss  0.009920517\n",
      "Epoch  8903  G loss  0.017770344\n",
      "Epoch  8904  G loss  0.2238719\n",
      "Epoch  8905  G loss  0.0131323235\n",
      "Epoch  8906  G loss  0.032609574\n",
      "Epoch  8907  G loss  0.013225515\n",
      "Epoch  8908  G loss  0.008796441\n",
      "Epoch  8909  G loss  0.026133368\n",
      "Epoch  8910  G loss  0.020038454\n",
      "Epoch  8911  G loss  0.04634007\n",
      "Epoch  8912  G loss  0.118230805\n",
      "Epoch  8913  G loss  0.045240816\n",
      "Epoch  8914  G loss  0.029946337\n",
      "Epoch  8915  G loss  0.020901196\n",
      "Epoch  8916  G loss  0.048689865\n",
      "Epoch  8917  G loss  0.010978839\n",
      "Epoch  8918  G loss  0.02553539\n",
      "Epoch  8919  G loss  0.019464996\n",
      "Epoch  8920  G loss  0.032566287\n",
      "Epoch  8921  G loss  0.040820353\n",
      "Epoch  8922  G loss  0.017821534\n",
      "Epoch  8923  G loss  0.026300624\n",
      "Epoch  8924  G loss  0.22141051\n",
      "Epoch  8925  G loss  0.005046062\n",
      "Epoch  8926  G loss  0.028894376\n",
      "Epoch  8927  G loss  0.04172103\n",
      "Epoch  8928  G loss  0.1564474\n",
      "Epoch  8929  G loss  0.13898759\n",
      "Epoch  8930  G loss  0.005488257\n",
      "Epoch  8931  G loss  0.024240041\n",
      "Epoch  8932  G loss  0.07806106\n",
      "Epoch  8933  G loss  0.11023064\n",
      "Epoch  8934  G loss  0.06919191\n",
      "Epoch  8935  G loss  0.044583037\n",
      "Epoch  8936  G loss  0.007431916\n",
      "Epoch  8937  G loss  0.25302252\n",
      "Epoch  8938  G loss  0.037949935\n",
      "Epoch  8939  G loss  0.024117917\n",
      "Epoch  8940  G loss  0.0028974381\n",
      "Epoch  8941  G loss  0.051790476\n",
      "Epoch  8942  G loss  0.00573576\n",
      "Epoch  8943  G loss  0.19509119\n",
      "Epoch  8944  G loss  0.008502641\n",
      "Epoch  8945  G loss  0.051639747\n",
      "Epoch  8946  G loss  0.016480308\n",
      "Epoch  8947  G loss  0.036670186\n",
      "Epoch  8948  G loss  0.047774307\n",
      "Epoch  8949  G loss  0.019157786\n",
      "Epoch  8950  G loss  0.023538776\n",
      "Epoch  8951  G loss  0.12695584\n",
      "Epoch  8952  G loss  0.03959302\n",
      "Epoch  8953  G loss  0.003234366\n",
      "Epoch  8954  G loss  0.011648437\n",
      "Epoch  8955  G loss  0.0047540804\n",
      "Epoch  8956  G loss  0.15517803\n",
      "Epoch  8957  G loss  0.04025647\n",
      "Epoch  8958  G loss  0.070057474\n",
      "Epoch  8959  G loss  0.108647525\n",
      "Epoch  8960  G loss  0.05253176\n",
      "Epoch  8961  G loss  0.058580197\n",
      "Epoch  8962  G loss  0.007888554\n",
      "Epoch  8963  G loss  0.0051676063\n",
      "Epoch  8964  G loss  0.06283598\n",
      "Epoch  8965  G loss  0.11019994\n",
      "Epoch  8966  G loss  0.03187234\n",
      "Epoch  8967  G loss  0.09688228\n",
      "Epoch  8968  G loss  0.15757212\n",
      "Epoch  8969  G loss  0.0056969146\n",
      "Epoch  8970  G loss  0.06549513\n",
      "Epoch  8971  G loss  0.016292756\n",
      "Epoch  8972  G loss  0.00624355\n",
      "Epoch  8973  G loss  0.25393397\n",
      "Epoch  8974  G loss  0.04687324\n",
      "Epoch  8975  G loss  0.0194826\n",
      "Epoch  8976  G loss  0.23063883\n",
      "Epoch  8977  G loss  0.0075670597\n",
      "Epoch  8978  G loss  0.028319791\n",
      "Epoch  8979  G loss  0.023176935\n",
      "Epoch  8980  G loss  0.07733397\n",
      "Epoch  8981  G loss  0.039038867\n",
      "Epoch  8982  G loss  0.090894684\n",
      "Epoch  8983  G loss  0.03042462\n",
      "Epoch  8984  G loss  0.024355615\n",
      "Epoch  8985  G loss  0.040215053\n",
      "Epoch  8986  G loss  0.03142181\n",
      "Epoch  8987  G loss  0.21504089\n",
      "Epoch  8988  G loss  0.19911882\n",
      "Epoch  8989  G loss  0.045350887\n",
      "Epoch  8990  G loss  0.029688692\n",
      "Epoch  8991  G loss  0.12722795\n",
      "Epoch  8992  G loss  0.038898252\n",
      "Epoch  8993  G loss  0.012194956\n",
      "Epoch  8994  G loss  0.0046779634\n",
      "Epoch  8995  G loss  0.020531662\n",
      "Epoch  8996  G loss  0.03730543\n",
      "Epoch  8997  G loss  0.026176136\n",
      "Epoch  8998  G loss  0.035982694\n",
      "Epoch  8999  G loss  0.057590634\n",
      "Epoch  9000  G loss  0.058999553\n",
      "Epoch  9001  G loss  0.06483659\n",
      "Epoch  9002  G loss  0.012499151\n",
      "Epoch  9003  G loss  0.018287309\n",
      "Epoch  9004  G loss  0.20993552\n",
      "Epoch  9005  G loss  0.0823722\n",
      "Epoch  9006  G loss  0.010700143\n",
      "Epoch  9007  G loss  0.24863489\n",
      "Epoch  9008  G loss  0.00832155\n",
      "Epoch  9009  G loss  0.14004637\n",
      "Epoch  9010  G loss  0.015672455\n",
      "Epoch  9011  G loss  0.12305385\n",
      "Epoch  9012  G loss  0.0902119\n",
      "Epoch  9013  G loss  0.0052070133\n",
      "Epoch  9014  G loss  0.023067296\n",
      "Epoch  9015  G loss  0.10229729\n",
      "Epoch  9016  G loss  0.0066753663\n",
      "Epoch  9017  G loss  0.055309683\n",
      "Epoch  9018  G loss  0.13037807\n",
      "Epoch  9019  G loss  0.01716429\n",
      "Epoch  9020  G loss  0.010980217\n",
      "Epoch  9021  G loss  0.0077385157\n",
      "Epoch  9022  G loss  0.02334297\n",
      "Epoch  9023  G loss  0.02833134\n",
      "Epoch  9024  G loss  0.08678219\n",
      "Epoch  9025  G loss  0.12741095\n",
      "Epoch  9026  G loss  0.0028618583\n",
      "Epoch  9027  G loss  0.011442139\n",
      "Epoch  9028  G loss  0.064742\n",
      "Epoch  9029  G loss  0.023692414\n",
      "Epoch  9030  G loss  0.0058374726\n",
      "Epoch  9031  G loss  0.004309783\n",
      "Epoch  9032  G loss  0.004403886\n",
      "Epoch  9033  G loss  0.03898847\n",
      "Epoch  9034  G loss  0.026476257\n",
      "Epoch  9035  G loss  0.017790519\n",
      "Epoch  9036  G loss  0.03389071\n",
      "Epoch  9037  G loss  0.05681803\n",
      "Epoch  9038  G loss  0.0074526593\n",
      "Epoch  9039  G loss  0.0066500367\n",
      "Epoch  9040  G loss  0.047644608\n",
      "Epoch  9041  G loss  0.035194628\n",
      "Epoch  9042  G loss  0.010047333\n",
      "Epoch  9043  G loss  0.041155376\n",
      "Epoch  9044  G loss  0.06698493\n",
      "Epoch  9045  G loss  0.0076643294\n",
      "Epoch  9046  G loss  0.014809648\n",
      "Epoch  9047  G loss  0.023406552\n",
      "Epoch  9048  G loss  0.019793892\n",
      "Epoch  9049  G loss  0.008476883\n",
      "Epoch  9050  G loss  0.02739608\n",
      "Epoch  9051  G loss  0.013954687\n",
      "Epoch  9052  G loss  0.0049173767\n",
      "Epoch  9053  G loss  0.060883366\n",
      "Epoch  9054  G loss  0.029010113\n",
      "Epoch  9055  G loss  0.017601922\n",
      "Epoch  9056  G loss  0.060328115\n",
      "Epoch  9057  G loss  0.19997704\n",
      "Epoch  9058  G loss  0.0863644\n",
      "Epoch  9059  G loss  0.019010115\n",
      "Epoch  9060  G loss  0.010685409\n",
      "Epoch  9061  G loss  0.05219342\n",
      "Epoch  9062  G loss  0.066493794\n",
      "Epoch  9063  G loss  0.017350473\n",
      "Epoch  9064  G loss  0.016806804\n",
      "Epoch  9065  G loss  0.023330621\n",
      "Epoch  9066  G loss  0.020616356\n",
      "Epoch  9067  G loss  0.0030851616\n",
      "Epoch  9068  G loss  0.10635491\n",
      "Epoch  9069  G loss  0.015862525\n",
      "Epoch  9070  G loss  0.0057674563\n",
      "Epoch  9071  G loss  0.022540193\n",
      "Epoch  9072  G loss  0.24171525\n",
      "Epoch  9073  G loss  0.0060837194\n",
      "Epoch  9074  G loss  0.02596404\n",
      "Epoch  9075  G loss  0.0035868904\n",
      "Epoch  9076  G loss  0.034584716\n",
      "Epoch  9077  G loss  0.08705963\n",
      "Epoch  9078  G loss  0.10764405\n",
      "Epoch  9079  G loss  0.0087733995\n",
      "Epoch  9080  G loss  0.015129252\n",
      "Epoch  9081  G loss  0.029799547\n",
      "Epoch  9082  G loss  0.015757143\n",
      "Epoch  9083  G loss  0.025417823\n",
      "Epoch  9084  G loss  0.029485129\n",
      "Epoch  9085  G loss  0.018212255\n",
      "Epoch  9086  G loss  0.015650459\n",
      "Epoch  9087  G loss  0.065824404\n",
      "Epoch  9088  G loss  0.016377378\n",
      "Epoch  9089  G loss  0.018037716\n",
      "Epoch  9090  G loss  0.028508568\n",
      "Epoch  9091  G loss  0.091979474\n",
      "Epoch  9092  G loss  0.014911142\n",
      "Epoch  9093  G loss  0.030647654\n",
      "Epoch  9094  G loss  0.00586161\n",
      "Epoch  9095  G loss  0.07024746\n",
      "Epoch  9096  G loss  0.013968121\n",
      "Epoch  9097  G loss  0.037700485\n",
      "Epoch  9098  G loss  0.0044797217\n",
      "Epoch  9099  G loss  0.0108710155\n",
      "Epoch  9100  G loss  0.053639982\n",
      "Epoch  9101  G loss  0.02410042\n",
      "Epoch  9102  G loss  0.06825169\n",
      "Epoch  9103  G loss  0.009086775\n",
      "Epoch  9104  G loss  0.016367827\n",
      "Epoch  9105  G loss  0.0064946087\n",
      "Epoch  9106  G loss  0.094366096\n",
      "Epoch  9107  G loss  0.06913701\n",
      "Epoch  9108  G loss  0.041039534\n",
      "Epoch  9109  G loss  0.08934325\n",
      "Epoch  9110  G loss  0.0093848985\n",
      "Epoch  9111  G loss  0.022372216\n",
      "Epoch  9112  G loss  0.13512422\n",
      "Epoch  9113  G loss  0.013769695\n",
      "Epoch  9114  G loss  0.013535161\n",
      "Epoch  9115  G loss  0.029966038\n",
      "Epoch  9116  G loss  0.01609686\n",
      "Epoch  9117  G loss  0.021265287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  9118  G loss  0.007548049\n",
      "Epoch  9119  G loss  0.014236419\n",
      "Epoch  9120  G loss  0.03196144\n",
      "Epoch  9121  G loss  0.019258682\n",
      "Epoch  9122  G loss  0.031102844\n",
      "Epoch  9123  G loss  0.012883933\n",
      "Epoch  9124  G loss  0.040439762\n",
      "Epoch  9125  G loss  0.084615186\n",
      "Epoch  9126  G loss  0.013878004\n",
      "Epoch  9127  G loss  0.061493844\n",
      "Epoch  9128  G loss  0.049472295\n",
      "Epoch  9129  G loss  0.014623499\n",
      "Epoch  9130  G loss  0.17440915\n",
      "Epoch  9131  G loss  0.011954222\n",
      "Epoch  9132  G loss  0.009431703\n",
      "Epoch  9133  G loss  0.052195318\n",
      "Epoch  9134  G loss  0.008489778\n",
      "Epoch  9135  G loss  0.08268459\n",
      "Epoch  9136  G loss  0.009877913\n",
      "Epoch  9137  G loss  0.01630994\n",
      "Epoch  9138  G loss  0.049839236\n",
      "Epoch  9139  G loss  0.027029052\n",
      "Epoch  9140  G loss  0.025240133\n",
      "Epoch  9141  G loss  0.016486282\n",
      "Epoch  9142  G loss  0.047891635\n",
      "Epoch  9143  G loss  0.04246571\n",
      "Epoch  9144  G loss  0.053373784\n",
      "Epoch  9145  G loss  0.106749676\n",
      "Epoch  9146  G loss  0.0063475072\n",
      "Epoch  9147  G loss  0.06082189\n",
      "Epoch  9148  G loss  0.055728257\n",
      "Epoch  9149  G loss  0.1077214\n",
      "Epoch  9150  G loss  0.03499248\n",
      "Epoch  9151  G loss  0.0055890237\n",
      "Epoch  9152  G loss  0.01649989\n",
      "Epoch  9153  G loss  0.01820308\n",
      "Epoch  9154  G loss  0.25839937\n",
      "Epoch  9155  G loss  0.04799205\n",
      "Epoch  9156  G loss  0.010209286\n",
      "Epoch  9157  G loss  0.00801203\n",
      "Epoch  9158  G loss  0.040018804\n",
      "Epoch  9159  G loss  0.15885875\n",
      "Epoch  9160  G loss  0.10926529\n",
      "Epoch  9161  G loss  0.04731004\n",
      "Epoch  9162  G loss  0.00522892\n",
      "Epoch  9163  G loss  0.082067475\n",
      "Epoch  9164  G loss  0.012977327\n",
      "Epoch  9165  G loss  0.0129676275\n",
      "Epoch  9166  G loss  0.014902702\n",
      "Epoch  9167  G loss  0.06815054\n",
      "Epoch  9168  G loss  0.007950589\n",
      "Epoch  9169  G loss  0.060674593\n",
      "Epoch  9170  G loss  0.0039290274\n",
      "Epoch  9171  G loss  0.013870224\n",
      "Epoch  9172  G loss  0.019594453\n",
      "Epoch  9173  G loss  0.059246127\n",
      "Epoch  9174  G loss  0.020636842\n",
      "Epoch  9175  G loss  0.0058648675\n",
      "Epoch  9176  G loss  0.011028684\n",
      "Epoch  9177  G loss  0.022097027\n",
      "Epoch  9178  G loss  0.016829964\n",
      "Epoch  9179  G loss  0.038835473\n",
      "Epoch  9180  G loss  0.0147015285\n",
      "Epoch  9181  G loss  0.22101207\n",
      "Epoch  9182  G loss  0.12517214\n",
      "Epoch  9183  G loss  0.03938672\n",
      "Epoch  9184  G loss  0.014725236\n",
      "Epoch  9185  G loss  0.14283638\n",
      "Epoch  9186  G loss  0.05599496\n",
      "Epoch  9187  G loss  0.035196833\n",
      "Epoch  9188  G loss  0.008641385\n",
      "Epoch  9189  G loss  0.014490173\n",
      "Epoch  9190  G loss  0.026930992\n",
      "Epoch  9191  G loss  0.021399574\n",
      "Epoch  9192  G loss  0.11514659\n",
      "Epoch  9193  G loss  0.031922963\n",
      "Epoch  9194  G loss  0.017928544\n",
      "Epoch  9195  G loss  0.11714591\n",
      "Epoch  9196  G loss  0.0122278165\n",
      "Epoch  9197  G loss  0.030821241\n",
      "Epoch  9198  G loss  0.014810046\n",
      "Epoch  9199  G loss  0.06569342\n",
      "Epoch  9200  G loss  0.012724404\n",
      "Epoch  9201  G loss  0.06191869\n",
      "Epoch  9202  G loss  0.0046693673\n",
      "Epoch  9203  G loss  0.012781434\n",
      "Epoch  9204  G loss  0.06458974\n",
      "Epoch  9205  G loss  0.13080755\n",
      "Epoch  9206  G loss  0.0070574414\n",
      "Epoch  9207  G loss  0.010671522\n",
      "Epoch  9208  G loss  0.022372391\n",
      "Epoch  9209  G loss  0.063993536\n",
      "Epoch  9210  G loss  0.17659439\n",
      "Epoch  9211  G loss  0.017723858\n",
      "Epoch  9212  G loss  0.032374345\n",
      "Epoch  9213  G loss  0.116497636\n",
      "Epoch  9214  G loss  0.0061858087\n",
      "Epoch  9215  G loss  0.10537501\n",
      "Epoch  9216  G loss  0.10875511\n",
      "Epoch  9217  G loss  0.10894547\n",
      "Epoch  9218  G loss  0.07359277\n",
      "Epoch  9219  G loss  0.011798624\n",
      "Epoch  9220  G loss  0.013452484\n",
      "Epoch  9221  G loss  0.012143001\n",
      "Epoch  9222  G loss  0.016264075\n",
      "Epoch  9223  G loss  0.102691874\n",
      "Epoch  9224  G loss  0.020792432\n",
      "Epoch  9225  G loss  0.061219472\n",
      "Epoch  9226  G loss  0.009734488\n",
      "Epoch  9227  G loss  0.10243343\n",
      "Epoch  9228  G loss  0.016435113\n",
      "Epoch  9229  G loss  0.07494974\n",
      "Epoch  9230  G loss  0.03627774\n",
      "Epoch  9231  G loss  0.025884446\n",
      "Epoch  9232  G loss  0.045061897\n",
      "Epoch  9233  G loss  0.038274027\n",
      "Epoch  9234  G loss  0.113050535\n",
      "Epoch  9235  G loss  0.035635144\n",
      "Epoch  9236  G loss  0.059837844\n",
      "Epoch  9237  G loss  0.04351089\n",
      "Epoch  9238  G loss  0.010213875\n",
      "Epoch  9239  G loss  0.03313737\n",
      "Epoch  9240  G loss  0.005385549\n",
      "Epoch  9241  G loss  0.11992706\n",
      "Epoch  9242  G loss  0.1326659\n",
      "Epoch  9243  G loss  0.09144509\n",
      "Epoch  9244  G loss  0.01354336\n",
      "Epoch  9245  G loss  0.025263024\n",
      "Epoch  9246  G loss  0.008600909\n",
      "Epoch  9247  G loss  0.037982896\n",
      "Epoch  9248  G loss  0.034651067\n",
      "Epoch  9249  G loss  0.025695842\n",
      "Epoch  9250  G loss  0.07149914\n",
      "Epoch  9251  G loss  0.02008846\n",
      "Epoch  9252  G loss  0.19597162\n",
      "Epoch  9253  G loss  0.038442116\n",
      "Epoch  9254  G loss  0.031913046\n",
      "Epoch  9255  G loss  0.07742673\n",
      "Epoch  9256  G loss  0.052198984\n",
      "Epoch  9257  G loss  0.010809444\n",
      "Epoch  9258  G loss  0.029801073\n",
      "Epoch  9259  G loss  0.01860226\n",
      "Epoch  9260  G loss  0.06386031\n",
      "Epoch  9261  G loss  0.039083317\n",
      "Epoch  9262  G loss  0.035436556\n",
      "Epoch  9263  G loss  0.1639036\n",
      "Epoch  9264  G loss  0.011341466\n",
      "Epoch  9265  G loss  0.013228076\n",
      "Epoch  9266  G loss  0.011690922\n",
      "Epoch  9267  G loss  0.34001952\n",
      "Epoch  9268  G loss  0.016169678\n",
      "Epoch  9269  G loss  0.015476932\n",
      "Epoch  9270  G loss  0.014530601\n",
      "Epoch  9271  G loss  0.05965653\n",
      "Epoch  9272  G loss  0.024309982\n",
      "Epoch  9273  G loss  0.04019165\n",
      "Epoch  9274  G loss  0.18995148\n",
      "Epoch  9275  G loss  0.006779409\n",
      "Epoch  9276  G loss  0.08964592\n",
      "Epoch  9277  G loss  0.18065414\n",
      "Epoch  9278  G loss  0.13714327\n",
      "Epoch  9279  G loss  0.12101428\n",
      "Epoch  9280  G loss  0.038745478\n",
      "Epoch  9281  G loss  0.026112359\n",
      "Epoch  9282  G loss  0.014528887\n",
      "Epoch  9283  G loss  0.013067346\n",
      "Epoch  9284  G loss  0.03611772\n",
      "Epoch  9285  G loss  0.041693233\n",
      "Epoch  9286  G loss  0.011062957\n",
      "Epoch  9287  G loss  0.046007305\n",
      "Epoch  9288  G loss  0.012919479\n",
      "Epoch  9289  G loss  0.03532746\n",
      "Epoch  9290  G loss  0.06881336\n",
      "Epoch  9291  G loss  0.008482331\n",
      "Epoch  9292  G loss  0.022039985\n",
      "Epoch  9293  G loss  0.0045032287\n",
      "Epoch  9294  G loss  0.006226953\n",
      "Epoch  9295  G loss  0.041327134\n",
      "Epoch  9296  G loss  0.042881005\n",
      "Epoch  9297  G loss  0.03443958\n",
      "Epoch  9298  G loss  0.1055691\n",
      "Epoch  9299  G loss  0.03847321\n",
      "Epoch  9300  G loss  0.01566808\n",
      "Epoch  9301  G loss  0.024547905\n",
      "Epoch  9302  G loss  0.0034017204\n",
      "Epoch  9303  G loss  0.24925002\n",
      "Epoch  9304  G loss  0.13034934\n",
      "Epoch  9305  G loss  0.049299143\n",
      "Epoch  9306  G loss  0.013125767\n",
      "Epoch  9307  G loss  0.021148883\n",
      "Epoch  9308  G loss  0.0331892\n",
      "Epoch  9309  G loss  0.030653182\n",
      "Epoch  9310  G loss  0.032249577\n",
      "Epoch  9311  G loss  0.022976268\n",
      "Epoch  9312  G loss  0.08921854\n",
      "Epoch  9313  G loss  0.04566288\n",
      "Epoch  9314  G loss  0.034557663\n",
      "Epoch  9315  G loss  0.045104276\n",
      "Epoch  9316  G loss  0.025779737\n",
      "Epoch  9317  G loss  0.014256663\n",
      "Epoch  9318  G loss  0.0597187\n",
      "Epoch  9319  G loss  0.0046692723\n",
      "Epoch  9320  G loss  0.011544806\n",
      "Epoch  9321  G loss  0.06373157\n",
      "Epoch  9322  G loss  0.018619236\n",
      "Epoch  9323  G loss  0.035830736\n",
      "Epoch  9324  G loss  0.0040145908\n",
      "Epoch  9325  G loss  0.0074635698\n",
      "Epoch  9326  G loss  0.043880377\n",
      "Epoch  9327  G loss  0.07572228\n",
      "Epoch  9328  G loss  0.053888805\n",
      "Epoch  9329  G loss  0.014905866\n",
      "Epoch  9330  G loss  0.007011346\n",
      "Epoch  9331  G loss  0.011761816\n",
      "Epoch  9332  G loss  0.03366154\n",
      "Epoch  9333  G loss  0.012089369\n",
      "Epoch  9334  G loss  0.043851264\n",
      "Epoch  9335  G loss  0.0028502103\n",
      "Epoch  9336  G loss  0.006228927\n",
      "Epoch  9337  G loss  0.007859053\n",
      "Epoch  9338  G loss  0.0153348865\n",
      "Epoch  9339  G loss  0.07518334\n",
      "Epoch  9340  G loss  0.29393148\n",
      "Epoch  9341  G loss  0.003448355\n",
      "Epoch  9342  G loss  0.02347774\n",
      "Epoch  9343  G loss  0.09503101\n",
      "Epoch  9344  G loss  0.015003074\n",
      "Epoch  9345  G loss  0.011659278\n",
      "Epoch  9346  G loss  0.0115231285\n",
      "Epoch  9347  G loss  0.083645955\n",
      "Epoch  9348  G loss  0.036863483\n",
      "Epoch  9349  G loss  0.019298539\n",
      "Epoch  9350  G loss  0.012654627\n",
      "Epoch  9351  G loss  0.012309827\n",
      "Epoch  9352  G loss  0.0569439\n",
      "Epoch  9353  G loss  0.040217243\n",
      "Epoch  9354  G loss  0.026779678\n",
      "Epoch  9355  G loss  0.03623659\n",
      "Epoch  9356  G loss  0.042002976\n",
      "Epoch  9357  G loss  0.082277484\n",
      "Epoch  9358  G loss  0.061675392\n",
      "Epoch  9359  G loss  0.031647183\n",
      "Epoch  9360  G loss  0.16642818\n",
      "Epoch  9361  G loss  0.42044792\n",
      "Epoch  9362  G loss  0.015529592\n",
      "Epoch  9363  G loss  0.0058191544\n",
      "Epoch  9364  G loss  0.020462006\n",
      "Epoch  9365  G loss  0.035968028\n",
      "Epoch  9366  G loss  0.050183628\n",
      "Epoch  9367  G loss  0.047817893\n",
      "Epoch  9368  G loss  0.011792467\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  9369  G loss  0.0701506\n",
      "Epoch  9370  G loss  0.05944281\n",
      "Epoch  9371  G loss  0.12453189\n",
      "Epoch  9372  G loss  0.047630433\n",
      "Epoch  9373  G loss  0.012115506\n",
      "Epoch  9374  G loss  0.077161126\n",
      "Epoch  9375  G loss  0.052001\n",
      "Epoch  9376  G loss  0.028881658\n",
      "Epoch  9377  G loss  0.0093404595\n",
      "Epoch  9378  G loss  0.008246479\n",
      "Epoch  9379  G loss  0.00709223\n",
      "Epoch  9380  G loss  0.16074279\n",
      "Epoch  9381  G loss  0.055291217\n",
      "Epoch  9382  G loss  0.14764142\n",
      "Epoch  9383  G loss  0.07618806\n",
      "Epoch  9384  G loss  0.025155602\n",
      "Epoch  9385  G loss  0.026345775\n",
      "Epoch  9386  G loss  0.062338352\n",
      "Epoch  9387  G loss  0.089383185\n",
      "Epoch  9388  G loss  0.0061980514\n",
      "Epoch  9389  G loss  0.04471876\n",
      "Epoch  9390  G loss  0.09921909\n",
      "Epoch  9391  G loss  0.023605175\n",
      "Epoch  9392  G loss  0.025601264\n",
      "Epoch  9393  G loss  0.022800101\n",
      "Epoch  9394  G loss  0.010207021\n",
      "Epoch  9395  G loss  0.021670423\n",
      "Epoch  9396  G loss  0.08893083\n",
      "Epoch  9397  G loss  0.030239588\n",
      "Epoch  9398  G loss  0.029550195\n",
      "Epoch  9399  G loss  0.07113017\n",
      "Epoch  9400  G loss  0.061933435\n",
      "Epoch  9401  G loss  0.0073494986\n",
      "Epoch  9402  G loss  0.028894423\n",
      "Epoch  9403  G loss  0.038362615\n",
      "Epoch  9404  G loss  0.0492447\n",
      "Epoch  9405  G loss  0.009280188\n",
      "Epoch  9406  G loss  0.022657856\n",
      "Epoch  9407  G loss  0.015874654\n",
      "Epoch  9408  G loss  0.006550217\n",
      "Epoch  9409  G loss  0.061583064\n",
      "Epoch  9410  G loss  0.1449481\n",
      "Epoch  9411  G loss  0.043853946\n",
      "Epoch  9412  G loss  0.010821291\n",
      "Epoch  9413  G loss  0.011284709\n",
      "Epoch  9414  G loss  0.039873786\n",
      "Epoch  9415  G loss  0.020656973\n",
      "Epoch  9416  G loss  0.06854047\n",
      "Epoch  9417  G loss  0.0256126\n",
      "Epoch  9418  G loss  0.009570805\n",
      "Epoch  9419  G loss  0.008522861\n",
      "Epoch  9420  G loss  0.0767194\n",
      "Epoch  9421  G loss  0.050971813\n",
      "Epoch  9422  G loss  0.0370633\n",
      "Epoch  9423  G loss  0.015619971\n",
      "Epoch  9424  G loss  0.027358696\n",
      "Epoch  9425  G loss  0.2488197\n",
      "Epoch  9426  G loss  0.008099925\n",
      "Epoch  9427  G loss  0.017362025\n",
      "Epoch  9428  G loss  0.009640904\n",
      "Epoch  9429  G loss  0.039241087\n",
      "Epoch  9430  G loss  0.10810545\n",
      "Epoch  9431  G loss  0.011439504\n",
      "Epoch  9432  G loss  0.026013019\n",
      "Epoch  9433  G loss  0.10664265\n",
      "Epoch  9434  G loss  0.014868043\n",
      "Epoch  9435  G loss  0.04142626\n",
      "Epoch  9436  G loss  0.017964382\n",
      "Epoch  9437  G loss  0.026699018\n",
      "Epoch  9438  G loss  0.03010291\n",
      "Epoch  9439  G loss  0.014765019\n",
      "Epoch  9440  G loss  0.17468113\n",
      "Epoch  9441  G loss  0.03588817\n",
      "Epoch  9442  G loss  0.026776921\n",
      "Epoch  9443  G loss  0.016553454\n",
      "Epoch  9444  G loss  0.011933162\n",
      "Epoch  9445  G loss  0.12970027\n",
      "Epoch  9446  G loss  0.02751078\n",
      "Epoch  9447  G loss  0.03688108\n",
      "Epoch  9448  G loss  0.02661766\n",
      "Epoch  9449  G loss  0.023346443\n",
      "Epoch  9450  G loss  0.023909869\n",
      "Epoch  9451  G loss  0.004870075\n",
      "Epoch  9452  G loss  0.010062307\n",
      "Epoch  9453  G loss  0.011555316\n",
      "Epoch  9454  G loss  0.051384274\n",
      "Epoch  9455  G loss  0.0434227\n",
      "Epoch  9456  G loss  0.009669117\n",
      "Epoch  9457  G loss  0.0850158\n",
      "Epoch  9458  G loss  0.010688232\n",
      "Epoch  9459  G loss  0.018041704\n",
      "Epoch  9460  G loss  0.041589968\n",
      "Epoch  9461  G loss  0.008006384\n",
      "Epoch  9462  G loss  0.048867315\n",
      "Epoch  9463  G loss  0.06018079\n",
      "Epoch  9464  G loss  0.0499406\n",
      "Epoch  9465  G loss  0.31038544\n",
      "Epoch  9466  G loss  0.17981192\n",
      "Epoch  9467  G loss  0.044715542\n",
      "Epoch  9468  G loss  0.0058868323\n",
      "Epoch  9469  G loss  0.017519966\n",
      "Epoch  9470  G loss  0.043183997\n",
      "Epoch  9471  G loss  0.010645338\n",
      "Epoch  9472  G loss  0.044062153\n",
      "Epoch  9473  G loss  0.11405434\n",
      "Epoch  9474  G loss  0.013484042\n",
      "Epoch  9475  G loss  0.1908921\n",
      "Epoch  9476  G loss  0.10379171\n",
      "Epoch  9477  G loss  0.016483389\n",
      "Epoch  9478  G loss  0.03172939\n",
      "Epoch  9479  G loss  0.008603923\n",
      "Epoch  9480  G loss  0.06977896\n",
      "Epoch  9481  G loss  0.007136235\n",
      "Epoch  9482  G loss  0.02288875\n",
      "Epoch  9483  G loss  0.012728589\n",
      "Epoch  9484  G loss  0.1250528\n",
      "Epoch  9485  G loss  0.010201262\n",
      "Epoch  9486  G loss  0.20654747\n",
      "Epoch  9487  G loss  0.028129164\n",
      "Epoch  9488  G loss  0.0552285\n",
      "Epoch  9489  G loss  0.0602445\n",
      "Epoch  9490  G loss  0.07954331\n",
      "Epoch  9491  G loss  0.042455383\n",
      "Epoch  9492  G loss  0.009933559\n",
      "Epoch  9493  G loss  0.011058453\n",
      "Epoch  9494  G loss  0.022257686\n",
      "Epoch  9495  G loss  0.012783493\n",
      "Epoch  9496  G loss  0.12034393\n",
      "Epoch  9497  G loss  0.0071572284\n",
      "Epoch  9498  G loss  0.013758869\n",
      "Epoch  9499  G loss  0.038881406\n",
      "Epoch  9500  G loss  0.094120994\n",
      "Epoch  9501  G loss  0.023017978\n",
      "Epoch  9502  G loss  0.011706065\n",
      "Epoch  9503  G loss  0.12670575\n",
      "Epoch  9504  G loss  0.015335003\n",
      "Epoch  9505  G loss  0.04086432\n",
      "Epoch  9506  G loss  0.040094707\n",
      "Epoch  9507  G loss  0.1123968\n",
      "Epoch  9508  G loss  0.012195494\n",
      "Epoch  9509  G loss  0.041304898\n",
      "Epoch  9510  G loss  0.007096802\n",
      "Epoch  9511  G loss  0.04324048\n",
      "Epoch  9512  G loss  0.018853668\n",
      "Epoch  9513  G loss  0.051056135\n",
      "Epoch  9514  G loss  0.008129695\n",
      "Epoch  9515  G loss  0.0072559947\n",
      "Epoch  9516  G loss  0.015711125\n",
      "Epoch  9517  G loss  0.02962993\n",
      "Epoch  9518  G loss  0.040415417\n",
      "Epoch  9519  G loss  0.016713815\n",
      "Epoch  9520  G loss  0.021216564\n",
      "Epoch  9521  G loss  0.015567799\n",
      "Epoch  9522  G loss  0.009687819\n",
      "Epoch  9523  G loss  0.01141714\n",
      "Epoch  9524  G loss  0.107829124\n",
      "Epoch  9525  G loss  0.015732586\n",
      "Epoch  9526  G loss  0.0046755904\n",
      "Epoch  9527  G loss  0.045545973\n",
      "Epoch  9528  G loss  0.024642617\n",
      "Epoch  9529  G loss  0.010086128\n",
      "Epoch  9530  G loss  0.055693395\n",
      "Epoch  9531  G loss  0.06977661\n",
      "Epoch  9532  G loss  0.008068789\n",
      "Epoch  9533  G loss  0.060044587\n",
      "Epoch  9534  G loss  0.0053922664\n",
      "Epoch  9535  G loss  0.012613125\n",
      "Epoch  9536  G loss  0.060472324\n",
      "Epoch  9537  G loss  0.026101641\n",
      "Epoch  9538  G loss  0.03619853\n",
      "Epoch  9539  G loss  0.26270688\n",
      "Epoch  9540  G loss  0.0027846047\n",
      "Epoch  9541  G loss  0.03379534\n",
      "Epoch  9542  G loss  0.012680791\n",
      "Epoch  9543  G loss  0.0160727\n",
      "Epoch  9544  G loss  0.01824101\n",
      "Epoch  9545  G loss  0.014384784\n",
      "Epoch  9546  G loss  0.010630162\n",
      "Epoch  9547  G loss  0.036197156\n",
      "Epoch  9548  G loss  0.03403213\n",
      "Epoch  9549  G loss  0.02110978\n",
      "Epoch  9550  G loss  0.025640946\n",
      "Epoch  9551  G loss  0.03170921\n",
      "Epoch  9552  G loss  0.011825931\n",
      "Epoch  9553  G loss  0.121368825\n",
      "Epoch  9554  G loss  0.012556057\n",
      "Epoch  9555  G loss  0.17816427\n",
      "Epoch  9556  G loss  0.007543929\n",
      "Epoch  9557  G loss  0.010055112\n",
      "Epoch  9558  G loss  0.016882868\n",
      "Epoch  9559  G loss  0.07018137\n",
      "Epoch  9560  G loss  0.05420307\n",
      "Epoch  9561  G loss  0.06708689\n",
      "Epoch  9562  G loss  0.021148521\n",
      "Epoch  9563  G loss  0.024970092\n",
      "Epoch  9564  G loss  0.05755696\n",
      "Epoch  9565  G loss  0.010233333\n",
      "Epoch  9566  G loss  0.009484959\n",
      "Epoch  9567  G loss  0.0047246073\n",
      "Epoch  9568  G loss  0.2324201\n",
      "Epoch  9569  G loss  0.020538025\n",
      "Epoch  9570  G loss  0.09757399\n",
      "Epoch  9571  G loss  0.20182452\n",
      "Epoch  9572  G loss  0.008100595\n",
      "Epoch  9573  G loss  0.051177125\n",
      "Epoch  9574  G loss  0.0041360245\n",
      "Epoch  9575  G loss  0.03793296\n",
      "Epoch  9576  G loss  0.011476439\n",
      "Epoch  9577  G loss  0.044046886\n",
      "Epoch  9578  G loss  0.01775095\n",
      "Epoch  9579  G loss  0.0030180425\n",
      "Epoch  9580  G loss  0.01500622\n",
      "Epoch  9581  G loss  0.00393188\n",
      "Epoch  9582  G loss  0.054638594\n",
      "Epoch  9583  G loss  0.018197425\n",
      "Epoch  9584  G loss  0.021878019\n",
      "Epoch  9585  G loss  0.061718408\n",
      "Epoch  9586  G loss  0.034988012\n",
      "Epoch  9587  G loss  0.028413732\n",
      "Epoch  9588  G loss  0.011201588\n",
      "Epoch  9589  G loss  0.007476626\n",
      "Epoch  9590  G loss  0.027655667\n",
      "Epoch  9591  G loss  0.046130575\n",
      "Epoch  9592  G loss  0.14100063\n",
      "Epoch  9593  G loss  0.034032755\n",
      "Epoch  9594  G loss  0.018666137\n",
      "Epoch  9595  G loss  0.037070848\n",
      "Epoch  9596  G loss  0.010247668\n",
      "Epoch  9597  G loss  0.011234399\n",
      "Epoch  9598  G loss  0.049641464\n",
      "Epoch  9599  G loss  0.12314875\n",
      "Epoch  9600  G loss  0.046534717\n",
      "Epoch  9601  G loss  0.007637306\n",
      "Epoch  9602  G loss  0.025144897\n",
      "Epoch  9603  G loss  0.042356852\n",
      "Epoch  9604  G loss  0.23200059\n",
      "Epoch  9605  G loss  0.013013516\n",
      "Epoch  9606  G loss  0.020610627\n",
      "Epoch  9607  G loss  0.053836517\n",
      "Epoch  9608  G loss  0.01101421\n",
      "Epoch  9609  G loss  0.05757358\n",
      "Epoch  9610  G loss  0.03837579\n",
      "Epoch  9611  G loss  0.16006845\n",
      "Epoch  9612  G loss  0.015063504\n",
      "Epoch  9613  G loss  0.1414214\n",
      "Epoch  9614  G loss  0.03868447\n",
      "Epoch  9615  G loss  0.25054348\n",
      "Epoch  9616  G loss  0.047449283\n",
      "Epoch  9617  G loss  0.006973024\n",
      "Epoch  9618  G loss  0.00944813\n",
      "Epoch  9619  G loss  0.02800041\n",
      "Epoch  9620  G loss  0.034776922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  9621  G loss  0.029227357\n",
      "Epoch  9622  G loss  0.015022758\n",
      "Epoch  9623  G loss  0.08690153\n",
      "Epoch  9624  G loss  0.15692972\n",
      "Epoch  9625  G loss  0.009855836\n",
      "Epoch  9626  G loss  0.010197313\n",
      "Epoch  9627  G loss  0.17753026\n",
      "Epoch  9628  G loss  0.013322193\n",
      "Epoch  9629  G loss  0.044744976\n",
      "Epoch  9630  G loss  0.027460633\n",
      "Epoch  9631  G loss  0.068621725\n",
      "Epoch  9632  G loss  0.012539263\n",
      "Epoch  9633  G loss  0.011782568\n",
      "Epoch  9634  G loss  0.11582312\n",
      "Epoch  9635  G loss  0.015143448\n",
      "Epoch  9636  G loss  0.007221083\n",
      "Epoch  9637  G loss  0.02071014\n",
      "Epoch  9638  G loss  0.0072303438\n",
      "Epoch  9639  G loss  0.023998592\n",
      "Epoch  9640  G loss  0.024727106\n",
      "Epoch  9641  G loss  0.014992366\n",
      "Epoch  9642  G loss  0.012305187\n",
      "Epoch  9643  G loss  0.06994657\n",
      "Epoch  9644  G loss  0.008848036\n",
      "Epoch  9645  G loss  0.029864205\n",
      "Epoch  9646  G loss  0.013002658\n",
      "Epoch  9647  G loss  0.013209534\n",
      "Epoch  9648  G loss  0.042431436\n",
      "Epoch  9649  G loss  0.05335044\n",
      "Epoch  9650  G loss  0.07326555\n",
      "Epoch  9651  G loss  0.06942037\n",
      "Epoch  9652  G loss  0.021111336\n",
      "Epoch  9653  G loss  0.015980922\n",
      "Epoch  9654  G loss  0.057461753\n",
      "Epoch  9655  G loss  0.013043936\n",
      "Epoch  9656  G loss  0.007404445\n",
      "Epoch  9657  G loss  0.022490684\n",
      "Epoch  9658  G loss  0.16314518\n",
      "Epoch  9659  G loss  0.013945242\n",
      "Epoch  9660  G loss  0.0305916\n",
      "Epoch  9661  G loss  0.022010572\n",
      "Epoch  9662  G loss  0.038236097\n",
      "Epoch  9663  G loss  0.010272434\n",
      "Epoch  9664  G loss  0.112954214\n",
      "Epoch  9665  G loss  0.0127225965\n",
      "Epoch  9666  G loss  0.0055424944\n",
      "Epoch  9667  G loss  0.043475386\n",
      "Epoch  9668  G loss  0.24266982\n",
      "Epoch  9669  G loss  0.013254855\n",
      "Epoch  9670  G loss  0.0069269943\n",
      "Epoch  9671  G loss  0.12829342\n",
      "Epoch  9672  G loss  0.022876851\n",
      "Epoch  9673  G loss  0.014273881\n",
      "Epoch  9674  G loss  0.056009483\n",
      "Epoch  9675  G loss  0.0422154\n",
      "Epoch  9676  G loss  0.015031479\n",
      "Epoch  9677  G loss  0.081928864\n",
      "Epoch  9678  G loss  0.01204294\n",
      "Epoch  9679  G loss  0.037646722\n",
      "Epoch  9680  G loss  0.07145834\n",
      "Epoch  9681  G loss  0.017065588\n",
      "Epoch  9682  G loss  0.0023954208\n",
      "Epoch  9683  G loss  0.014505383\n",
      "Epoch  9684  G loss  0.046520095\n",
      "Epoch  9685  G loss  0.00456662\n",
      "Epoch  9686  G loss  0.035977863\n",
      "Epoch  9687  G loss  0.14971438\n",
      "Epoch  9688  G loss  0.057241566\n",
      "Epoch  9689  G loss  0.01064085\n",
      "Epoch  9690  G loss  0.038005043\n",
      "Epoch  9691  G loss  0.03468887\n",
      "Epoch  9692  G loss  0.018840205\n",
      "Epoch  9693  G loss  0.015786981\n",
      "Epoch  9694  G loss  0.07548848\n",
      "Epoch  9695  G loss  0.09286168\n",
      "Epoch  9696  G loss  0.018057836\n",
      "Epoch  9697  G loss  0.020757195\n",
      "Epoch  9698  G loss  0.008892352\n",
      "Epoch  9699  G loss  0.04452013\n",
      "Epoch  9700  G loss  0.020859165\n",
      "Epoch  9701  G loss  0.009213828\n",
      "Epoch  9702  G loss  0.03382279\n",
      "Epoch  9703  G loss  0.087844916\n",
      "Epoch  9704  G loss  0.20510471\n",
      "Epoch  9705  G loss  0.043756224\n",
      "Epoch  9706  G loss  0.06981592\n",
      "Epoch  9707  G loss  0.039976053\n",
      "Epoch  9708  G loss  0.038853526\n",
      "Epoch  9709  G loss  0.01979315\n",
      "Epoch  9710  G loss  0.059692286\n",
      "Epoch  9711  G loss  0.36931673\n",
      "Epoch  9712  G loss  0.030368727\n",
      "Epoch  9713  G loss  0.0076531237\n",
      "Epoch  9714  G loss  0.03280228\n",
      "Epoch  9715  G loss  0.02296606\n",
      "Epoch  9716  G loss  0.14960133\n",
      "Epoch  9717  G loss  0.10296144\n",
      "Epoch  9718  G loss  0.020544544\n",
      "Epoch  9719  G loss  0.018614192\n",
      "Epoch  9720  G loss  0.058496624\n",
      "Epoch  9721  G loss  0.0941781\n",
      "Epoch  9722  G loss  0.03395358\n",
      "Epoch  9723  G loss  0.009570165\n",
      "Epoch  9724  G loss  0.07233851\n",
      "Epoch  9725  G loss  0.027433995\n",
      "Epoch  9726  G loss  0.021567002\n",
      "Epoch  9727  G loss  0.055469662\n",
      "Epoch  9728  G loss  0.06837124\n",
      "Epoch  9729  G loss  0.046368163\n",
      "Epoch  9730  G loss  0.028272647\n",
      "Epoch  9731  G loss  0.17568026\n",
      "Epoch  9732  G loss  0.018681759\n",
      "Epoch  9733  G loss  0.09517532\n",
      "Epoch  9734  G loss  0.02607896\n",
      "Epoch  9735  G loss  0.070592925\n",
      "Epoch  9736  G loss  0.015524823\n",
      "Epoch  9737  G loss  0.03457853\n",
      "Epoch  9738  G loss  0.019058688\n",
      "Epoch  9739  G loss  0.024217233\n",
      "Epoch  9740  G loss  0.004074003\n",
      "Epoch  9741  G loss  0.06655316\n",
      "Epoch  9742  G loss  0.06608233\n",
      "Epoch  9743  G loss  0.037737694\n",
      "Epoch  9744  G loss  0.0055572074\n",
      "Epoch  9745  G loss  0.007827588\n",
      "Epoch  9746  G loss  0.042077836\n",
      "Epoch  9747  G loss  0.010809107\n",
      "Epoch  9748  G loss  0.17755535\n",
      "Epoch  9749  G loss  0.08953989\n",
      "Epoch  9750  G loss  0.07826987\n",
      "Epoch  9751  G loss  0.091910765\n",
      "Epoch  9752  G loss  0.025384277\n",
      "Epoch  9753  G loss  0.016911427\n",
      "Epoch  9754  G loss  0.050694622\n",
      "Epoch  9755  G loss  0.038824968\n",
      "Epoch  9756  G loss  0.039653677\n",
      "Epoch  9757  G loss  0.04942599\n",
      "Epoch  9758  G loss  0.026276212\n",
      "Epoch  9759  G loss  0.029110849\n",
      "Epoch  9760  G loss  0.1340423\n",
      "Epoch  9761  G loss  0.048968732\n",
      "Epoch  9762  G loss  0.043721396\n",
      "Epoch  9763  G loss  0.04680865\n",
      "Epoch  9764  G loss  0.08012378\n",
      "Epoch  9765  G loss  0.05907211\n",
      "Epoch  9766  G loss  0.17163531\n",
      "Epoch  9767  G loss  0.23516397\n",
      "Epoch  9768  G loss  0.05338569\n",
      "Epoch  9769  G loss  0.1325004\n",
      "Epoch  9770  G loss  0.03946586\n",
      "Epoch  9771  G loss  0.012604899\n",
      "Epoch  9772  G loss  0.015793419\n",
      "Epoch  9773  G loss  0.029847832\n",
      "Epoch  9774  G loss  0.040018715\n",
      "Epoch  9775  G loss  0.025366805\n",
      "Epoch  9776  G loss  0.024187203\n",
      "Epoch  9777  G loss  0.025754094\n",
      "Epoch  9778  G loss  0.018831246\n",
      "Epoch  9779  G loss  0.04026097\n",
      "Epoch  9780  G loss  0.060597524\n",
      "Epoch  9781  G loss  0.007928951\n",
      "Epoch  9782  G loss  0.011899544\n",
      "Epoch  9783  G loss  0.020916093\n",
      "Epoch  9784  G loss  0.007688842\n",
      "Epoch  9785  G loss  0.039811943\n",
      "Epoch  9786  G loss  0.004798956\n",
      "Epoch  9787  G loss  0.011617534\n",
      "Epoch  9788  G loss  0.007879704\n",
      "Epoch  9789  G loss  0.041638948\n",
      "Epoch  9790  G loss  0.0726821\n",
      "Epoch  9791  G loss  0.04805454\n",
      "Epoch  9792  G loss  0.059580192\n",
      "Epoch  9793  G loss  0.0058609215\n",
      "Epoch  9794  G loss  0.017818078\n",
      "Epoch  9795  G loss  0.015707526\n",
      "Epoch  9796  G loss  0.0043554017\n",
      "Epoch  9797  G loss  0.012214211\n",
      "Epoch  9798  G loss  0.0133748595\n",
      "Epoch  9799  G loss  0.015412487\n",
      "Epoch  9800  G loss  0.056648098\n",
      "Epoch  9801  G loss  0.059671406\n",
      "Epoch  9802  G loss  0.0071157264\n",
      "Epoch  9803  G loss  0.024835875\n",
      "Epoch  9804  G loss  0.026146695\n",
      "Epoch  9805  G loss  0.059134617\n",
      "Epoch  9806  G loss  0.036258835\n",
      "Epoch  9807  G loss  0.12540406\n",
      "Epoch  9808  G loss  0.037625577\n",
      "Epoch  9809  G loss  0.072186634\n",
      "Epoch  9810  G loss  0.03970384\n",
      "Epoch  9811  G loss  0.02091242\n",
      "Epoch  9812  G loss  0.02724478\n",
      "Epoch  9813  G loss  0.06152283\n",
      "Epoch  9814  G loss  0.14276904\n",
      "Epoch  9815  G loss  0.011526827\n",
      "Epoch  9816  G loss  0.033040665\n",
      "Epoch  9817  G loss  0.0039050533\n",
      "Epoch  9818  G loss  0.020547997\n",
      "Epoch  9819  G loss  0.07361393\n",
      "Epoch  9820  G loss  0.02394579\n",
      "Epoch  9821  G loss  0.05728931\n",
      "Epoch  9822  G loss  0.031997275\n",
      "Epoch  9823  G loss  0.009787813\n",
      "Epoch  9824  G loss  0.017239308\n",
      "Epoch  9825  G loss  0.051361803\n",
      "Epoch  9826  G loss  0.05553472\n",
      "Epoch  9827  G loss  0.0064942385\n",
      "Epoch  9828  G loss  0.018207617\n",
      "Epoch  9829  G loss  0.018590065\n",
      "Epoch  9830  G loss  0.047009476\n",
      "Epoch  9831  G loss  0.011968685\n",
      "Epoch  9832  G loss  0.01696242\n",
      "Epoch  9833  G loss  0.04815849\n",
      "Epoch  9834  G loss  0.022097345\n",
      "Epoch  9835  G loss  0.064092346\n",
      "Epoch  9836  G loss  0.036867805\n",
      "Epoch  9837  G loss  0.055238105\n",
      "Epoch  9838  G loss  0.014316918\n",
      "Epoch  9839  G loss  0.11583008\n",
      "Epoch  9840  G loss  0.0368623\n",
      "Epoch  9841  G loss  0.17792332\n",
      "Epoch  9842  G loss  0.010892693\n",
      "Epoch  9843  G loss  0.09854107\n",
      "Epoch  9844  G loss  0.14661631\n",
      "Epoch  9845  G loss  0.073802985\n",
      "Epoch  9846  G loss  0.10580793\n",
      "Epoch  9847  G loss  0.0495539\n",
      "Epoch  9848  G loss  0.039067432\n",
      "Epoch  9849  G loss  0.008912629\n",
      "Epoch  9850  G loss  0.005506006\n",
      "Epoch  9851  G loss  0.010590465\n",
      "Epoch  9852  G loss  0.16163792\n",
      "Epoch  9853  G loss  0.12662324\n",
      "Epoch  9854  G loss  0.070698544\n",
      "Epoch  9855  G loss  0.03877413\n",
      "Epoch  9856  G loss  0.012692273\n",
      "Epoch  9857  G loss  0.04689981\n",
      "Epoch  9858  G loss  0.03322392\n",
      "Epoch  9859  G loss  0.27585465\n",
      "Epoch  9860  G loss  0.073379725\n",
      "Epoch  9861  G loss  0.0054435628\n",
      "Epoch  9862  G loss  0.08747799\n",
      "Epoch  9863  G loss  0.031837706\n",
      "Epoch  9864  G loss  0.009315123\n",
      "Epoch  9865  G loss  0.016090546\n",
      "Epoch  9866  G loss  0.006385172\n",
      "Epoch  9867  G loss  0.011863907\n",
      "Epoch  9868  G loss  0.020586427\n",
      "Epoch  9869  G loss  0.0379041\n",
      "Epoch  9870  G loss  0.45855755\n",
      "Epoch  9871  G loss  0.15855919\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  9872  G loss  0.009771364\n",
      "Epoch  9873  G loss  0.013910702\n",
      "Epoch  9874  G loss  0.046693735\n",
      "Epoch  9875  G loss  0.011938858\n",
      "Epoch  9876  G loss  0.009624537\n",
      "Epoch  9877  G loss  0.031167865\n",
      "Epoch  9878  G loss  0.01721834\n",
      "Epoch  9879  G loss  0.0910888\n",
      "Epoch  9880  G loss  0.18121204\n",
      "Epoch  9881  G loss  0.016553639\n",
      "Epoch  9882  G loss  0.017829485\n",
      "Epoch  9883  G loss  0.030639289\n",
      "Epoch  9884  G loss  0.021282129\n",
      "Epoch  9885  G loss  0.02893061\n",
      "Epoch  9886  G loss  0.01322764\n",
      "Epoch  9887  G loss  0.36265165\n",
      "Epoch  9888  G loss  0.035977367\n",
      "Epoch  9889  G loss  0.036555137\n",
      "Epoch  9890  G loss  0.13932744\n",
      "Epoch  9891  G loss  0.050227553\n",
      "Epoch  9892  G loss  0.021543976\n",
      "Epoch  9893  G loss  0.0054211468\n",
      "Epoch  9894  G loss  0.01591539\n",
      "Epoch  9895  G loss  0.19280854\n",
      "Epoch  9896  G loss  0.074735984\n",
      "Epoch  9897  G loss  0.008516696\n",
      "Epoch  9898  G loss  0.14720064\n",
      "Epoch  9899  G loss  0.010845161\n",
      "Epoch  9900  G loss  0.022091296\n",
      "Epoch  9901  G loss  0.011961824\n",
      "Epoch  9902  G loss  0.010854857\n",
      "Epoch  9903  G loss  0.059769582\n",
      "Epoch  9904  G loss  0.21761829\n",
      "Epoch  9905  G loss  0.021943677\n",
      "Epoch  9906  G loss  0.011454921\n",
      "Epoch  9907  G loss  0.060826696\n",
      "Epoch  9908  G loss  0.01946196\n",
      "Epoch  9909  G loss  0.020259209\n",
      "Epoch  9910  G loss  0.015970645\n",
      "Epoch  9911  G loss  0.01772729\n",
      "Epoch  9912  G loss  0.048498496\n",
      "Epoch  9913  G loss  0.17418598\n",
      "Epoch  9914  G loss  0.03224171\n",
      "Epoch  9915  G loss  0.012239665\n",
      "Epoch  9916  G loss  0.06904968\n",
      "Epoch  9917  G loss  0.021641832\n",
      "Epoch  9918  G loss  0.009946617\n",
      "Epoch  9919  G loss  0.07016429\n",
      "Epoch  9920  G loss  0.006276423\n",
      "Epoch  9921  G loss  0.18140486\n",
      "Epoch  9922  G loss  0.03406242\n",
      "Epoch  9923  G loss  0.07124767\n",
      "Epoch  9924  G loss  0.024432115\n",
      "Epoch  9925  G loss  0.017973388\n",
      "Epoch  9926  G loss  0.018827515\n",
      "Epoch  9927  G loss  0.023068547\n",
      "Epoch  9928  G loss  0.0066466383\n",
      "Epoch  9929  G loss  0.039410483\n",
      "Epoch  9930  G loss  0.018905373\n",
      "Epoch  9931  G loss  0.021629203\n",
      "Epoch  9932  G loss  0.041865587\n",
      "Epoch  9933  G loss  0.010710441\n",
      "Epoch  9934  G loss  0.034287322\n",
      "Epoch  9935  G loss  0.12642688\n",
      "Epoch  9936  G loss  0.02400487\n",
      "Epoch  9937  G loss  0.00942534\n",
      "Epoch  9938  G loss  0.06427017\n",
      "Epoch  9939  G loss  0.007946154\n",
      "Epoch  9940  G loss  0.017511357\n",
      "Epoch  9941  G loss  0.061876073\n",
      "Epoch  9942  G loss  0.08508921\n",
      "Epoch  9943  G loss  0.05902383\n",
      "Epoch  9944  G loss  0.009697299\n",
      "Epoch  9945  G loss  0.06651532\n",
      "Epoch  9946  G loss  0.022277929\n",
      "Epoch  9947  G loss  0.009705311\n",
      "Epoch  9948  G loss  0.025554221\n",
      "Epoch  9949  G loss  0.01032923\n",
      "Epoch  9950  G loss  0.01232947\n",
      "Epoch  9951  G loss  0.022157684\n",
      "Epoch  9952  G loss  0.09680237\n",
      "Epoch  9953  G loss  0.007925862\n",
      "Epoch  9954  G loss  0.016769087\n",
      "Epoch  9955  G loss  0.03609443\n",
      "Epoch  9956  G loss  0.06521806\n",
      "Epoch  9957  G loss  0.01931408\n",
      "Epoch  9958  G loss  0.20586847\n",
      "Epoch  9959  G loss  0.05629216\n",
      "Epoch  9960  G loss  0.0071448805\n",
      "Epoch  9961  G loss  0.029943788\n",
      "Epoch  9962  G loss  0.017272718\n",
      "Epoch  9963  G loss  0.30425173\n",
      "Epoch  9964  G loss  0.015494308\n",
      "Epoch  9965  G loss  0.036948662\n",
      "Epoch  9966  G loss  0.026996238\n",
      "Epoch  9967  G loss  0.013309132\n",
      "Epoch  9968  G loss  0.014344783\n",
      "Epoch  9969  G loss  0.031995118\n",
      "Epoch  9970  G loss  0.008638299\n",
      "Epoch  9971  G loss  0.12083821\n",
      "Epoch  9972  G loss  0.016265202\n",
      "Epoch  9973  G loss  0.020832438\n",
      "Epoch  9974  G loss  0.012351669\n",
      "Epoch  9975  G loss  0.04624837\n",
      "Epoch  9976  G loss  0.011220207\n",
      "Epoch  9977  G loss  0.023289789\n",
      "Epoch  9978  G loss  0.015519384\n",
      "Epoch  9979  G loss  0.011414258\n",
      "Epoch  9980  G loss  0.14702874\n",
      "Epoch  9981  G loss  0.02539146\n",
      "Epoch  9982  G loss  0.009974273\n",
      "Epoch  9983  G loss  0.07361883\n",
      "Epoch  9984  G loss  0.11073242\n",
      "Epoch  9985  G loss  0.034921385\n",
      "Epoch  9986  G loss  0.03343004\n",
      "Epoch  9987  G loss  0.007397066\n",
      "Epoch  9988  G loss  0.06868601\n",
      "Epoch  9989  G loss  0.031430714\n",
      "Epoch  9990  G loss  0.0751105\n",
      "Epoch  9991  G loss  0.0938952\n",
      "Epoch  9992  G loss  0.012882187\n",
      "Epoch  9993  G loss  0.095579684\n",
      "Epoch  9994  G loss  0.0068021975\n",
      "Epoch  9995  G loss  0.21631049\n",
      "Epoch  9996  G loss  0.24971002\n",
      "Epoch  9997  G loss  0.06273752\n",
      "Epoch  9998  G loss  0.07283658\n",
      "Epoch  9999  G loss  0.04580125\n",
      "Epoch  10000  G loss  0.008726177\n",
      "Epoch  10001  G loss  0.07810314\n",
      "Epoch  10002  G loss  0.049215555\n",
      "Epoch  10003  G loss  0.019853607\n",
      "Epoch  10004  G loss  0.013667472\n",
      "Epoch  10005  G loss  0.023590257\n",
      "Epoch  10006  G loss  0.116459996\n",
      "Epoch  10007  G loss  0.08123335\n",
      "Epoch  10008  G loss  0.026234377\n",
      "Epoch  10009  G loss  0.07193531\n",
      "Epoch  10010  G loss  0.009941613\n",
      "Epoch  10011  G loss  0.011620101\n",
      "Epoch  10012  G loss  0.015854206\n",
      "Epoch  10013  G loss  0.018874884\n",
      "Epoch  10014  G loss  0.040860623\n",
      "Epoch  10015  G loss  0.16385281\n",
      "Epoch  10016  G loss  0.12929703\n",
      "Epoch  10017  G loss  0.040313587\n",
      "Epoch  10018  G loss  0.008763533\n",
      "Epoch  10019  G loss  0.014938331\n",
      "Epoch  10020  G loss  0.017601527\n",
      "Epoch  10021  G loss  0.033388346\n",
      "Epoch  10022  G loss  0.057675153\n",
      "Epoch  10023  G loss  0.03212521\n",
      "Epoch  10024  G loss  0.008873234\n",
      "Epoch  10025  G loss  0.03167612\n",
      "Epoch  10026  G loss  0.0055890093\n",
      "Epoch  10027  G loss  0.014271841\n",
      "Epoch  10028  G loss  0.023052718\n",
      "Epoch  10029  G loss  0.04011953\n",
      "Epoch  10030  G loss  0.0774934\n",
      "Epoch  10031  G loss  0.15302007\n",
      "Epoch  10032  G loss  0.018038638\n",
      "Epoch  10033  G loss  0.018494312\n",
      "Epoch  10034  G loss  0.123352975\n",
      "Epoch  10035  G loss  0.0060621467\n",
      "Epoch  10036  G loss  0.036684975\n",
      "Epoch  10037  G loss  0.03303717\n",
      "Epoch  10038  G loss  0.010454742\n",
      "Epoch  10039  G loss  0.009457016\n",
      "Epoch  10040  G loss  0.03337373\n",
      "Epoch  10041  G loss  0.038738906\n",
      "Epoch  10042  G loss  0.006915882\n",
      "Epoch  10043  G loss  0.08445748\n",
      "Epoch  10044  G loss  0.009338681\n",
      "Epoch  10045  G loss  0.017127482\n",
      "Epoch  10046  G loss  0.013286375\n",
      "Epoch  10047  G loss  0.008287415\n",
      "Epoch  10048  G loss  0.22858322\n",
      "Epoch  10049  G loss  0.029822411\n",
      "Epoch  10050  G loss  0.019910935\n",
      "Epoch  10051  G loss  0.018165424\n",
      "Epoch  10052  G loss  0.012306848\n",
      "Epoch  10053  G loss  0.014988038\n",
      "Epoch  10054  G loss  0.06800608\n",
      "Epoch  10055  G loss  0.034633067\n",
      "Epoch  10056  G loss  0.013305584\n",
      "Epoch  10057  G loss  0.044581536\n",
      "Epoch  10058  G loss  0.04167328\n",
      "Epoch  10059  G loss  0.009977972\n",
      "Epoch  10060  G loss  0.059942797\n",
      "Epoch  10061  G loss  0.020730358\n",
      "Epoch  10062  G loss  0.068944216\n",
      "Epoch  10063  G loss  0.24239573\n",
      "Epoch  10064  G loss  0.07925436\n",
      "Epoch  10065  G loss  0.030410016\n",
      "Epoch  10066  G loss  0.064703725\n",
      "Epoch  10067  G loss  0.036133006\n",
      "Epoch  10068  G loss  0.059821248\n",
      "Epoch  10069  G loss  0.0734507\n",
      "Epoch  10070  G loss  0.080129646\n",
      "Epoch  10071  G loss  0.009925491\n",
      "Epoch  10072  G loss  0.0783139\n",
      "Epoch  10073  G loss  0.026691368\n",
      "Epoch  10074  G loss  0.16772005\n",
      "Epoch  10075  G loss  0.071285665\n",
      "Epoch  10076  G loss  0.012148697\n",
      "Epoch  10077  G loss  0.014412899\n",
      "Epoch  10078  G loss  0.00755458\n",
      "Epoch  10079  G loss  0.047700875\n",
      "Epoch  10080  G loss  0.01091487\n",
      "Epoch  10081  G loss  0.015592456\n",
      "Epoch  10082  G loss  0.109294355\n",
      "Epoch  10083  G loss  0.36328602\n",
      "Epoch  10084  G loss  0.018771585\n",
      "Epoch  10085  G loss  0.010155361\n",
      "Epoch  10086  G loss  0.009630361\n",
      "Epoch  10087  G loss  0.036489457\n",
      "Epoch  10088  G loss  0.014223627\n",
      "Epoch  10089  G loss  0.008883241\n",
      "Epoch  10090  G loss  0.30512148\n",
      "Epoch  10091  G loss  0.11213659\n",
      "Epoch  10092  G loss  0.050651956\n",
      "Epoch  10093  G loss  0.03789009\n",
      "Epoch  10094  G loss  0.01682432\n",
      "Epoch  10095  G loss  0.1491349\n",
      "Epoch  10096  G loss  0.05234659\n",
      "Epoch  10097  G loss  0.033699993\n",
      "Epoch  10098  G loss  0.046000242\n",
      "Epoch  10099  G loss  0.012510602\n",
      "Epoch  10100  G loss  0.035278432\n",
      "Epoch  10101  G loss  0.013559594\n",
      "Epoch  10102  G loss  0.03313073\n",
      "Epoch  10103  G loss  0.05203912\n",
      "Epoch  10104  G loss  0.0062727714\n",
      "Epoch  10105  G loss  0.020534724\n",
      "Epoch  10106  G loss  0.009537086\n",
      "Epoch  10107  G loss  0.017085623\n",
      "Epoch  10108  G loss  0.1314671\n",
      "Epoch  10109  G loss  0.1387056\n",
      "Epoch  10110  G loss  0.01799386\n",
      "Epoch  10111  G loss  0.01330871\n",
      "Epoch  10112  G loss  0.1352157\n",
      "Epoch  10113  G loss  0.04307549\n",
      "Epoch  10114  G loss  0.012213093\n",
      "Epoch  10115  G loss  0.10691449\n",
      "Epoch  10116  G loss  0.1303393\n",
      "Epoch  10117  G loss  0.016080033\n",
      "Epoch  10118  G loss  0.10096066\n",
      "Epoch  10119  G loss  0.010561725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  10120  G loss  0.016595129\n",
      "Epoch  10121  G loss  0.24813381\n",
      "Epoch  10122  G loss  0.022688096\n",
      "Epoch  10123  G loss  0.021247879\n",
      "Epoch  10124  G loss  0.016070008\n",
      "Epoch  10125  G loss  0.02397876\n",
      "Epoch  10126  G loss  0.040907517\n",
      "Epoch  10127  G loss  0.0152982455\n",
      "Epoch  10128  G loss  0.03272735\n",
      "Epoch  10129  G loss  0.06440553\n",
      "Epoch  10130  G loss  0.01712197\n",
      "Epoch  10131  G loss  0.068685055\n",
      "Epoch  10132  G loss  0.009550544\n",
      "Epoch  10133  G loss  0.004193281\n",
      "Epoch  10134  G loss  0.017026933\n",
      "Epoch  10135  G loss  0.030022062\n",
      "Epoch  10136  G loss  0.0048295646\n",
      "Epoch  10137  G loss  0.012508953\n",
      "Epoch  10138  G loss  0.030441638\n",
      "Epoch  10139  G loss  0.041253056\n",
      "Epoch  10140  G loss  0.09946297\n",
      "Epoch  10141  G loss  0.023672236\n",
      "Epoch  10142  G loss  0.08607414\n",
      "Epoch  10143  G loss  0.01692676\n",
      "Epoch  10144  G loss  0.14217415\n",
      "Epoch  10145  G loss  0.007328367\n",
      "Epoch  10146  G loss  0.013509458\n",
      "Epoch  10147  G loss  0.04148493\n",
      "Epoch  10148  G loss  0.04967761\n",
      "Epoch  10149  G loss  0.022962132\n",
      "Epoch  10150  G loss  0.071564354\n",
      "Epoch  10151  G loss  0.052723706\n",
      "Epoch  10152  G loss  0.006414224\n",
      "Epoch  10153  G loss  0.13759212\n",
      "Epoch  10154  G loss  0.022948101\n",
      "Epoch  10155  G loss  0.008034501\n",
      "Epoch  10156  G loss  0.003975946\n",
      "Epoch  10157  G loss  0.008095141\n",
      "Epoch  10158  G loss  0.010095728\n",
      "Epoch  10159  G loss  0.014362575\n",
      "Epoch  10160  G loss  0.091394074\n",
      "Epoch  10161  G loss  0.022896674\n",
      "Epoch  10162  G loss  0.028795563\n",
      "Epoch  10163  G loss  0.027826909\n",
      "Epoch  10164  G loss  0.01442803\n",
      "Epoch  10165  G loss  0.020783652\n",
      "Epoch  10166  G loss  0.011127861\n",
      "Epoch  10167  G loss  0.0103809815\n",
      "Epoch  10168  G loss  0.03853468\n",
      "Epoch  10169  G loss  0.13186805\n",
      "Epoch  10170  G loss  0.014882962\n",
      "Epoch  10171  G loss  0.03351462\n",
      "Epoch  10172  G loss  0.08155689\n",
      "Epoch  10173  G loss  0.13398951\n",
      "Epoch  10174  G loss  0.05898126\n",
      "Epoch  10175  G loss  0.16978785\n",
      "Epoch  10176  G loss  0.0035355105\n",
      "Epoch  10177  G loss  0.00614559\n",
      "Epoch  10178  G loss  0.07587719\n",
      "Epoch  10179  G loss  0.21113464\n",
      "Epoch  10180  G loss  0.0212261\n",
      "Epoch  10181  G loss  0.0071435897\n",
      "Epoch  10182  G loss  0.037417226\n",
      "Epoch  10183  G loss  0.012535743\n",
      "Epoch  10184  G loss  0.015418992\n",
      "Epoch  10185  G loss  0.014134938\n",
      "Epoch  10186  G loss  0.027970988\n",
      "Epoch  10187  G loss  0.012200326\n",
      "Epoch  10188  G loss  0.10546608\n",
      "Epoch  10189  G loss  0.15132098\n",
      "Epoch  10190  G loss  0.006251123\n",
      "Epoch  10191  G loss  0.08506197\n",
      "Epoch  10192  G loss  0.09295103\n",
      "Epoch  10193  G loss  0.05915398\n",
      "Epoch  10194  G loss  0.009035932\n",
      "Epoch  10195  G loss  0.17583351\n",
      "Epoch  10196  G loss  0.018338125\n",
      "Epoch  10197  G loss  0.018199245\n",
      "Epoch  10198  G loss  0.021561489\n",
      "Epoch  10199  G loss  0.0053026765\n",
      "Epoch  10200  G loss  0.059214693\n",
      "Epoch  10201  G loss  0.0127460435\n",
      "Epoch  10202  G loss  0.020117931\n",
      "Epoch  10203  G loss  0.0035279589\n",
      "Epoch  10204  G loss  0.132463\n",
      "Epoch  10205  G loss  0.008822545\n",
      "Epoch  10206  G loss  0.018142045\n",
      "Epoch  10207  G loss  0.042837456\n",
      "Epoch  10208  G loss  0.004546438\n",
      "Epoch  10209  G loss  0.011324661\n",
      "Epoch  10210  G loss  0.010554575\n",
      "Epoch  10211  G loss  0.10153472\n",
      "Epoch  10212  G loss  0.01957854\n",
      "Epoch  10213  G loss  0.032550678\n",
      "Epoch  10214  G loss  0.02712124\n",
      "Epoch  10215  G loss  0.009209728\n",
      "Epoch  10216  G loss  0.11825444\n",
      "Epoch  10217  G loss  0.09549789\n",
      "Epoch  10218  G loss  0.011693197\n",
      "Epoch  10219  G loss  0.0058021676\n",
      "Epoch  10220  G loss  0.019118812\n",
      "Epoch  10221  G loss  0.006680671\n",
      "Epoch  10222  G loss  0.009298148\n",
      "Epoch  10223  G loss  0.01052447\n",
      "Epoch  10224  G loss  0.018296663\n",
      "Epoch  10225  G loss  0.052558538\n",
      "Epoch  10226  G loss  0.072598316\n",
      "Epoch  10227  G loss  0.028173283\n",
      "Epoch  10228  G loss  0.2477558\n",
      "Epoch  10229  G loss  0.18461174\n",
      "Epoch  10230  G loss  0.020770779\n",
      "Epoch  10231  G loss  0.05501508\n",
      "Epoch  10232  G loss  0.0047153304\n",
      "Epoch  10233  G loss  0.01248358\n",
      "Epoch  10234  G loss  0.24656883\n",
      "Epoch  10235  G loss  0.022910649\n",
      "Epoch  10236  G loss  0.10738205\n",
      "Epoch  10237  G loss  0.008906126\n",
      "Epoch  10238  G loss  0.058189988\n",
      "Epoch  10239  G loss  0.07003227\n",
      "Epoch  10240  G loss  0.01197445\n",
      "Epoch  10241  G loss  0.02178263\n",
      "Epoch  10242  G loss  0.0136705665\n",
      "Epoch  10243  G loss  0.11185418\n",
      "Epoch  10244  G loss  0.0077154413\n",
      "Epoch  10245  G loss  0.11233577\n",
      "Epoch  10246  G loss  0.050123446\n",
      "Epoch  10247  G loss  0.023842264\n",
      "Epoch  10248  G loss  0.048271146\n",
      "Epoch  10249  G loss  0.023441764\n",
      "Epoch  10250  G loss  0.060727462\n",
      "Epoch  10251  G loss  0.014896297\n",
      "Epoch  10252  G loss  0.03051533\n",
      "Epoch  10253  G loss  0.24793518\n",
      "Epoch  10254  G loss  0.023045648\n",
      "Epoch  10255  G loss  0.0068651647\n",
      "Epoch  10256  G loss  0.04922174\n",
      "Epoch  10257  G loss  0.032374118\n",
      "Epoch  10258  G loss  0.005101304\n",
      "Epoch  10259  G loss  0.032659076\n",
      "Epoch  10260  G loss  0.22544883\n",
      "Epoch  10261  G loss  0.016152006\n",
      "Epoch  10262  G loss  0.043679077\n",
      "Epoch  10263  G loss  0.012548399\n",
      "Epoch  10264  G loss  0.16045216\n",
      "Epoch  10265  G loss  0.22766554\n",
      "Epoch  10266  G loss  0.052568007\n",
      "Epoch  10267  G loss  0.0060942234\n",
      "Epoch  10268  G loss  0.044887032\n",
      "Epoch  10269  G loss  0.04715296\n",
      "Epoch  10270  G loss  0.09817658\n",
      "Epoch  10271  G loss  0.01481359\n",
      "Epoch  10272  G loss  0.040581066\n",
      "Epoch  10273  G loss  0.04154553\n",
      "Epoch  10274  G loss  0.042606525\n",
      "Epoch  10275  G loss  0.038085192\n",
      "Epoch  10276  G loss  0.048502512\n",
      "Epoch  10277  G loss  0.23453908\n",
      "Epoch  10278  G loss  0.010859083\n",
      "Epoch  10279  G loss  0.06537445\n",
      "Epoch  10280  G loss  0.03859998\n",
      "Epoch  10281  G loss  0.023823343\n",
      "Epoch  10282  G loss  0.01977862\n",
      "Epoch  10283  G loss  0.052788027\n",
      "Epoch  10284  G loss  0.019178208\n",
      "Epoch  10285  G loss  0.1004919\n",
      "Epoch  10286  G loss  0.031898875\n",
      "Epoch  10287  G loss  0.09355028\n",
      "Epoch  10288  G loss  0.18630598\n",
      "Epoch  10289  G loss  0.08669619\n",
      "Epoch  10290  G loss  0.22352178\n",
      "Epoch  10291  G loss  0.011949281\n",
      "Epoch  10292  G loss  0.03850382\n",
      "Epoch  10293  G loss  0.043990787\n",
      "Epoch  10294  G loss  0.047104917\n",
      "Epoch  10295  G loss  0.11510832\n",
      "Epoch  10296  G loss  0.04072737\n",
      "Epoch  10297  G loss  0.043176543\n",
      "Epoch  10298  G loss  0.011256017\n",
      "Epoch  10299  G loss  0.0323522\n",
      "Epoch  10300  G loss  0.012580166\n",
      "Epoch  10301  G loss  0.11402069\n",
      "Epoch  10302  G loss  0.15642036\n",
      "Epoch  10303  G loss  0.05218099\n",
      "Epoch  10304  G loss  0.011457207\n",
      "Epoch  10305  G loss  0.10161123\n",
      "Epoch  10306  G loss  0.14364466\n",
      "Epoch  10307  G loss  0.10031264\n",
      "Epoch  10308  G loss  0.14480573\n",
      "Epoch  10309  G loss  0.11391266\n",
      "Epoch  10310  G loss  0.13259457\n",
      "Epoch  10311  G loss  0.00699621\n",
      "Epoch  10312  G loss  0.007596966\n",
      "Epoch  10313  G loss  0.041087337\n",
      "Epoch  10314  G loss  0.040301643\n",
      "Epoch  10315  G loss  0.019136362\n",
      "Epoch  10316  G loss  0.052983772\n",
      "Epoch  10317  G loss  0.016973356\n",
      "Epoch  10318  G loss  0.031106768\n",
      "Epoch  10319  G loss  0.010191265\n",
      "Epoch  10320  G loss  0.053118378\n",
      "Epoch  10321  G loss  0.070559904\n",
      "Epoch  10322  G loss  0.03317967\n",
      "Epoch  10323  G loss  0.121320285\n",
      "Epoch  10324  G loss  0.21350402\n",
      "Epoch  10325  G loss  0.12267426\n",
      "Epoch  10326  G loss  0.023185112\n",
      "Epoch  10327  G loss  0.05657927\n",
      "Epoch  10328  G loss  0.010879472\n",
      "Epoch  10329  G loss  0.018235166\n",
      "Epoch  10330  G loss  0.040325336\n",
      "Epoch  10331  G loss  0.086345315\n",
      "Epoch  10332  G loss  0.022422452\n",
      "Epoch  10333  G loss  0.028346853\n",
      "Epoch  10334  G loss  0.006888961\n",
      "Epoch  10335  G loss  0.057056263\n",
      "Epoch  10336  G loss  0.021122158\n",
      "Epoch  10337  G loss  0.018909778\n",
      "Epoch  10338  G loss  0.03206141\n",
      "Epoch  10339  G loss  0.039880674\n",
      "Epoch  10340  G loss  0.06682245\n",
      "Epoch  10341  G loss  0.012482993\n",
      "Epoch  10342  G loss  0.010234958\n",
      "Epoch  10343  G loss  0.005309621\n",
      "Epoch  10344  G loss  0.0034850885\n",
      "Epoch  10345  G loss  0.0060236054\n",
      "Epoch  10346  G loss  0.034489986\n",
      "Epoch  10347  G loss  0.027868582\n",
      "Epoch  10348  G loss  0.015677549\n",
      "Epoch  10349  G loss  0.016991206\n",
      "Epoch  10350  G loss  0.007495083\n",
      "Epoch  10351  G loss  0.017927328\n",
      "Epoch  10352  G loss  0.029811509\n",
      "Epoch  10353  G loss  0.03598679\n",
      "Epoch  10354  G loss  0.027548227\n",
      "Epoch  10355  G loss  0.10624464\n",
      "Epoch  10356  G loss  0.032673165\n",
      "Epoch  10357  G loss  0.016099684\n",
      "Epoch  10358  G loss  0.02819916\n",
      "Epoch  10359  G loss  0.0075047994\n",
      "Epoch  10360  G loss  0.046528168\n",
      "Epoch  10361  G loss  0.02138327\n",
      "Epoch  10362  G loss  0.122039914\n",
      "Epoch  10363  G loss  0.016761668\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  10364  G loss  0.08980468\n",
      "Epoch  10365  G loss  0.008300694\n",
      "Epoch  10366  G loss  0.0893303\n",
      "Epoch  10367  G loss  0.092303626\n",
      "Epoch  10368  G loss  0.026048362\n",
      "Epoch  10369  G loss  0.012603717\n",
      "Epoch  10370  G loss  0.010474922\n",
      "Epoch  10371  G loss  0.032827586\n",
      "Epoch  10372  G loss  0.1912182\n",
      "Epoch  10373  G loss  0.037445623\n",
      "Epoch  10374  G loss  0.00791299\n",
      "Epoch  10375  G loss  0.014869326\n",
      "Epoch  10376  G loss  0.01524001\n",
      "Epoch  10377  G loss  0.009886277\n",
      "Epoch  10378  G loss  0.18663071\n",
      "Epoch  10379  G loss  0.02849956\n",
      "Epoch  10380  G loss  0.06774072\n",
      "Epoch  10381  G loss  0.08729253\n",
      "Epoch  10382  G loss  0.06522769\n",
      "Epoch  10383  G loss  0.18973778\n",
      "Epoch  10384  G loss  0.0097929295\n",
      "Epoch  10385  G loss  0.026448019\n",
      "Epoch  10386  G loss  0.060375992\n",
      "Epoch  10387  G loss  0.032602537\n",
      "Epoch  10388  G loss  0.030780638\n",
      "Epoch  10389  G loss  0.024183914\n",
      "Epoch  10390  G loss  0.0109268585\n",
      "Epoch  10391  G loss  0.01441147\n",
      "Epoch  10392  G loss  0.014597268\n",
      "Epoch  10393  G loss  0.038875237\n",
      "Epoch  10394  G loss  0.18224508\n",
      "Epoch  10395  G loss  0.031793047\n",
      "Epoch  10396  G loss  0.03924044\n",
      "Epoch  10397  G loss  0.007344021\n",
      "Epoch  10398  G loss  0.028599609\n",
      "Epoch  10399  G loss  0.017094653\n",
      "Epoch  10400  G loss  0.02668428\n",
      "Epoch  10401  G loss  0.023340862\n",
      "Epoch  10402  G loss  0.014885271\n",
      "Epoch  10403  G loss  0.012477154\n",
      "Epoch  10404  G loss  0.030071717\n",
      "Epoch  10405  G loss  0.020850062\n",
      "Epoch  10406  G loss  0.09660402\n",
      "Epoch  10407  G loss  0.0665078\n",
      "Epoch  10408  G loss  0.027023155\n",
      "Epoch  10409  G loss  0.011924262\n",
      "Epoch  10410  G loss  0.06709239\n",
      "Epoch  10411  G loss  0.014764678\n",
      "Epoch  10412  G loss  0.017363157\n",
      "Epoch  10413  G loss  0.015954068\n",
      "Epoch  10414  G loss  0.0077904514\n",
      "Epoch  10415  G loss  0.01600752\n",
      "Epoch  10416  G loss  0.012130346\n",
      "Epoch  10417  G loss  0.0144445915\n",
      "Epoch  10418  G loss  0.01841491\n",
      "Epoch  10419  G loss  0.03523448\n",
      "Epoch  10420  G loss  0.012076508\n",
      "Epoch  10421  G loss  0.016338043\n",
      "Epoch  10422  G loss  0.041859917\n",
      "Epoch  10423  G loss  0.022354152\n",
      "Epoch  10424  G loss  0.04027676\n",
      "Epoch  10425  G loss  0.051396087\n",
      "Epoch  10426  G loss  0.029349234\n",
      "Epoch  10427  G loss  0.028031737\n",
      "Epoch  10428  G loss  0.06182915\n",
      "Epoch  10429  G loss  0.01608583\n",
      "Epoch  10430  G loss  0.043803528\n",
      "Epoch  10431  G loss  0.025173318\n",
      "Epoch  10432  G loss  0.019442655\n",
      "Epoch  10433  G loss  0.014363017\n",
      "Epoch  10434  G loss  0.015509108\n",
      "Epoch  10435  G loss  0.019609537\n",
      "Epoch  10436  G loss  0.06968787\n",
      "Epoch  10437  G loss  0.12028763\n",
      "Epoch  10438  G loss  0.0042782016\n",
      "Epoch  10439  G loss  0.05609624\n",
      "Epoch  10440  G loss  0.18546028\n",
      "Epoch  10441  G loss  0.095464684\n",
      "Epoch  10442  G loss  0.02684443\n",
      "Epoch  10443  G loss  0.021984592\n",
      "Epoch  10444  G loss  0.055505887\n",
      "Epoch  10445  G loss  0.06725952\n",
      "Epoch  10446  G loss  0.005710502\n",
      "Epoch  10447  G loss  0.0118735805\n",
      "Epoch  10448  G loss  0.027498785\n",
      "Epoch  10449  G loss  0.006556834\n",
      "Epoch  10450  G loss  0.0058312807\n",
      "Epoch  10451  G loss  0.03066875\n",
      "Epoch  10452  G loss  0.025374796\n",
      "Epoch  10453  G loss  0.035877626\n",
      "Epoch  10454  G loss  0.020500913\n",
      "Epoch  10455  G loss  0.032266904\n",
      "Epoch  10456  G loss  0.011453812\n",
      "Epoch  10457  G loss  0.06295854\n",
      "Epoch  10458  G loss  0.04157831\n",
      "Epoch  10459  G loss  0.053919062\n",
      "Epoch  10460  G loss  0.063955896\n",
      "Epoch  10461  G loss  0.004734558\n",
      "Epoch  10462  G loss  0.11001331\n",
      "Epoch  10463  G loss  0.01637445\n",
      "Epoch  10464  G loss  0.019185636\n",
      "Epoch  10465  G loss  0.011923259\n",
      "Epoch  10466  G loss  0.16755876\n",
      "Epoch  10467  G loss  0.115692906\n",
      "Epoch  10468  G loss  0.006215788\n",
      "Epoch  10469  G loss  0.038123284\n",
      "Epoch  10470  G loss  0.08850692\n",
      "Epoch  10471  G loss  0.08100034\n",
      "Epoch  10472  G loss  0.15971555\n",
      "Epoch  10473  G loss  0.050196096\n",
      "Epoch  10474  G loss  0.01817932\n",
      "Epoch  10475  G loss  0.026725158\n",
      "Epoch  10476  G loss  0.03968395\n",
      "Epoch  10477  G loss  0.015243351\n",
      "Epoch  10478  G loss  0.03511482\n",
      "Epoch  10479  G loss  0.00809061\n",
      "Epoch  10480  G loss  0.27154124\n",
      "Epoch  10481  G loss  0.08151139\n",
      "Epoch  10482  G loss  0.021545686\n",
      "Epoch  10483  G loss  0.029741485\n",
      "Epoch  10484  G loss  0.06034471\n",
      "Epoch  10485  G loss  0.06557868\n",
      "Epoch  10486  G loss  0.010066121\n",
      "Epoch  10487  G loss  0.022372888\n",
      "Epoch  10488  G loss  0.052517876\n",
      "Epoch  10489  G loss  0.072589055\n",
      "Epoch  10490  G loss  0.07035016\n",
      "Epoch  10491  G loss  0.01894328\n",
      "Epoch  10492  G loss  0.02995476\n",
      "Epoch  10493  G loss  0.19369215\n",
      "Epoch  10494  G loss  0.010893416\n",
      "Epoch  10495  G loss  0.016881285\n",
      "Epoch  10496  G loss  0.030559931\n",
      "Epoch  10497  G loss  0.09366355\n",
      "Epoch  10498  G loss  0.02477318\n",
      "Epoch  10499  G loss  0.039594118\n",
      "Epoch  10500  G loss  0.014350264\n",
      "Epoch  10501  G loss  0.038494535\n",
      "Epoch  10502  G loss  0.016615186\n",
      "Epoch  10503  G loss  0.038542576\n",
      "Epoch  10504  G loss  0.03142997\n",
      "Epoch  10505  G loss  0.020907238\n",
      "Epoch  10506  G loss  0.0699963\n",
      "Epoch  10507  G loss  0.05090353\n",
      "Epoch  10508  G loss  0.01016373\n",
      "Epoch  10509  G loss  0.027027916\n",
      "Epoch  10510  G loss  0.02867378\n",
      "Epoch  10511  G loss  0.02271996\n",
      "Epoch  10512  G loss  0.036584\n",
      "Epoch  10513  G loss  0.038366064\n",
      "Epoch  10514  G loss  0.007357217\n",
      "Epoch  10515  G loss  0.030951016\n",
      "Epoch  10516  G loss  0.014373759\n",
      "Epoch  10517  G loss  0.008579185\n",
      "Epoch  10518  G loss  0.06006635\n",
      "Epoch  10519  G loss  0.021502988\n",
      "Epoch  10520  G loss  0.037627064\n",
      "Epoch  10521  G loss  0.017857509\n",
      "Epoch  10522  G loss  0.022471478\n",
      "Epoch  10523  G loss  0.049992524\n",
      "Epoch  10524  G loss  0.02857843\n",
      "Epoch  10525  G loss  0.026144898\n",
      "Epoch  10526  G loss  0.039651155\n",
      "Epoch  10527  G loss  0.041689515\n",
      "Epoch  10528  G loss  0.010509626\n",
      "Epoch  10529  G loss  0.050587535\n",
      "Epoch  10530  G loss  0.0082785655\n",
      "Epoch  10531  G loss  0.03626962\n",
      "Epoch  10532  G loss  0.038219627\n",
      "Epoch  10533  G loss  0.083525784\n",
      "Epoch  10534  G loss  0.053536072\n",
      "Epoch  10535  G loss  0.043694727\n",
      "Epoch  10536  G loss  0.035559177\n",
      "Epoch  10537  G loss  0.02853477\n",
      "Epoch  10538  G loss  0.09221777\n",
      "Epoch  10539  G loss  0.013228596\n",
      "Epoch  10540  G loss  0.1729606\n",
      "Epoch  10541  G loss  0.0060442337\n",
      "Epoch  10542  G loss  0.054523483\n",
      "Epoch  10543  G loss  0.04668953\n",
      "Epoch  10544  G loss  0.06431076\n",
      "Epoch  10545  G loss  0.07249383\n",
      "Epoch  10546  G loss  0.014110212\n",
      "Epoch  10547  G loss  0.004242629\n",
      "Epoch  10548  G loss  0.02592872\n",
      "Epoch  10549  G loss  0.06998049\n",
      "Epoch  10550  G loss  0.06448513\n",
      "Epoch  10551  G loss  0.030318897\n",
      "Epoch  10552  G loss  0.04669109\n",
      "Epoch  10553  G loss  0.01626833\n",
      "Epoch  10554  G loss  0.080535285\n",
      "Epoch  10555  G loss  0.0287203\n",
      "Epoch  10556  G loss  0.007222424\n",
      "Epoch  10557  G loss  0.014339484\n",
      "Epoch  10558  G loss  0.099413976\n",
      "Epoch  10559  G loss  0.014761137\n",
      "Epoch  10560  G loss  0.037565425\n",
      "Epoch  10561  G loss  0.026689166\n",
      "Epoch  10562  G loss  0.027203105\n",
      "Epoch  10563  G loss  0.015089108\n",
      "Epoch  10564  G loss  0.027448092\n",
      "Epoch  10565  G loss  0.038291097\n",
      "Epoch  10566  G loss  0.0752427\n",
      "Epoch  10567  G loss  0.015440665\n",
      "Epoch  10568  G loss  0.032950744\n",
      "Epoch  10569  G loss  0.029330045\n",
      "Epoch  10570  G loss  0.009994974\n",
      "Epoch  10571  G loss  0.07762774\n",
      "Epoch  10572  G loss  0.032007366\n",
      "Epoch  10573  G loss  0.031445805\n",
      "Epoch  10574  G loss  0.027451482\n",
      "Epoch  10575  G loss  0.28947476\n",
      "Epoch  10576  G loss  0.006231431\n",
      "Epoch  10577  G loss  0.01099508\n",
      "Epoch  10578  G loss  0.003968819\n",
      "Epoch  10579  G loss  0.06680799\n",
      "Epoch  10580  G loss  0.018127993\n",
      "Epoch  10581  G loss  0.036902994\n",
      "Epoch  10582  G loss  0.017531762\n",
      "Epoch  10583  G loss  0.033013932\n",
      "Epoch  10584  G loss  0.022646923\n",
      "Epoch  10585  G loss  0.0267282\n",
      "Epoch  10586  G loss  0.0040724603\n",
      "Epoch  10587  G loss  0.04538338\n",
      "Epoch  10588  G loss  0.06955287\n",
      "Epoch  10589  G loss  0.02480863\n",
      "Epoch  10590  G loss  0.07035793\n",
      "Epoch  10591  G loss  0.010587904\n",
      "Epoch  10592  G loss  0.023421202\n",
      "Epoch  10593  G loss  0.008535574\n",
      "Epoch  10594  G loss  0.015102289\n",
      "Epoch  10595  G loss  0.011610122\n",
      "Epoch  10596  G loss  0.099923156\n",
      "Epoch  10597  G loss  0.009529436\n",
      "Epoch  10598  G loss  0.055581585\n",
      "Epoch  10599  G loss  0.0050359275\n",
      "Epoch  10600  G loss  0.00928426\n",
      "Epoch  10601  G loss  0.045235835\n",
      "Epoch  10602  G loss  0.10236054\n",
      "Epoch  10603  G loss  0.016909776\n",
      "Epoch  10604  G loss  0.012445668\n",
      "Epoch  10605  G loss  0.13517866\n",
      "Epoch  10606  G loss  0.012611852\n",
      "Epoch  10607  G loss  0.02263171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  10608  G loss  0.007640277\n",
      "Epoch  10609  G loss  0.011339166\n",
      "Epoch  10610  G loss  0.011015648\n",
      "Epoch  10611  G loss  0.0203433\n",
      "Epoch  10612  G loss  0.1209912\n",
      "Epoch  10613  G loss  0.04847355\n",
      "Epoch  10614  G loss  0.03606765\n",
      "Epoch  10615  G loss  0.023037016\n",
      "Epoch  10616  G loss  0.007346602\n",
      "Epoch  10617  G loss  0.0064320974\n",
      "Epoch  10618  G loss  0.015454639\n",
      "Epoch  10619  G loss  0.010106336\n",
      "Epoch  10620  G loss  0.00756933\n",
      "Epoch  10621  G loss  0.045043066\n",
      "Epoch  10622  G loss  0.053553626\n",
      "Epoch  10623  G loss  0.09367275\n",
      "Epoch  10624  G loss  0.0882702\n",
      "Epoch  10625  G loss  0.030294133\n",
      "Epoch  10626  G loss  0.006067479\n",
      "Epoch  10627  G loss  0.08804746\n",
      "Epoch  10628  G loss  0.021423161\n",
      "Epoch  10629  G loss  0.018339835\n",
      "Epoch  10630  G loss  0.031193987\n",
      "Epoch  10631  G loss  0.056809284\n",
      "Epoch  10632  G loss  0.0043546767\n",
      "Epoch  10633  G loss  0.0034945551\n",
      "Epoch  10634  G loss  0.022374608\n",
      "Epoch  10635  G loss  0.12825957\n",
      "Epoch  10636  G loss  0.025701463\n",
      "Epoch  10637  G loss  0.021827266\n",
      "Epoch  10638  G loss  0.005162423\n",
      "Epoch  10639  G loss  0.005241487\n",
      "Epoch  10640  G loss  0.052582\n",
      "Epoch  10641  G loss  0.02078237\n",
      "Epoch  10642  G loss  0.111998945\n",
      "Epoch  10643  G loss  0.105332084\n",
      "Epoch  10644  G loss  0.016283039\n",
      "Epoch  10645  G loss  0.010267869\n",
      "Epoch  10646  G loss  0.12736505\n",
      "Epoch  10647  G loss  0.040486\n",
      "Epoch  10648  G loss  0.0192067\n",
      "Epoch  10649  G loss  0.033669822\n",
      "Epoch  10650  G loss  0.062379576\n",
      "Epoch  10651  G loss  0.035540298\n",
      "Epoch  10652  G loss  0.03968606\n",
      "Epoch  10653  G loss  0.020977348\n",
      "Epoch  10654  G loss  0.06373\n",
      "Epoch  10655  G loss  0.11169143\n",
      "Epoch  10656  G loss  0.004788872\n",
      "Epoch  10657  G loss  0.009106251\n",
      "Epoch  10658  G loss  0.009537417\n",
      "Epoch  10659  G loss  0.016446294\n",
      "Epoch  10660  G loss  0.052745607\n",
      "Epoch  10661  G loss  0.014549644\n",
      "Epoch  10662  G loss  0.027808387\n",
      "Epoch  10663  G loss  0.092419274\n",
      "Epoch  10664  G loss  0.113919556\n",
      "Epoch  10665  G loss  0.048509017\n",
      "Epoch  10666  G loss  0.015827784\n",
      "Epoch  10667  G loss  0.014336394\n",
      "Epoch  10668  G loss  0.020587694\n",
      "Epoch  10669  G loss  0.034509387\n",
      "Epoch  10670  G loss  0.038729295\n",
      "Epoch  10671  G loss  0.02032407\n",
      "Epoch  10672  G loss  0.061154053\n",
      "Epoch  10673  G loss  0.051314384\n",
      "Epoch  10674  G loss  0.07934624\n",
      "Epoch  10675  G loss  0.07612165\n",
      "Epoch  10676  G loss  0.0060198996\n",
      "Epoch  10677  G loss  0.014007508\n",
      "Epoch  10678  G loss  0.01092124\n",
      "Epoch  10679  G loss  0.16748892\n",
      "Epoch  10680  G loss  0.022542058\n",
      "Epoch  10681  G loss  0.059818514\n",
      "Epoch  10682  G loss  0.007500682\n",
      "Epoch  10683  G loss  0.27172363\n",
      "Epoch  10684  G loss  0.0504148\n",
      "Epoch  10685  G loss  0.015182269\n",
      "Epoch  10686  G loss  0.025510613\n",
      "Epoch  10687  G loss  0.014834844\n",
      "Epoch  10688  G loss  0.017649338\n",
      "Epoch  10689  G loss  0.029486518\n",
      "Epoch  10690  G loss  0.024257785\n",
      "Epoch  10691  G loss  0.045466945\n",
      "Epoch  10692  G loss  0.0653731\n",
      "Epoch  10693  G loss  0.031446163\n",
      "Epoch  10694  G loss  0.0136029925\n",
      "Epoch  10695  G loss  0.09594715\n",
      "Epoch  10696  G loss  0.14515895\n",
      "Epoch  10697  G loss  0.04822036\n",
      "Epoch  10698  G loss  0.03195933\n",
      "Epoch  10699  G loss  0.14116259\n",
      "Epoch  10700  G loss  0.08182869\n",
      "Epoch  10701  G loss  0.078621134\n",
      "Epoch  10702  G loss  0.08254634\n",
      "Epoch  10703  G loss  0.06561417\n",
      "Epoch  10704  G loss  0.17823845\n",
      "Epoch  10705  G loss  0.008425395\n",
      "Epoch  10706  G loss  0.020823877\n",
      "Epoch  10707  G loss  0.037628524\n",
      "Epoch  10708  G loss  0.031101277\n",
      "Epoch  10709  G loss  0.0040084245\n",
      "Epoch  10710  G loss  0.03173711\n",
      "Epoch  10711  G loss  0.091895536\n",
      "Epoch  10712  G loss  0.061348274\n",
      "Epoch  10713  G loss  0.11492072\n",
      "Epoch  10714  G loss  0.0063220784\n",
      "Epoch  10715  G loss  0.00480696\n",
      "Epoch  10716  G loss  0.011947999\n",
      "Epoch  10717  G loss  0.10000059\n",
      "Epoch  10718  G loss  0.0087286355\n",
      "Epoch  10719  G loss  0.06289025\n",
      "Epoch  10720  G loss  0.11549326\n",
      "Epoch  10721  G loss  0.0057039475\n",
      "Epoch  10722  G loss  0.03258179\n",
      "Epoch  10723  G loss  0.022240449\n",
      "Epoch  10724  G loss  0.26548433\n",
      "Epoch  10725  G loss  0.028044995\n",
      "Epoch  10726  G loss  0.39335966\n",
      "Epoch  10727  G loss  0.046753295\n",
      "Epoch  10728  G loss  0.0292495\n",
      "Epoch  10729  G loss  0.01887998\n",
      "Epoch  10730  G loss  0.025553426\n",
      "Epoch  10731  G loss  0.03835332\n",
      "Epoch  10732  G loss  0.17155275\n",
      "Epoch  10733  G loss  0.295491\n",
      "Epoch  10734  G loss  0.08673109\n",
      "Epoch  10735  G loss  0.008922452\n",
      "Epoch  10736  G loss  0.12516639\n",
      "Epoch  10737  G loss  0.041789576\n",
      "Epoch  10738  G loss  0.16833763\n",
      "Epoch  10739  G loss  0.043719057\n",
      "Epoch  10740  G loss  0.020005755\n",
      "Epoch  10741  G loss  0.38744605\n",
      "Epoch  10742  G loss  0.008953325\n",
      "Epoch  10743  G loss  0.08999278\n",
      "Epoch  10744  G loss  0.0766787\n",
      "Epoch  10745  G loss  0.015773678\n",
      "Epoch  10746  G loss  0.01096768\n",
      "Epoch  10747  G loss  0.03215433\n",
      "Epoch  10748  G loss  0.11946574\n",
      "Epoch  10749  G loss  0.029072102\n",
      "Epoch  10750  G loss  0.021618715\n",
      "Epoch  10751  G loss  0.021853171\n",
      "Epoch  10752  G loss  0.016177129\n",
      "Epoch  10753  G loss  0.031206513\n",
      "Epoch  10754  G loss  0.016058017\n",
      "Epoch  10755  G loss  0.022553105\n",
      "Epoch  10756  G loss  0.044468127\n",
      "Epoch  10757  G loss  0.028702062\n",
      "Epoch  10758  G loss  0.09840091\n",
      "Epoch  10759  G loss  0.0856455\n",
      "Epoch  10760  G loss  0.10576203\n",
      "Epoch  10761  G loss  0.10043484\n",
      "Epoch  10762  G loss  0.030530533\n",
      "Epoch  10763  G loss  0.056540847\n",
      "Epoch  10764  G loss  0.15985748\n",
      "Epoch  10765  G loss  0.01049378\n",
      "Epoch  10766  G loss  0.026014818\n",
      "Epoch  10767  G loss  0.072018\n",
      "Epoch  10768  G loss  0.089610234\n",
      "Epoch  10769  G loss  0.07378949\n",
      "Epoch  10770  G loss  0.08085275\n",
      "Epoch  10771  G loss  0.03554741\n",
      "Epoch  10772  G loss  0.00814204\n",
      "Epoch  10773  G loss  0.010073869\n",
      "Epoch  10774  G loss  0.054232255\n",
      "Epoch  10775  G loss  0.019348389\n",
      "Epoch  10776  G loss  0.010511544\n",
      "Epoch  10777  G loss  0.04360041\n",
      "Epoch  10778  G loss  0.019906204\n",
      "Epoch  10779  G loss  0.027831618\n",
      "Epoch  10780  G loss  0.015858531\n",
      "Epoch  10781  G loss  0.21541785\n",
      "Epoch  10782  G loss  0.035792228\n",
      "Epoch  10783  G loss  0.0037067921\n",
      "Epoch  10784  G loss  0.03307819\n",
      "Epoch  10785  G loss  0.05443485\n",
      "Epoch  10786  G loss  0.010401614\n",
      "Epoch  10787  G loss  0.07071853\n",
      "Epoch  10788  G loss  0.0092428345\n",
      "Epoch  10789  G loss  0.021736193\n",
      "Epoch  10790  G loss  0.097310916\n",
      "Epoch  10791  G loss  0.0049994355\n",
      "Epoch  10792  G loss  0.096885085\n",
      "Epoch  10793  G loss  0.02451869\n",
      "Epoch  10794  G loss  0.07951188\n",
      "Epoch  10795  G loss  0.044429973\n",
      "Epoch  10796  G loss  0.11074449\n",
      "Epoch  10797  G loss  0.01721245\n",
      "Epoch  10798  G loss  0.032715674\n",
      "Epoch  10799  G loss  0.005698488\n",
      "Epoch  10800  G loss  0.0067670634\n",
      "Epoch  10801  G loss  0.0061883205\n",
      "Epoch  10802  G loss  0.009735504\n",
      "Epoch  10803  G loss  0.026258804\n",
      "Epoch  10804  G loss  0.029878747\n",
      "Epoch  10805  G loss  0.13768941\n",
      "Epoch  10806  G loss  0.025085295\n",
      "Epoch  10807  G loss  0.14516836\n",
      "Epoch  10808  G loss  0.05593232\n",
      "Epoch  10809  G loss  0.045842938\n",
      "Epoch  10810  G loss  0.0217781\n",
      "Epoch  10811  G loss  0.09968325\n",
      "Epoch  10812  G loss  0.07364167\n",
      "Epoch  10813  G loss  0.00532552\n",
      "Epoch  10814  G loss  0.16145334\n",
      "Epoch  10815  G loss  0.09218852\n",
      "Epoch  10816  G loss  0.03292018\n",
      "Epoch  10817  G loss  0.10092005\n",
      "Epoch  10818  G loss  0.06796073\n",
      "Epoch  10819  G loss  0.022545755\n",
      "Epoch  10820  G loss  0.044320554\n",
      "Epoch  10821  G loss  0.014434325\n",
      "Epoch  10822  G loss  0.09591538\n",
      "Epoch  10823  G loss  0.023916831\n",
      "Epoch  10824  G loss  0.045810536\n",
      "Epoch  10825  G loss  0.09269795\n",
      "Epoch  10826  G loss  0.08671251\n",
      "Epoch  10827  G loss  0.06026786\n",
      "Epoch  10828  G loss  0.02117971\n",
      "Epoch  10829  G loss  0.043585874\n",
      "Epoch  10830  G loss  0.009473372\n",
      "Epoch  10831  G loss  0.05322376\n",
      "Epoch  10832  G loss  0.008893178\n",
      "Epoch  10833  G loss  0.023804154\n",
      "Epoch  10834  G loss  0.044222876\n",
      "Epoch  10835  G loss  0.04231975\n",
      "Epoch  10836  G loss  0.06550104\n",
      "Epoch  10837  G loss  0.009886418\n",
      "Epoch  10838  G loss  0.19735184\n",
      "Epoch  10839  G loss  0.01610773\n",
      "Epoch  10840  G loss  0.0727766\n",
      "Epoch  10841  G loss  0.16166408\n",
      "Epoch  10842  G loss  0.024383027\n",
      "Epoch  10843  G loss  0.009964889\n",
      "Epoch  10844  G loss  0.17748514\n",
      "Epoch  10845  G loss  0.023569502\n",
      "Epoch  10846  G loss  0.028108064\n",
      "Epoch  10847  G loss  0.16678223\n",
      "Epoch  10848  G loss  0.08194324\n",
      "Epoch  10849  G loss  0.050076745\n",
      "Epoch  10850  G loss  0.20674497\n",
      "Epoch  10851  G loss  0.07841356\n",
      "Epoch  10852  G loss  0.012939855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  10853  G loss  0.013415461\n",
      "Epoch  10854  G loss  0.008987209\n",
      "Epoch  10855  G loss  0.12029124\n",
      "Epoch  10856  G loss  0.0056034\n",
      "Epoch  10857  G loss  0.046514362\n",
      "Epoch  10858  G loss  0.009746572\n",
      "Epoch  10859  G loss  0.04979522\n",
      "Epoch  10860  G loss  0.061461736\n",
      "Epoch  10861  G loss  0.021226212\n",
      "Epoch  10862  G loss  0.528651\n",
      "Epoch  10863  G loss  0.06839364\n",
      "Epoch  10864  G loss  0.13719243\n",
      "Epoch  10865  G loss  0.041822262\n",
      "Epoch  10866  G loss  0.012715916\n",
      "Epoch  10867  G loss  0.010178644\n",
      "Epoch  10868  G loss  0.011766646\n",
      "Epoch  10869  G loss  0.008238168\n",
      "Epoch  10870  G loss  0.045928285\n",
      "Epoch  10871  G loss  0.004860428\n",
      "Epoch  10872  G loss  0.022785429\n",
      "Epoch  10873  G loss  0.01907977\n",
      "Epoch  10874  G loss  0.024814542\n",
      "Epoch  10875  G loss  0.009483336\n",
      "Epoch  10876  G loss  0.032657642\n",
      "Epoch  10877  G loss  0.011681745\n",
      "Epoch  10878  G loss  0.085902154\n",
      "Epoch  10879  G loss  0.102954134\n",
      "Epoch  10880  G loss  0.016503435\n",
      "Epoch  10881  G loss  0.039362714\n",
      "Epoch  10882  G loss  0.03306133\n",
      "Epoch  10883  G loss  0.051241472\n",
      "Epoch  10884  G loss  0.091926664\n",
      "Epoch  10885  G loss  0.035838965\n",
      "Epoch  10886  G loss  0.045833334\n",
      "Epoch  10887  G loss  0.013039049\n",
      "Epoch  10888  G loss  0.38029897\n",
      "Epoch  10889  G loss  0.03951019\n",
      "Epoch  10890  G loss  0.023562841\n",
      "Epoch  10891  G loss  0.095790505\n",
      "Epoch  10892  G loss  0.08358204\n",
      "Epoch  10893  G loss  0.2600502\n",
      "Epoch  10894  G loss  0.12502858\n",
      "Epoch  10895  G loss  0.00940446\n",
      "Epoch  10896  G loss  0.045126274\n",
      "Epoch  10897  G loss  0.027560081\n",
      "Epoch  10898  G loss  0.08579975\n",
      "Epoch  10899  G loss  0.026712077\n",
      "Epoch  10900  G loss  0.009758608\n",
      "Epoch  10901  G loss  0.07743208\n",
      "Epoch  10902  G loss  0.010579336\n",
      "Epoch  10903  G loss  0.0065964228\n",
      "Epoch  10904  G loss  0.009349327\n",
      "Epoch  10905  G loss  0.008155358\n",
      "Epoch  10906  G loss  0.13544467\n",
      "Epoch  10907  G loss  0.0058211144\n",
      "Epoch  10908  G loss  0.12538743\n",
      "Epoch  10909  G loss  0.013874838\n",
      "Epoch  10910  G loss  0.031116895\n",
      "Epoch  10911  G loss  0.062738\n",
      "Epoch  10912  G loss  0.0152278645\n",
      "Epoch  10913  G loss  0.22647291\n",
      "Epoch  10914  G loss  0.016402928\n",
      "Epoch  10915  G loss  0.034115236\n",
      "Epoch  10916  G loss  0.04521951\n",
      "Epoch  10917  G loss  0.041605096\n",
      "Epoch  10918  G loss  0.03731208\n",
      "Epoch  10919  G loss  0.011753822\n",
      "Epoch  10920  G loss  0.03098366\n",
      "Epoch  10921  G loss  0.045961518\n",
      "Epoch  10922  G loss  0.044058695\n",
      "Epoch  10923  G loss  0.014289042\n",
      "Epoch  10924  G loss  0.011697681\n",
      "Epoch  10925  G loss  0.06335896\n",
      "Epoch  10926  G loss  0.008830308\n",
      "Epoch  10927  G loss  0.026919289\n",
      "Epoch  10928  G loss  0.02843026\n",
      "Epoch  10929  G loss  0.03769421\n",
      "Epoch  10930  G loss  0.028565895\n",
      "Epoch  10931  G loss  0.06343113\n",
      "Epoch  10932  G loss  0.033788808\n",
      "Epoch  10933  G loss  0.019634662\n",
      "Epoch  10934  G loss  0.045920864\n",
      "Epoch  10935  G loss  0.026039016\n",
      "Epoch  10936  G loss  0.052824963\n",
      "Epoch  10937  G loss  0.0067715375\n",
      "Epoch  10938  G loss  0.020622468\n",
      "Epoch  10939  G loss  0.09679162\n",
      "Epoch  10940  G loss  0.033796635\n",
      "Epoch  10941  G loss  0.026097156\n",
      "Epoch  10942  G loss  0.12013346\n",
      "Epoch  10943  G loss  0.019297343\n",
      "Epoch  10944  G loss  0.19552845\n",
      "Epoch  10945  G loss  0.013965354\n",
      "Epoch  10946  G loss  0.1023899\n",
      "Epoch  10947  G loss  0.023437569\n",
      "Epoch  10948  G loss  0.044067807\n",
      "Epoch  10949  G loss  0.08344975\n",
      "Epoch  10950  G loss  0.008640304\n",
      "Epoch  10951  G loss  0.03136189\n",
      "Epoch  10952  G loss  0.054669037\n",
      "Epoch  10953  G loss  0.028312411\n",
      "Epoch  10954  G loss  0.015562451\n",
      "Epoch  10955  G loss  0.07393257\n",
      "Epoch  10956  G loss  0.0056309486\n",
      "Epoch  10957  G loss  0.017205138\n",
      "Epoch  10958  G loss  0.020803973\n",
      "Epoch  10959  G loss  0.047064014\n",
      "Epoch  10960  G loss  0.021517448\n",
      "Epoch  10961  G loss  0.18803252\n",
      "Epoch  10962  G loss  0.06283951\n",
      "Epoch  10963  G loss  0.058972366\n",
      "Epoch  10964  G loss  0.09306545\n",
      "Epoch  10965  G loss  0.025035404\n",
      "Epoch  10966  G loss  0.0146502275\n",
      "Epoch  10967  G loss  0.029424436\n",
      "Epoch  10968  G loss  0.10784222\n",
      "Epoch  10969  G loss  0.052220624\n",
      "Epoch  10970  G loss  0.022865087\n",
      "Epoch  10971  G loss  0.078305915\n",
      "Epoch  10972  G loss  0.024691582\n",
      "Epoch  10973  G loss  0.0344038\n",
      "Epoch  10974  G loss  0.01862742\n",
      "Epoch  10975  G loss  0.08392698\n",
      "Epoch  10976  G loss  0.046431668\n",
      "Epoch  10977  G loss  0.049988125\n",
      "Epoch  10978  G loss  0.07442286\n",
      "Epoch  10979  G loss  0.044350903\n",
      "Epoch  10980  G loss  0.007992959\n",
      "Epoch  10981  G loss  0.025758713\n",
      "Epoch  10982  G loss  0.1698659\n",
      "Epoch  10983  G loss  0.07924259\n",
      "Epoch  10984  G loss  0.023612458\n",
      "Epoch  10985  G loss  0.019002851\n",
      "Epoch  10986  G loss  0.20687076\n",
      "Epoch  10987  G loss  0.018743254\n",
      "Epoch  10988  G loss  0.023564931\n",
      "Epoch  10989  G loss  0.06608325\n",
      "Epoch  10990  G loss  0.15293448\n",
      "Epoch  10991  G loss  0.015128181\n",
      "Epoch  10992  G loss  0.028196493\n",
      "Epoch  10993  G loss  0.22899199\n",
      "Epoch  10994  G loss  0.10021348\n",
      "Epoch  10995  G loss  0.10356386\n",
      "Epoch  10996  G loss  0.20960872\n",
      "Epoch  10997  G loss  0.023825083\n",
      "Epoch  10998  G loss  0.022928951\n",
      "Epoch  10999  G loss  0.010684795\n",
      "Epoch  11000  G loss  0.018827977\n",
      "Epoch  11001  G loss  0.23987688\n",
      "Epoch  11002  G loss  0.009153513\n",
      "Epoch  11003  G loss  0.10109422\n",
      "Epoch  11004  G loss  0.040383894\n",
      "Epoch  11005  G loss  0.28097305\n",
      "Epoch  11006  G loss  0.2597169\n",
      "Epoch  11007  G loss  0.024046913\n",
      "Epoch  11008  G loss  0.022926312\n",
      "Epoch  11009  G loss  0.12418905\n",
      "Epoch  11010  G loss  0.036478497\n",
      "Epoch  11011  G loss  0.12187989\n",
      "Epoch  11012  G loss  0.020371595\n",
      "Epoch  11013  G loss  0.0052311462\n",
      "Epoch  11014  G loss  0.008793859\n",
      "Epoch  11015  G loss  0.27793097\n",
      "Epoch  11016  G loss  0.03347054\n",
      "Epoch  11017  G loss  0.16233398\n",
      "Epoch  11018  G loss  0.012133168\n",
      "Epoch  11019  G loss  0.08311047\n",
      "Epoch  11020  G loss  0.014967717\n",
      "Epoch  11021  G loss  0.02292038\n",
      "Epoch  11022  G loss  0.022869445\n",
      "Epoch  11023  G loss  0.009419516\n",
      "Epoch  11024  G loss  0.05544871\n",
      "Epoch  11025  G loss  0.0099957585\n",
      "Epoch  11026  G loss  0.032441705\n",
      "Epoch  11027  G loss  0.012282044\n",
      "Epoch  11028  G loss  0.14274779\n",
      "Epoch  11029  G loss  0.010707253\n",
      "Epoch  11030  G loss  0.029727764\n",
      "Epoch  11031  G loss  0.008935101\n",
      "Epoch  11032  G loss  0.057008002\n",
      "Epoch  11033  G loss  0.024890102\n",
      "Epoch  11034  G loss  0.018317524\n",
      "Epoch  11035  G loss  0.036880467\n",
      "Epoch  11036  G loss  0.007960465\n",
      "Epoch  11037  G loss  0.15697926\n",
      "Epoch  11038  G loss  0.09649158\n",
      "Epoch  11039  G loss  0.063621484\n",
      "Epoch  11040  G loss  0.024705574\n",
      "Epoch  11041  G loss  0.048249472\n",
      "Epoch  11042  G loss  0.22899409\n",
      "Epoch  11043  G loss  0.019744687\n",
      "Epoch  11044  G loss  0.107484266\n",
      "Epoch  11045  G loss  0.0040125716\n",
      "Epoch  11046  G loss  0.019332906\n",
      "Epoch  11047  G loss  0.03788069\n",
      "Epoch  11048  G loss  0.07524908\n",
      "Epoch  11049  G loss  0.017678913\n",
      "Epoch  11050  G loss  0.016292688\n",
      "Epoch  11051  G loss  0.013830563\n",
      "Epoch  11052  G loss  0.07133599\n",
      "Epoch  11053  G loss  0.029761279\n",
      "Epoch  11054  G loss  0.030931385\n",
      "Epoch  11055  G loss  0.056675836\n",
      "Epoch  11056  G loss  0.011673061\n",
      "Epoch  11057  G loss  0.020250311\n",
      "Epoch  11058  G loss  0.05150737\n",
      "Epoch  11059  G loss  0.053717386\n",
      "Epoch  11060  G loss  0.01356832\n",
      "Epoch  11061  G loss  0.09263492\n",
      "Epoch  11062  G loss  0.013606049\n",
      "Epoch  11063  G loss  0.06614686\n",
      "Epoch  11064  G loss  0.065491565\n",
      "Epoch  11065  G loss  0.02325071\n",
      "Epoch  11066  G loss  0.06904982\n",
      "Epoch  11067  G loss  0.10871345\n",
      "Epoch  11068  G loss  0.2639138\n",
      "Epoch  11069  G loss  0.023721635\n",
      "Epoch  11070  G loss  0.03105231\n",
      "Epoch  11071  G loss  0.005381802\n",
      "Epoch  11072  G loss  0.019286837\n",
      "Epoch  11073  G loss  0.048756424\n",
      "Epoch  11074  G loss  0.010611721\n",
      "Epoch  11075  G loss  0.047784485\n",
      "Epoch  11076  G loss  0.06263981\n",
      "Epoch  11077  G loss  0.0041901204\n",
      "Epoch  11078  G loss  0.023498446\n",
      "Epoch  11079  G loss  0.007994996\n",
      "Epoch  11080  G loss  0.13739525\n",
      "Epoch  11081  G loss  0.08101925\n",
      "Epoch  11082  G loss  0.008402554\n",
      "Epoch  11083  G loss  0.030774903\n",
      "Epoch  11084  G loss  0.079193756\n",
      "Epoch  11085  G loss  0.026818417\n",
      "Epoch  11086  G loss  0.020357704\n",
      "Epoch  11087  G loss  0.13333876\n",
      "Epoch  11088  G loss  0.044767953\n",
      "Epoch  11089  G loss  0.03239999\n",
      "Epoch  11090  G loss  0.0159447\n",
      "Epoch  11091  G loss  0.034345955\n",
      "Epoch  11092  G loss  0.049566925\n",
      "Epoch  11093  G loss  0.021749431\n",
      "Epoch  11094  G loss  0.17087498\n",
      "Epoch  11095  G loss  0.052049465\n",
      "Epoch  11096  G loss  0.0057266504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  11097  G loss  0.01956642\n",
      "Epoch  11098  G loss  0.06617381\n",
      "Epoch  11099  G loss  0.010485492\n",
      "Epoch  11100  G loss  0.13992362\n",
      "Epoch  11101  G loss  0.12082614\n",
      "Epoch  11102  G loss  0.008189443\n",
      "Epoch  11103  G loss  0.014871703\n",
      "Epoch  11104  G loss  0.020860957\n",
      "Epoch  11105  G loss  0.21504712\n",
      "Epoch  11106  G loss  0.10199818\n",
      "Epoch  11107  G loss  0.07080333\n",
      "Epoch  11108  G loss  0.01732589\n",
      "Epoch  11109  G loss  0.054432277\n",
      "Epoch  11110  G loss  0.06059959\n",
      "Epoch  11111  G loss  0.08873576\n",
      "Epoch  11112  G loss  0.09864799\n",
      "Epoch  11113  G loss  0.07144466\n",
      "Epoch  11114  G loss  0.004372258\n",
      "Epoch  11115  G loss  0.014945054\n",
      "Epoch  11116  G loss  0.05471477\n",
      "Epoch  11117  G loss  0.037365302\n",
      "Epoch  11118  G loss  0.039825674\n",
      "Epoch  11119  G loss  0.015853235\n",
      "Epoch  11120  G loss  0.007711354\n",
      "Epoch  11121  G loss  0.037764728\n",
      "Epoch  11122  G loss  0.110462755\n",
      "Epoch  11123  G loss  0.018793419\n",
      "Epoch  11124  G loss  0.049875595\n",
      "Epoch  11125  G loss  0.013782298\n",
      "Epoch  11126  G loss  0.043798707\n",
      "Epoch  11127  G loss  0.013473235\n",
      "Epoch  11128  G loss  0.0052425303\n",
      "Epoch  11129  G loss  0.022683984\n",
      "Epoch  11130  G loss  0.011050446\n",
      "Epoch  11131  G loss  0.042443816\n",
      "Epoch  11132  G loss  0.008631006\n",
      "Epoch  11133  G loss  0.020891387\n",
      "Epoch  11134  G loss  0.011006861\n",
      "Epoch  11135  G loss  0.047679447\n",
      "Epoch  11136  G loss  0.57621115\n",
      "Epoch  11137  G loss  0.16659784\n",
      "Epoch  11138  G loss  0.019323628\n",
      "Epoch  11139  G loss  0.043562487\n",
      "Epoch  11140  G loss  0.07547089\n",
      "Epoch  11141  G loss  0.023769386\n",
      "Epoch  11142  G loss  0.053426884\n",
      "Epoch  11143  G loss  0.11869065\n",
      "Epoch  11144  G loss  0.012680709\n",
      "Epoch  11145  G loss  0.032169394\n",
      "Epoch  11146  G loss  0.021247655\n",
      "Epoch  11147  G loss  0.013751038\n",
      "Epoch  11148  G loss  0.10989195\n",
      "Epoch  11149  G loss  0.0042368015\n",
      "Epoch  11150  G loss  0.0044836286\n",
      "Epoch  11151  G loss  0.017609268\n",
      "Epoch  11152  G loss  0.008034233\n",
      "Epoch  11153  G loss  0.03812392\n",
      "Epoch  11154  G loss  0.12918496\n",
      "Epoch  11155  G loss  0.019745983\n",
      "Epoch  11156  G loss  0.2225893\n",
      "Epoch  11157  G loss  0.044814114\n",
      "Epoch  11158  G loss  0.06132386\n",
      "Epoch  11159  G loss  0.025733296\n",
      "Epoch  11160  G loss  0.02522429\n",
      "Epoch  11161  G loss  0.011046309\n",
      "Epoch  11162  G loss  0.027732428\n",
      "Epoch  11163  G loss  0.11806168\n",
      "Epoch  11164  G loss  0.048692573\n",
      "Epoch  11165  G loss  0.028326316\n",
      "Epoch  11166  G loss  0.046035085\n",
      "Epoch  11167  G loss  0.0039987024\n",
      "Epoch  11168  G loss  0.008050475\n",
      "Epoch  11169  G loss  0.014922813\n",
      "Epoch  11170  G loss  0.04334485\n",
      "Epoch  11171  G loss  0.006298111\n",
      "Epoch  11172  G loss  0.010314465\n",
      "Epoch  11173  G loss  0.028873889\n",
      "Epoch  11174  G loss  0.01737724\n",
      "Epoch  11175  G loss  0.029396558\n",
      "Epoch  11176  G loss  0.0047406475\n",
      "Epoch  11177  G loss  0.022948705\n",
      "Epoch  11178  G loss  0.013443374\n",
      "Epoch  11179  G loss  0.1067987\n",
      "Epoch  11180  G loss  0.010706358\n",
      "Epoch  11181  G loss  0.080028065\n",
      "Epoch  11182  G loss  0.0165409\n",
      "Epoch  11183  G loss  0.038904592\n",
      "Epoch  11184  G loss  0.017372966\n",
      "Epoch  11185  G loss  0.08398209\n",
      "Epoch  11186  G loss  0.019664057\n",
      "Epoch  11187  G loss  0.023454553\n",
      "Epoch  11188  G loss  0.022652572\n",
      "Epoch  11189  G loss  0.020759521\n",
      "Epoch  11190  G loss  0.10104973\n",
      "Epoch  11191  G loss  0.060426593\n",
      "Epoch  11192  G loss  0.10726136\n",
      "Epoch  11193  G loss  0.0070748143\n",
      "Epoch  11194  G loss  0.012230915\n",
      "Epoch  11195  G loss  0.013580463\n",
      "Epoch  11196  G loss  0.13580862\n",
      "Epoch  11197  G loss  0.016105006\n",
      "Epoch  11198  G loss  0.004879244\n",
      "Epoch  11199  G loss  0.04136385\n",
      "Epoch  11200  G loss  0.012000399\n",
      "Epoch  11201  G loss  0.23238224\n",
      "Epoch  11202  G loss  0.17974488\n",
      "Epoch  11203  G loss  0.023011133\n",
      "Epoch  11204  G loss  0.005345223\n",
      "Epoch  11205  G loss  0.01530806\n",
      "Epoch  11206  G loss  0.09204148\n",
      "Epoch  11207  G loss  0.028190052\n",
      "Epoch  11208  G loss  0.043924928\n",
      "Epoch  11209  G loss  0.0638837\n",
      "Epoch  11210  G loss  0.041903317\n",
      "Epoch  11211  G loss  0.007414682\n",
      "Epoch  11212  G loss  0.041543767\n",
      "Epoch  11213  G loss  0.07605152\n",
      "Epoch  11214  G loss  0.018110432\n",
      "Epoch  11215  G loss  0.012686357\n",
      "Epoch  11216  G loss  0.0073038386\n",
      "Epoch  11217  G loss  0.005864727\n",
      "Epoch  11218  G loss  0.0074727535\n",
      "Epoch  11219  G loss  0.01328406\n",
      "Epoch  11220  G loss  0.04796882\n",
      "Epoch  11221  G loss  0.07946059\n",
      "Epoch  11222  G loss  0.01309341\n",
      "Epoch  11223  G loss  0.029770864\n",
      "Epoch  11224  G loss  0.01880512\n",
      "Epoch  11225  G loss  0.14904895\n",
      "Epoch  11226  G loss  0.008546079\n",
      "Epoch  11227  G loss  0.118069544\n",
      "Epoch  11228  G loss  0.010690865\n",
      "Epoch  11229  G loss  0.02251074\n",
      "Epoch  11230  G loss  0.027188271\n",
      "Epoch  11231  G loss  0.08714686\n",
      "Epoch  11232  G loss  0.070527196\n",
      "Epoch  11233  G loss  0.06929292\n",
      "Epoch  11234  G loss  0.025354462\n",
      "Epoch  11235  G loss  0.034679215\n",
      "Epoch  11236  G loss  0.004173619\n",
      "Epoch  11237  G loss  0.0071795327\n",
      "Epoch  11238  G loss  0.060056858\n",
      "Epoch  11239  G loss  0.009211663\n",
      "Epoch  11240  G loss  0.02177576\n",
      "Epoch  11241  G loss  0.017867923\n",
      "Epoch  11242  G loss  0.018378766\n",
      "Epoch  11243  G loss  0.045837153\n",
      "Epoch  11244  G loss  0.006325023\n",
      "Epoch  11245  G loss  0.01250997\n",
      "Epoch  11246  G loss  0.055868793\n",
      "Epoch  11247  G loss  0.08938885\n",
      "Epoch  11248  G loss  0.02027996\n",
      "Epoch  11249  G loss  0.012204282\n",
      "Epoch  11250  G loss  0.08648321\n",
      "Epoch  11251  G loss  0.019994602\n",
      "Epoch  11252  G loss  0.09823628\n",
      "Epoch  11253  G loss  0.08457422\n",
      "Epoch  11254  G loss  0.011137161\n",
      "Epoch  11255  G loss  0.007243774\n",
      "Epoch  11256  G loss  0.053262204\n",
      "Epoch  11257  G loss  0.08373186\n",
      "Epoch  11258  G loss  0.048430324\n",
      "Epoch  11259  G loss  0.02900467\n",
      "Epoch  11260  G loss  0.014476935\n",
      "Epoch  11261  G loss  0.01970892\n",
      "Epoch  11262  G loss  0.0061374432\n",
      "Epoch  11263  G loss  0.010115231\n",
      "Epoch  11264  G loss  0.031883344\n",
      "Epoch  11265  G loss  0.009688864\n",
      "Epoch  11266  G loss  0.014487084\n",
      "Epoch  11267  G loss  0.037487723\n",
      "Epoch  11268  G loss  0.09148787\n",
      "Epoch  11269  G loss  0.015283508\n",
      "Epoch  11270  G loss  0.014406186\n",
      "Epoch  11271  G loss  0.013164165\n",
      "Epoch  11272  G loss  0.008711578\n",
      "Epoch  11273  G loss  0.14258574\n",
      "Epoch  11274  G loss  0.013673065\n",
      "Epoch  11275  G loss  0.005623583\n",
      "Epoch  11276  G loss  0.03489763\n",
      "Epoch  11277  G loss  0.03750527\n",
      "Epoch  11278  G loss  0.025228672\n",
      "Epoch  11279  G loss  0.012572166\n",
      "Epoch  11280  G loss  0.010154031\n",
      "Epoch  11281  G loss  0.0082884\n",
      "Epoch  11282  G loss  0.040238768\n",
      "Epoch  11283  G loss  0.061341166\n",
      "Epoch  11284  G loss  0.014903133\n",
      "Epoch  11285  G loss  0.049772445\n",
      "Epoch  11286  G loss  0.019503374\n",
      "Epoch  11287  G loss  0.12758009\n",
      "Epoch  11288  G loss  0.027987111\n",
      "Epoch  11289  G loss  0.029905993\n",
      "Epoch  11290  G loss  0.014654983\n",
      "Epoch  11291  G loss  0.23861946\n",
      "Epoch  11292  G loss  0.07351725\n",
      "Epoch  11293  G loss  0.0579258\n",
      "Epoch  11294  G loss  0.041971155\n",
      "Epoch  11295  G loss  0.06495114\n",
      "Epoch  11296  G loss  0.011305443\n",
      "Epoch  11297  G loss  0.047151558\n",
      "Epoch  11298  G loss  0.07507417\n",
      "Epoch  11299  G loss  0.034899753\n",
      "Epoch  11300  G loss  0.057185225\n",
      "Epoch  11301  G loss  0.0119291525\n",
      "Epoch  11302  G loss  0.06536318\n",
      "Epoch  11303  G loss  0.027559755\n",
      "Epoch  11304  G loss  0.0104057845\n",
      "Epoch  11305  G loss  0.013856818\n",
      "Epoch  11306  G loss  0.091267034\n",
      "Epoch  11307  G loss  0.009657549\n",
      "Epoch  11308  G loss  0.019879013\n",
      "Epoch  11309  G loss  0.04176201\n",
      "Epoch  11310  G loss  0.030671954\n",
      "Epoch  11311  G loss  0.07867114\n",
      "Epoch  11312  G loss  0.2979777\n",
      "Epoch  11313  G loss  0.018017873\n",
      "Epoch  11314  G loss  0.03916648\n",
      "Epoch  11315  G loss  0.006087184\n",
      "Epoch  11316  G loss  0.010471212\n",
      "Epoch  11317  G loss  0.014575312\n",
      "Epoch  11318  G loss  0.05664353\n",
      "Epoch  11319  G loss  0.09451924\n",
      "Epoch  11320  G loss  0.04738325\n",
      "Epoch  11321  G loss  0.08914542\n",
      "Epoch  11322  G loss  0.016513791\n",
      "Epoch  11323  G loss  0.06563207\n",
      "Epoch  11324  G loss  0.01135795\n",
      "Epoch  11325  G loss  0.0058359606\n",
      "Epoch  11326  G loss  0.19889203\n",
      "Epoch  11327  G loss  0.28072518\n",
      "Epoch  11328  G loss  0.0059415\n",
      "Epoch  11329  G loss  0.11209658\n",
      "Epoch  11330  G loss  0.04561784\n",
      "Epoch  11331  G loss  0.019457448\n",
      "Epoch  11332  G loss  0.024252715\n",
      "Epoch  11333  G loss  0.04784571\n",
      "Epoch  11334  G loss  0.014650421\n",
      "Epoch  11335  G loss  0.025471978\n",
      "Epoch  11336  G loss  0.020651478\n",
      "Epoch  11337  G loss  0.004801376\n",
      "Epoch  11338  G loss  0.016078657\n",
      "Epoch  11339  G loss  0.018130738\n",
      "Epoch  11340  G loss  0.013733616\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  11341  G loss  0.009867279\n",
      "Epoch  11342  G loss  0.014563131\n",
      "Epoch  11343  G loss  0.0112878885\n",
      "Epoch  11344  G loss  0.009369047\n",
      "Epoch  11345  G loss  0.17348713\n",
      "Epoch  11346  G loss  0.026246808\n",
      "Epoch  11347  G loss  0.13768205\n",
      "Epoch  11348  G loss  0.056446712\n",
      "Epoch  11349  G loss  0.033326354\n",
      "Epoch  11350  G loss  0.0129823275\n",
      "Epoch  11351  G loss  0.034768235\n",
      "Epoch  11352  G loss  0.030481474\n",
      "Epoch  11353  G loss  0.007903598\n",
      "Epoch  11354  G loss  0.028390747\n",
      "Epoch  11355  G loss  0.02227205\n",
      "Epoch  11356  G loss  0.005350791\n",
      "Epoch  11357  G loss  0.044007976\n",
      "Epoch  11358  G loss  0.009281942\n",
      "Epoch  11359  G loss  0.039736755\n",
      "Epoch  11360  G loss  0.08483076\n",
      "Epoch  11361  G loss  0.016160123\n",
      "Epoch  11362  G loss  0.048857078\n",
      "Epoch  11363  G loss  0.04983915\n",
      "Epoch  11364  G loss  0.06783221\n",
      "Epoch  11365  G loss  0.16218683\n",
      "Epoch  11366  G loss  0.0036839847\n",
      "Epoch  11367  G loss  0.0073479097\n",
      "Epoch  11368  G loss  0.06010595\n",
      "Epoch  11369  G loss  0.024894644\n",
      "Epoch  11370  G loss  0.032592606\n",
      "Epoch  11371  G loss  0.010239263\n",
      "Epoch  11372  G loss  0.01081488\n",
      "Epoch  11373  G loss  0.020369932\n",
      "Epoch  11374  G loss  0.14971185\n",
      "Epoch  11375  G loss  0.059015974\n",
      "Epoch  11376  G loss  0.019786313\n",
      "Epoch  11377  G loss  0.012603834\n",
      "Epoch  11378  G loss  0.05288151\n",
      "Epoch  11379  G loss  0.06332631\n",
      "Epoch  11380  G loss  0.22976889\n",
      "Epoch  11381  G loss  0.02388168\n",
      "Epoch  11382  G loss  0.004736559\n",
      "Epoch  11383  G loss  0.01053059\n",
      "Epoch  11384  G loss  0.038990527\n",
      "Epoch  11385  G loss  0.009141912\n",
      "Epoch  11386  G loss  0.020865967\n",
      "Epoch  11387  G loss  0.031189725\n",
      "Epoch  11388  G loss  0.008033049\n",
      "Epoch  11389  G loss  0.008838765\n",
      "Epoch  11390  G loss  0.010776499\n",
      "Epoch  11391  G loss  0.017286109\n",
      "Epoch  11392  G loss  0.052363846\n",
      "Epoch  11393  G loss  0.0028486676\n",
      "Epoch  11394  G loss  0.021797737\n",
      "Epoch  11395  G loss  0.06412232\n",
      "Epoch  11396  G loss  0.022216896\n",
      "Epoch  11397  G loss  0.08705273\n",
      "Epoch  11398  G loss  0.04863863\n",
      "Epoch  11399  G loss  0.10502951\n",
      "Epoch  11400  G loss  0.009019314\n",
      "Epoch  11401  G loss  0.051594414\n",
      "Epoch  11402  G loss  0.015605008\n",
      "Epoch  11403  G loss  0.0333111\n",
      "Epoch  11404  G loss  0.032011073\n",
      "Epoch  11405  G loss  0.016598266\n",
      "Epoch  11406  G loss  0.049150534\n",
      "Epoch  11407  G loss  0.035877064\n",
      "Epoch  11408  G loss  0.018929418\n",
      "Epoch  11409  G loss  0.0075688963\n",
      "Epoch  11410  G loss  0.036793683\n",
      "Epoch  11411  G loss  0.024728574\n",
      "Epoch  11412  G loss  0.04226847\n",
      "Epoch  11413  G loss  0.010015501\n",
      "Epoch  11414  G loss  0.1941663\n",
      "Epoch  11415  G loss  0.010293033\n",
      "Epoch  11416  G loss  0.00790347\n",
      "Epoch  11417  G loss  0.012222164\n",
      "Epoch  11418  G loss  0.27668673\n",
      "Epoch  11419  G loss  0.015898723\n",
      "Epoch  11420  G loss  0.01601444\n",
      "Epoch  11421  G loss  0.038554035\n",
      "Epoch  11422  G loss  0.012342088\n",
      "Epoch  11423  G loss  0.006519355\n",
      "Epoch  11424  G loss  0.033852186\n",
      "Epoch  11425  G loss  0.056954987\n",
      "Epoch  11426  G loss  0.09281297\n",
      "Epoch  11427  G loss  0.013822942\n",
      "Epoch  11428  G loss  0.13917638\n",
      "Epoch  11429  G loss  0.02326852\n",
      "Epoch  11430  G loss  0.113011815\n",
      "Epoch  11431  G loss  0.07003547\n",
      "Epoch  11432  G loss  0.013823496\n",
      "Epoch  11433  G loss  0.0062374678\n",
      "Epoch  11434  G loss  0.070604004\n",
      "Epoch  11435  G loss  0.13051829\n",
      "Epoch  11436  G loss  0.018339157\n",
      "Epoch  11437  G loss  0.1152811\n",
      "Epoch  11438  G loss  0.046799265\n",
      "Epoch  11439  G loss  0.007197314\n",
      "Epoch  11440  G loss  0.05643635\n",
      "Epoch  11441  G loss  0.032320846\n",
      "Epoch  11442  G loss  0.08969417\n",
      "Epoch  11443  G loss  0.11381874\n",
      "Epoch  11444  G loss  0.11276592\n",
      "Epoch  11445  G loss  0.09248057\n",
      "Epoch  11446  G loss  0.032324873\n",
      "Epoch  11447  G loss  0.015933752\n",
      "Epoch  11448  G loss  0.04174617\n",
      "Epoch  11449  G loss  0.069343016\n",
      "Epoch  11450  G loss  0.044565745\n",
      "Epoch  11451  G loss  0.08397567\n",
      "Epoch  11452  G loss  0.04144658\n",
      "Epoch  11453  G loss  0.04649876\n",
      "Epoch  11454  G loss  0.09817818\n",
      "Epoch  11455  G loss  0.19616935\n",
      "Epoch  11456  G loss  0.00652879\n",
      "Epoch  11457  G loss  0.0036949052\n",
      "Epoch  11458  G loss  0.07899773\n",
      "Epoch  11459  G loss  0.009779743\n",
      "Epoch  11460  G loss  0.011162816\n",
      "Epoch  11461  G loss  0.008238794\n",
      "Epoch  11462  G loss  0.08084312\n",
      "Epoch  11463  G loss  0.1835806\n",
      "Epoch  11464  G loss  0.022647148\n",
      "Epoch  11465  G loss  0.19082928\n",
      "Epoch  11466  G loss  0.03826313\n",
      "Epoch  11467  G loss  0.01447487\n",
      "Epoch  11468  G loss  0.029355682\n",
      "Epoch  11469  G loss  0.1452007\n",
      "Epoch  11470  G loss  0.092408925\n",
      "Epoch  11471  G loss  0.0047165793\n",
      "Epoch  11472  G loss  0.022099456\n",
      "Epoch  11473  G loss  0.13348159\n",
      "Epoch  11474  G loss  0.010561743\n",
      "Epoch  11475  G loss  0.12611246\n",
      "Epoch  11476  G loss  0.01010582\n",
      "Epoch  11477  G loss  0.0385528\n",
      "Epoch  11478  G loss  0.009065802\n",
      "Epoch  11479  G loss  0.025621727\n",
      "Epoch  11480  G loss  0.021964327\n",
      "Epoch  11481  G loss  0.05881976\n",
      "Epoch  11482  G loss  0.0344726\n",
      "Epoch  11483  G loss  0.008603648\n",
      "Epoch  11484  G loss  0.0068540294\n",
      "Epoch  11485  G loss  0.0473325\n",
      "Epoch  11486  G loss  0.20581011\n",
      "Epoch  11487  G loss  0.15224347\n",
      "Epoch  11488  G loss  0.0052927393\n",
      "Epoch  11489  G loss  0.07846906\n",
      "Epoch  11490  G loss  0.033671707\n",
      "Epoch  11491  G loss  0.03745066\n",
      "Epoch  11492  G loss  0.25190628\n",
      "Epoch  11493  G loss  0.04232053\n",
      "Epoch  11494  G loss  0.05119038\n",
      "Epoch  11495  G loss  0.01611286\n",
      "Epoch  11496  G loss  0.038532123\n",
      "Epoch  11497  G loss  0.012664584\n",
      "Epoch  11498  G loss  0.0144910915\n",
      "Epoch  11499  G loss  0.047306176\n",
      "Epoch  11500  G loss  0.04482241\n",
      "Epoch  11501  G loss  0.06516831\n",
      "Epoch  11502  G loss  0.079420894\n",
      "Epoch  11503  G loss  0.021286342\n",
      "Epoch  11504  G loss  0.01583364\n",
      "Epoch  11505  G loss  0.043060917\n",
      "Epoch  11506  G loss  0.057919957\n",
      "Epoch  11507  G loss  0.022453254\n",
      "Epoch  11508  G loss  0.1137003\n",
      "Epoch  11509  G loss  0.016034443\n",
      "Epoch  11510  G loss  0.012319105\n",
      "Epoch  11511  G loss  0.054736413\n",
      "Epoch  11512  G loss  0.01569723\n",
      "Epoch  11513  G loss  0.0816019\n",
      "Epoch  11514  G loss  0.06802807\n",
      "Epoch  11515  G loss  0.12936227\n",
      "Epoch  11516  G loss  0.0066096457\n",
      "Epoch  11517  G loss  0.018039782\n",
      "Epoch  11518  G loss  0.033562295\n",
      "Epoch  11519  G loss  0.008149341\n",
      "Epoch  11520  G loss  0.028721655\n",
      "Epoch  11521  G loss  0.02636865\n",
      "Epoch  11522  G loss  0.050037026\n",
      "Epoch  11523  G loss  0.008960331\n",
      "Epoch  11524  G loss  0.12188971\n",
      "Epoch  11525  G loss  0.020339835\n",
      "Epoch  11526  G loss  0.061994985\n",
      "Epoch  11527  G loss  0.008740498\n",
      "Epoch  11528  G loss  0.02237609\n",
      "Epoch  11529  G loss  0.018458266\n",
      "Epoch  11530  G loss  0.20913333\n",
      "Epoch  11531  G loss  0.047530036\n",
      "Epoch  11532  G loss  0.04108058\n",
      "Epoch  11533  G loss  0.006260981\n",
      "Epoch  11534  G loss  0.025770832\n",
      "Epoch  11535  G loss  0.018664625\n",
      "Epoch  11536  G loss  0.0029245876\n",
      "Epoch  11537  G loss  0.10509813\n",
      "Epoch  11538  G loss  0.016976725\n",
      "Epoch  11539  G loss  0.042913757\n",
      "Epoch  11540  G loss  0.050908603\n",
      "Epoch  11541  G loss  0.004268377\n",
      "Epoch  11542  G loss  0.0088279825\n",
      "Epoch  11543  G loss  0.07988468\n",
      "Epoch  11544  G loss  0.005892088\n",
      "Epoch  11545  G loss  0.089541435\n",
      "Epoch  11546  G loss  0.13525997\n",
      "Epoch  11547  G loss  0.08078816\n",
      "Epoch  11548  G loss  0.09345256\n",
      "Epoch  11549  G loss  0.07510416\n",
      "Epoch  11550  G loss  0.023192193\n",
      "Epoch  11551  G loss  0.11930176\n",
      "Epoch  11552  G loss  0.014953514\n",
      "Epoch  11553  G loss  0.08421022\n",
      "Epoch  11554  G loss  0.039548017\n",
      "Epoch  11555  G loss  0.03426818\n",
      "Epoch  11556  G loss  0.22088802\n",
      "Epoch  11557  G loss  0.13350558\n",
      "Epoch  11558  G loss  0.0024416512\n",
      "Epoch  11559  G loss  0.040810727\n",
      "Epoch  11560  G loss  0.026608717\n",
      "Epoch  11561  G loss  0.015230178\n",
      "Epoch  11562  G loss  0.010709984\n",
      "Epoch  11563  G loss  0.025446936\n",
      "Epoch  11564  G loss  0.018304825\n",
      "Epoch  11565  G loss  0.021140745\n",
      "Epoch  11566  G loss  0.010760219\n",
      "Epoch  11567  G loss  0.09672178\n",
      "Epoch  11568  G loss  0.071633056\n",
      "Epoch  11569  G loss  0.008253384\n",
      "Epoch  11570  G loss  0.0023063438\n",
      "Epoch  11571  G loss  0.006399271\n",
      "Epoch  11572  G loss  0.030065184\n",
      "Epoch  11573  G loss  0.22950244\n",
      "Epoch  11574  G loss  0.018703787\n",
      "Epoch  11575  G loss  0.051760133\n",
      "Epoch  11576  G loss  0.037594154\n",
      "Epoch  11577  G loss  0.009421485\n",
      "Epoch  11578  G loss  0.0378701\n",
      "Epoch  11579  G loss  0.020909846\n",
      "Epoch  11580  G loss  0.16913465\n",
      "Epoch  11581  G loss  0.017114343\n",
      "Epoch  11582  G loss  0.09387141\n",
      "Epoch  11583  G loss  0.06617795\n",
      "Epoch  11584  G loss  0.029345248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  11585  G loss  0.08957981\n",
      "Epoch  11586  G loss  0.019518988\n",
      "Epoch  11587  G loss  0.005921647\n",
      "Epoch  11588  G loss  0.072333455\n",
      "Epoch  11589  G loss  0.06141962\n",
      "Epoch  11590  G loss  0.14047378\n",
      "Epoch  11591  G loss  0.095361635\n",
      "Epoch  11592  G loss  0.01376402\n",
      "Epoch  11593  G loss  0.084189504\n",
      "Epoch  11594  G loss  0.21491054\n",
      "Epoch  11595  G loss  0.1017092\n",
      "Epoch  11596  G loss  0.015195521\n",
      "Epoch  11597  G loss  0.0116598\n",
      "Epoch  11598  G loss  0.016125781\n",
      "Epoch  11599  G loss  0.030728118\n",
      "Epoch  11600  G loss  0.016911145\n",
      "Epoch  11601  G loss  0.050396025\n",
      "Epoch  11602  G loss  0.11418536\n",
      "Epoch  11603  G loss  0.02197327\n",
      "Epoch  11604  G loss  0.0030013076\n",
      "Epoch  11605  G loss  0.056890845\n",
      "Epoch  11606  G loss  0.14909047\n",
      "Epoch  11607  G loss  0.0057585607\n",
      "Epoch  11608  G loss  0.053760096\n",
      "Epoch  11609  G loss  0.02620329\n",
      "Epoch  11610  G loss  0.013163216\n",
      "Epoch  11611  G loss  0.008746181\n",
      "Epoch  11612  G loss  0.16123275\n",
      "Epoch  11613  G loss  0.041312724\n",
      "Epoch  11614  G loss  0.008354822\n",
      "Epoch  11615  G loss  0.02653424\n",
      "Epoch  11616  G loss  0.0097169755\n",
      "Epoch  11617  G loss  0.031010535\n",
      "Epoch  11618  G loss  0.022189751\n",
      "Epoch  11619  G loss  0.13653044\n",
      "Epoch  11620  G loss  0.10091664\n",
      "Epoch  11621  G loss  0.038893204\n",
      "Epoch  11622  G loss  0.0054087937\n",
      "Epoch  11623  G loss  0.028607834\n",
      "Epoch  11624  G loss  0.4632252\n",
      "Epoch  11625  G loss  0.019261306\n",
      "Epoch  11626  G loss  0.25148788\n",
      "Epoch  11627  G loss  0.015500802\n",
      "Epoch  11628  G loss  0.00833169\n",
      "Epoch  11629  G loss  0.06266604\n",
      "Epoch  11630  G loss  0.020042377\n",
      "Epoch  11631  G loss  0.010099584\n",
      "Epoch  11632  G loss  0.14906025\n",
      "Epoch  11633  G loss  0.0200265\n",
      "Epoch  11634  G loss  0.17321292\n",
      "Epoch  11635  G loss  0.33492535\n",
      "Epoch  11636  G loss  0.047858354\n",
      "Epoch  11637  G loss  0.04077064\n",
      "Epoch  11638  G loss  0.017128404\n",
      "Epoch  11639  G loss  0.101707384\n",
      "Epoch  11640  G loss  0.035078056\n",
      "Epoch  11641  G loss  0.0075760167\n",
      "Epoch  11642  G loss  0.033081036\n",
      "Epoch  11643  G loss  0.020462926\n",
      "Epoch  11644  G loss  0.036088362\n",
      "Epoch  11645  G loss  0.0087545775\n",
      "Epoch  11646  G loss  0.101568714\n",
      "Epoch  11647  G loss  0.0348341\n",
      "Epoch  11648  G loss  0.004972615\n",
      "Epoch  11649  G loss  0.047670696\n",
      "Epoch  11650  G loss  0.04419042\n",
      "Epoch  11651  G loss  0.13731827\n",
      "Epoch  11652  G loss  0.0067189606\n",
      "Epoch  11653  G loss  0.025466032\n",
      "Epoch  11654  G loss  0.23882136\n",
      "Epoch  11655  G loss  0.009395796\n",
      "Epoch  11656  G loss  0.037297312\n",
      "Epoch  11657  G loss  0.009813737\n",
      "Epoch  11658  G loss  0.009667602\n",
      "Epoch  11659  G loss  0.0154667925\n",
      "Epoch  11660  G loss  0.013051355\n",
      "Epoch  11661  G loss  0.17437431\n",
      "Epoch  11662  G loss  0.06316327\n",
      "Epoch  11663  G loss  0.0043441607\n",
      "Epoch  11664  G loss  0.019756302\n",
      "Epoch  11665  G loss  0.00838621\n",
      "Epoch  11666  G loss  0.0076907263\n",
      "Epoch  11667  G loss  0.019190606\n",
      "Epoch  11668  G loss  0.04938399\n",
      "Epoch  11669  G loss  0.006441502\n",
      "Epoch  11670  G loss  0.0060110986\n",
      "Epoch  11671  G loss  0.0070754196\n",
      "Epoch  11672  G loss  0.019647995\n",
      "Epoch  11673  G loss  0.09088388\n",
      "Epoch  11674  G loss  0.021544352\n",
      "Epoch  11675  G loss  0.022780055\n",
      "Epoch  11676  G loss  0.0311001\n",
      "Epoch  11677  G loss  0.024742253\n",
      "Epoch  11678  G loss  0.053682122\n",
      "Epoch  11679  G loss  0.0075847907\n",
      "Epoch  11680  G loss  0.045483463\n",
      "Epoch  11681  G loss  0.032178737\n",
      "Epoch  11682  G loss  0.15164432\n",
      "Epoch  11683  G loss  0.0063712103\n",
      "Epoch  11684  G loss  0.010530853\n",
      "Epoch  11685  G loss  0.05647498\n",
      "Epoch  11686  G loss  0.11222616\n",
      "Epoch  11687  G loss  0.053667918\n",
      "Epoch  11688  G loss  0.079379156\n",
      "Epoch  11689  G loss  0.031155122\n",
      "Epoch  11690  G loss  0.20625979\n",
      "Epoch  11691  G loss  0.045550227\n",
      "Epoch  11692  G loss  0.059616916\n",
      "Epoch  11693  G loss  0.006736761\n",
      "Epoch  11694  G loss  0.011302351\n",
      "Epoch  11695  G loss  0.01510117\n",
      "Epoch  11696  G loss  0.012494973\n",
      "Epoch  11697  G loss  0.052038286\n",
      "Epoch  11698  G loss  0.06878947\n",
      "Epoch  11699  G loss  0.074183986\n",
      "Epoch  11700  G loss  0.05582288\n",
      "Epoch  11701  G loss  0.032086972\n",
      "Epoch  11702  G loss  0.011140926\n",
      "Epoch  11703  G loss  0.043698728\n",
      "Epoch  11704  G loss  0.023776492\n",
      "Epoch  11705  G loss  0.01454635\n",
      "Epoch  11706  G loss  0.020212198\n",
      "Epoch  11707  G loss  0.05108711\n",
      "Epoch  11708  G loss  0.053857334\n",
      "Epoch  11709  G loss  0.019317636\n",
      "Epoch  11710  G loss  0.023620691\n",
      "Epoch  11711  G loss  0.012959241\n",
      "Epoch  11712  G loss  0.11402127\n",
      "Epoch  11713  G loss  0.01733709\n",
      "Epoch  11714  G loss  0.06283784\n",
      "Epoch  11715  G loss  0.0463142\n",
      "Epoch  11716  G loss  0.009619735\n",
      "Epoch  11717  G loss  0.090951055\n",
      "Epoch  11718  G loss  0.09128761\n",
      "Epoch  11719  G loss  0.017869797\n",
      "Epoch  11720  G loss  0.07260614\n",
      "Epoch  11721  G loss  0.17238812\n",
      "Epoch  11722  G loss  0.04128518\n",
      "Epoch  11723  G loss  0.0118580945\n",
      "Epoch  11724  G loss  0.071715124\n",
      "Epoch  11725  G loss  0.17958221\n",
      "Epoch  11726  G loss  0.017291097\n",
      "Epoch  11727  G loss  0.046117537\n",
      "Epoch  11728  G loss  0.10496986\n",
      "Epoch  11729  G loss  0.056854323\n",
      "Epoch  11730  G loss  0.011201706\n",
      "Epoch  11731  G loss  0.08543721\n",
      "Epoch  11732  G loss  0.024659768\n",
      "Epoch  11733  G loss  0.02895562\n",
      "Epoch  11734  G loss  0.013707148\n",
      "Epoch  11735  G loss  0.014897658\n",
      "Epoch  11736  G loss  0.0074117077\n",
      "Epoch  11737  G loss  0.007880829\n",
      "Epoch  11738  G loss  0.009257261\n",
      "Epoch  11739  G loss  0.020277658\n",
      "Epoch  11740  G loss  0.053770263\n",
      "Epoch  11741  G loss  0.028138122\n",
      "Epoch  11742  G loss  0.010612812\n",
      "Epoch  11743  G loss  0.013834872\n",
      "Epoch  11744  G loss  0.056731924\n",
      "Epoch  11745  G loss  0.005810487\n",
      "Epoch  11746  G loss  0.016449336\n",
      "Epoch  11747  G loss  0.007276219\n",
      "Epoch  11748  G loss  0.007090502\n",
      "Epoch  11749  G loss  0.13190456\n",
      "Epoch  11750  G loss  0.020026619\n",
      "Epoch  11751  G loss  0.059613958\n",
      "Epoch  11752  G loss  0.025114408\n",
      "Epoch  11753  G loss  0.04050215\n",
      "Epoch  11754  G loss  0.01699363\n",
      "Epoch  11755  G loss  0.04227151\n",
      "Epoch  11756  G loss  0.017756466\n",
      "Epoch  11757  G loss  0.025264125\n",
      "Epoch  11758  G loss  0.0037655202\n",
      "Epoch  11759  G loss  0.013423079\n",
      "Epoch  11760  G loss  0.00879054\n",
      "Epoch  11761  G loss  0.025806569\n",
      "Epoch  11762  G loss  0.17141344\n",
      "Epoch  11763  G loss  0.25324982\n",
      "Epoch  11764  G loss  0.09963043\n",
      "Epoch  11765  G loss  0.0913252\n",
      "Epoch  11766  G loss  0.013957203\n",
      "Epoch  11767  G loss  0.012458682\n",
      "Epoch  11768  G loss  0.014603\n",
      "Epoch  11769  G loss  0.022168353\n",
      "Epoch  11770  G loss  0.006108897\n",
      "Epoch  11771  G loss  0.013345949\n",
      "Epoch  11772  G loss  0.012780337\n",
      "Epoch  11773  G loss  0.0807202\n",
      "Epoch  11774  G loss  0.27886146\n",
      "Epoch  11775  G loss  0.026571821\n",
      "Epoch  11776  G loss  0.057164676\n",
      "Epoch  11777  G loss  0.011153641\n",
      "Epoch  11778  G loss  0.02057864\n",
      "Epoch  11779  G loss  0.022952568\n",
      "Epoch  11780  G loss  0.046905685\n",
      "Epoch  11781  G loss  0.075013354\n",
      "Epoch  11782  G loss  0.01231015\n",
      "Epoch  11783  G loss  0.037573878\n",
      "Epoch  11784  G loss  0.004381209\n",
      "Epoch  11785  G loss  0.013558867\n",
      "Epoch  11786  G loss  0.0053224843\n",
      "Epoch  11787  G loss  0.015437302\n",
      "Epoch  11788  G loss  0.05074712\n",
      "Epoch  11789  G loss  0.033318814\n",
      "Epoch  11790  G loss  0.012626822\n",
      "Epoch  11791  G loss  0.035142504\n"
     ]
    },
    {
>>>>>>> superresolution
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
<<<<<<< HEAD
      "\u001b[0;32m<ipython-input-33-906b9b8bd43f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcgan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCGAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-32-0ee4ffa9b421>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epochs, batch_size, sample_interval)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m             \u001b[0;31m# Generate a half batch of new images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m             \u001b[0mgen_imgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m             \u001b[0;31m# Train the discriminator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1167\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m                                             steps=steps)\n\u001b[0m\u001b[1;32m   1170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
=======
      "\u001b[0;32m<ipython-input-7-a0172846a23b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcgan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCGAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_autoencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_generator_autoencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-9d120fc426dd>\u001b[0m in \u001b[0;36mtrain_generator_autoencoder\u001b[0;34m(self, epochs, batch_size, sample_interval)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmyGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m             \u001b[0mg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m             \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" G loss \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msample_interval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
>>>>>>> superresolution
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
<<<<<<< HEAD
     ]
    }
   ],
   "source": [
    "cgan = CGAN()\n",
    "cgan.train(10000, batch_size=10, sample_interval=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 1, 200)       72000000    input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_9 (InputLayer)            (None, 192, 192, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_7 (Flatten)             (None, 200)          0           embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "sequential_5 (Sequential)       (None, 100)          1520228     input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 100)          20100       flatten_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "multiply_3 (Multiply)           (None, 100)          0           sequential_5[1][0]               \n",
      "                                                                 dense_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 32)           3232        multiply_3[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_27 (LeakyReLU)      (None, 32)           0           dense_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 1)            33          leaky_re_lu_27[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 73,543,593\n",
      "Trainable params: 73,542,697\n",
      "Non-trainable params: 896\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_11 (InputLayer)        (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "embedding_3 (Embedding)      (None, 1, 200)            72000000  \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "sequential_6 (Sequential)    (None, 192, 192, 3)       22608515  \n",
      "=================================================================\n",
      "Total params: 94,608,515\n",
      "Trainable params: 94,607,619\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "None\n"
=======
>>>>>>> superresolution
     ]
    }
   ],
   "source": [
    "cgan = CGAN()\n",
    "cgan.build_autoencoder()\n",
    "cgan.train_generator_autoencoder(100000, 8, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cgan.train(10000, batch_size=10, sample_interval=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cgan2 = CGAN()\n",
    "cgan2.generator.load_weights('../data/weights/generator_weights_3000.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "for i in range(100):\n",
    "    x = myGenerator(1)\n",
    "    xtest, ytest = next(x)\n",
    "    #print('xtrain shape:',xtest.shape)\n",
    "    #print('ytrain shape:',ytest.shape)\n",
    "    pred = cgan.generator.predict(xtest)\n",
    "    pred = pred*127.5 + 127.5\n",
    "    pred = pred.astype(int)\n",
    "    print(pred.dtype)\n",
    "    plt.imshow(pred[0])\n",
    "    plt.show()\n",
    "    ytest = ytest*127.5+127.5\n",
    "    ytest = ytest.astype(int)\n",
    "    plt.imshow(ytest[0])\n",
    "    plt.show()\n",
    "    #break\n",
    "    #time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = cgan.generator.predict(xtest)\n",
    "print(pred[0])"
=======
    "#cgan.build_discriminator()\n",
    "#cgan.build_generator()\n",
    "cgan.train(100000, 6, 100)\n",
    "#cgan.build_autoencoder()\n",
    "#cgan.train_generator_autoencoder(100000, 8, 100)"
>>>>>>> ganvanilla
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
