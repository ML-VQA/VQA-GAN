{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, multiply, GlobalAveragePooling2D, MaxPooling2D\n",
    "from keras.layers import BatchNormalization, Activation, Embedding, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from scipy.misc import imread\n",
    "from scipy.misc import imresize\n",
    "from scipy.misc import imsave\n",
    "import glob\n",
    "import random\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import applications\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import to_categorical\n",
    "#from keras.applications.resnet50 import preprocess_input\n",
    "\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "#from keras.applications.vgg19 import preprocess_input as preprocess_vgg\n",
    "from keras.applications.inception_v3 import preprocess_input as inception_preprocess\n",
    "\n",
    "#from keras.applications.inception_resnet_v2 import preprocess_input\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import keras\n",
    "import copy\n",
    "import cv2\n",
    "\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "\n",
    "import numpy as np \n",
    "import os\n",
    "import numpy as np\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras import backend as keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../data/nparrs/'\n",
    "savepath = '../data/'\n",
    "images = glob.glob(path+'*.npy')\n",
    "totalImages = 1800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:16: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "  app.launch_new_instance()\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:17: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xtrain shape: (10, 192, 192, 3)\n",
      "ytrain shape: (10, 192, 192, 3)\n"
     ]
    }
   ],
   "source": [
    "def myGenerator(batch_size):\n",
    "    while True:\n",
    "        index_list = random.sample(range(1, totalImages), batch_size)\n",
    "        alldata_x = []\n",
    "        alldata_y = []\n",
    "        \n",
    "        for i in index_list:\n",
    "            frame = images[i]\n",
    "            frame = np.load(frame)\n",
    "            \n",
    "            tile_index = np.random.randint(0, 199)\n",
    "            temp       = tile_index*totalImages+i\n",
    "            b          = frame[tile_index][:, :, :]\n",
    "            \n",
    "            alldata_y.append(b)\n",
    "            temp       = imresize(b, (12, 12))\n",
    "            tempa      = imresize(temp, (192, 192))\n",
    "            \n",
    "            alldata_x.append(tempa)\n",
    "        \n",
    "        alldata_x = np.array(alldata_x)\n",
    "        alldata_y = np.array(alldata_y)\n",
    "        \n",
    "        #alldata_y = (alldata_y.astype(np.float32) - 127.5) / 127.5\n",
    "        #alldata_x = (alldata_x.astype(np.float32) - 127.5) / 127.5\n",
    "        yield alldata_x, alldata_y\n",
    "\n",
    "x = myGenerator(10)\n",
    "xtrain, ytrain = next(x)\n",
    "print('xtrain shape:',xtrain.shape)\n",
    "print('ytrain shape:',ytrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CGAN():\n",
    "    def __init__(self):\n",
    "        # Input shape\n",
    "        self.img_rows = 384\n",
    "        self.img_cols = 384\n",
    "        self.channels = 3\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        #self.discriminator = self.build_discriminator()\n",
    "        #self.discriminator.compile(loss=['mse'],\n",
    "        #    optimizer=optimizer,\n",
    "        #    metrics=['accuracy'])\n",
    "        \n",
    "        #print(self.discriminator.summary())\n",
    "        \n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "        \n",
    "        print(self.generator.summary())\n",
    "        \n",
    "        noise = Input(shape=(384, 384, 3))\n",
    "        img = self.generator(noise)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        #self.discriminator.trainable = False\n",
    "\n",
    "        # The discriminator takes generated image as input and determines validity\n",
    "        # and the label of that image\n",
    "        #valid = self.discriminator(img)\n",
    "        \n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains generator to fool discriminator\n",
    "        #self.combined = Model(noise, [img, valid])\n",
    "        #self.combined.compile(loss=['mse', 'mse'],\n",
    "        #    loss_weights=[0.9, 0.1],\n",
    "        #    optimizer=optimizer)\n",
    "\n",
    "    def build_generator(self):\n",
    "        input_size = (384,384,3)\n",
    "        inputs = Input(input_size)\n",
    "\n",
    "        conv1_a = Conv2D(16, 3, padding = 'same', kernel_initializer = 'he_normal')(inputs)\n",
    "        conv1_a = LeakyReLU(alpha=0.2)(conv1_a)\n",
    "        conv1_a = BatchNormalization(momentum=0.8)(conv1_a)\n",
    "\n",
    "        conv1_a = Conv2D(16, 3, padding = 'valid', kernel_initializer = 'he_normal')(conv1_a)\n",
    "        conv1_a = LeakyReLU(alpha=0.2)(conv1_a)\n",
    "        conv1_a = BatchNormalization(momentum=0.8)(conv1_a)\n",
    "\n",
    "        pool1_a = Conv2D(16, 3, subsample=(2, 2), kernel_initializer = 'he_normal')(conv1_a)\n",
    "        pool1_a = LeakyReLU(alpha=0.2)(pool1_a)\n",
    "        pool1_a = BatchNormalization(momentum=0.8)(pool1_a)\n",
    "        pool1_a = ZeroPadding2D(padding=(1, 1))(pool1_a)\n",
    "\n",
    "\n",
    "\n",
    "        conv1 = Conv2D(32, 3, padding = 'same', kernel_initializer = 'he_normal')(pool1_a)\n",
    "        conv1 = LeakyReLU(alpha=0.2)(conv1)\n",
    "        conv1 = BatchNormalization(momentum=0.8)(conv1)\n",
    "\n",
    "        conv1 = Conv2D(32, 3, padding = 'valid', kernel_initializer = 'he_normal')(conv1)\n",
    "        conv1 = LeakyReLU(alpha=0.2)(conv1)\n",
    "        conv1 = BatchNormalization(momentum=0.8)(conv1)\n",
    "\n",
    "        pool1 = Conv2D(32, 3, subsample=(2, 2), kernel_initializer = 'he_normal')(conv1)\n",
    "        pool1 = LeakyReLU(alpha=0.2)(pool1)\n",
    "        pool1 = BatchNormalization(momentum=0.8)(pool1)\n",
    "\n",
    "\n",
    "\n",
    "        conv2 = Conv2D(64, 3, padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
    "        conv2 = LeakyReLU(alpha=0.2)(conv2)\n",
    "        conv2 = BatchNormalization(momentum=0.8)(conv2)\n",
    "\n",
    "        conv2 = Conv2D(64, 3, padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
    "        conv2 = LeakyReLU(alpha=0.2)(conv2)\n",
    "        conv2 = BatchNormalization(momentum=0.8)(conv2)\n",
    "\n",
    "        pool2 = Conv2D(64, 3, subsample=(2, 2), kernel_initializer = 'he_normal')(conv2)\n",
    "        pool2 = LeakyReLU(alpha=0.2)(pool2)\n",
    "        pool2 = BatchNormalization(momentum=0.8)(pool2)\n",
    "\n",
    "\n",
    "\n",
    "        conv2_a = Conv2D(64, 3, padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
    "        conv2_a = LeakyReLU(alpha=0.2)(conv2_a)\n",
    "        conv2_a = BatchNormalization(momentum=0.8)(conv2_a)\n",
    "\n",
    "        conv2_a = Conv2D(64, 3, padding = 'same', kernel_initializer = 'he_normal')(conv2_a)\n",
    "        conv2_a = LeakyReLU(alpha=0.2)(conv2_a)\n",
    "        conv2_a = BatchNormalization(momentum=0.8)(conv2_a)\n",
    "\n",
    "        pool3   = Conv2D(64, 3, subsample=(2, 2), kernel_initializer = 'he_normal')(conv2_a)\n",
    "        pool3   = LeakyReLU(alpha=0.2)(pool3)\n",
    "        pool3   = BatchNormalization(momentum=0.8)(pool3)\n",
    "\n",
    "\n",
    "\n",
    "        conv3 = Conv2D(64, 3, padding = 'same', kernel_initializer = 'he_normal')(pool3)\n",
    "        conv3 = LeakyReLU(alpha=0.2)(conv3)\n",
    "        conv3 = BatchNormalization(momentum=0.8)(conv3)\n",
    "\n",
    "        conv3 = Conv2D(1, 3, padding = 'valid', kernel_initializer = 'he_normal')(conv3)\n",
    "        conv3 = LeakyReLU(alpha=0.2)(conv3)\n",
    "        conv3 = BatchNormalization(momentum=0.8)(conv3)\n",
    "\n",
    "\n",
    "        conv3_pool = Conv2D(1, 3, subsample=(2, 2), kernel_initializer = 'he_normal')(conv3)\n",
    "        conv3_pool = LeakyReLU(alpha=0.2)(conv3_pool)\n",
    "        conv3_pool = BatchNormalization(momentum=0.8)(conv3_pool)\n",
    "\n",
    "        # #Decoder\n",
    "\n",
    "        conv3_unpool = UpSampling2D(size = (2,2))(conv3_pool)\n",
    "        conv3_unpool = ZeroPadding2D(padding=(1, 1))(conv3_unpool)\n",
    "\n",
    "        conv3_unpool = Conv2D(1, 3, padding = 'same', kernel_initializer = 'he_normal')(conv3_unpool)\n",
    "        conv3_unpool = LeakyReLU(alpha=0.2)(conv3_unpool)\n",
    "        conv3_unpool = BatchNormalization(momentum=0.8)(conv3_unpool)\n",
    "\n",
    "\n",
    "        up1    = UpSampling2D(size = (2,2))(conv3_unpool)\n",
    "        #up1    = UpSampling2D(size = (2,2))(conv3)\n",
    "        up1    = ZeroPadding2D(padding=(3, 3))(up1)\n",
    "\n",
    "\n",
    "        conv4 = Conv2D(64, 3, padding = 'same', kernel_initializer = 'he_normal')(up1)\n",
    "        conv4 = LeakyReLU(alpha=0.2)(conv4)\n",
    "        conv4 = BatchNormalization(momentum=0.8)(conv4)\n",
    "\n",
    "        merge1 = concatenate([conv4, conv2_a], axis = 3)\n",
    "\n",
    "        conv5 = Conv2D(64, 3, padding = 'same', kernel_initializer = 'he_normal')(merge1)\n",
    "        conv5 = LeakyReLU(alpha=0.2)(conv5)\n",
    "        conv5 = BatchNormalization(momentum=0.8)(conv5)\n",
    "\n",
    "        conv5 = Conv2D(64, 3, padding = 'same', kernel_initializer = 'he_normal')(conv5)\n",
    "        conv5 = LeakyReLU(alpha=0.2)(conv5)\n",
    "        conv5 = BatchNormalization(momentum=0.8)(conv5)\n",
    "\n",
    "\n",
    "        up2    = UpSampling2D(size = (2,2))(conv5)\n",
    "        up2    = ZeroPadding2D(padding=(1, 1))(up2)\n",
    "\n",
    "        conv6 = Conv2D(64, 3, padding = 'same', kernel_initializer = 'he_normal')(up2)\n",
    "        conv6 = LeakyReLU(alpha=0.2)(conv6)\n",
    "        conv6 = BatchNormalization(momentum=0.8)(conv6)\n",
    "\n",
    "        merge2 = concatenate([conv2, conv6], axis = 3)\n",
    "\n",
    "        conv7 = Conv2D(64, 3, padding = 'same', kernel_initializer = 'he_normal')(merge2)\n",
    "        conv7 = LeakyReLU(alpha=0.2)(conv7)\n",
    "        conv7 = BatchNormalization(momentum=0.8)(conv7)\n",
    "\n",
    "        conv8 = Conv2D(64, 3, padding = 'same', kernel_initializer = 'he_normal')(conv7)\n",
    "        conv8 = LeakyReLU(alpha=0.2)(conv8)\n",
    "        conv8 = BatchNormalization(momentum=0.8)(conv8)\n",
    "\n",
    "        up3    = UpSampling2D(size = (2,2))(conv8)\n",
    "        up3    = ZeroPadding2D(padding=(1, 1))(up3)\n",
    "\n",
    "        merge3 = concatenate([conv1, up3], axis = 3)\n",
    "        merge3 = ZeroPadding2D(padding=(1, 1))(merge3)\n",
    "\n",
    "        conv9 = Conv2D(32, 3, padding = 'same', kernel_initializer = 'he_normal')(merge3)\n",
    "        conv9 = LeakyReLU(alpha=0.2)(conv9)\n",
    "        conv9 = BatchNormalization(momentum=0.8)(conv9)\n",
    "\n",
    "        conv10 = Conv2D(32, 3, padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "        conv10 = LeakyReLU(alpha=0.2)(conv10)\n",
    "        conv10 = BatchNormalization(momentum=0.8)(conv10)\n",
    "\n",
    "\n",
    "        up4    = UpSampling2D(size = (2,2))(conv10)\n",
    "\n",
    "        conv11 = Conv2D(16, 3, padding = 'valid', kernel_initializer = 'he_normal')(up4)\n",
    "        conv11 = LeakyReLU(alpha=0.2)(conv11)\n",
    "        conv11 = BatchNormalization(momentum=0.8)(conv11)\n",
    "\n",
    "        merge4 = concatenate([conv1_a, conv11], axis = 3)\n",
    "        merge4 = ZeroPadding2D(padding=(1, 1))(merge4)\n",
    "\n",
    "        conv12 = Conv2D(16, 3, padding = 'same', kernel_initializer = 'he_normal')(merge4)\n",
    "        conv12 = LeakyReLU(alpha=0.2)(conv12)\n",
    "        conv12 = BatchNormalization(momentum=0.8)(conv12)\n",
    "\n",
    "        conv13 = Conv2D(16, 3, padding = 'same', kernel_initializer = 'he_normal')(conv12)\n",
    "        conv13 = LeakyReLU(alpha=0.2)(conv13)\n",
    "        conv13 = BatchNormalization(momentum=0.8)(conv13)\n",
    "\n",
    "\n",
    "        conv14 = Conv2D(3, 1, padding = 'same', kernel_initializer = 'he_normal', activation='tanh')(conv13)\n",
    "        \n",
    "        model  = Model(input = inputs, output = conv14)\n",
    "        return model\n",
    "    \n",
    "    def build_autoencoder(self):\n",
    "        self.generator.compile(loss=['mse'],optimizer=self.optimizer)\n",
    "    \n",
    "    def train_generator_autoencoder(self, epochs, batch_size=128, sample_interval=10):\n",
    "        for epoch in range(epochs):\n",
    "            X_train, y_train = next(myGenerator(batch_size))\n",
    "            g_loss = self.generator.train_on_batch(X_train, X_train)\n",
    "            print (\"Epoch \", epoch, \" G loss \", g_loss)\n",
    "            if epoch % sample_interval == 0:\n",
    "                self.sample_images(epoch)\n",
    "                self.generator.save_weights(savepath+'weights/generator_weights_'+str(epoch)+'.h5')\n",
    "            \n",
    "    def build_discriminator(self):\n",
    "        img   = Input(shape=(384, 384, 3))\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(64, (3, 3), input_shape=(192, 192, 3)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        #model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(64,  (3, 3),  strides=(2, 2)))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        #model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(64, (3, 3)))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        #model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(64, (6, 6),  strides=(2, 2)))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        #model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(64, (3, 3),  strides=(2, 2)))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Flatten())\n",
    "        \n",
    "        model.add(Dense(100))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        \n",
    "        model.add(Dense(32))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(1))\n",
    "        \n",
    "        output    = model(img)\n",
    "        \n",
    "        model3 = Model(img, output)\n",
    "        return model3\n",
    "\n",
    "    def train(self, epochs, batch_size=128, sample_interval=50):\n",
    "        random.seed(10)\n",
    "        \n",
    "        # Load the dataset\n",
    "        for epoch in range(epochs):\n",
    "            X_train, y_train = next(myGenerator(batch_size))\n",
    "            \n",
    "            # Adversarial ground truths\n",
    "            valid = np.ones((batch_size, 1))\n",
    "            fake  = np.zeros((batch_size, 1))\n",
    "            \n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "            \n",
    "            # Generate a half batch of new images\n",
    "            gen_imgs = self.generator.predict(X_train)\n",
    "\n",
    "            # Train the discriminator\n",
    "            d_loss_real = self.discriminator.train_on_batch(X_train, valid)\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            # Train the generator\n",
    "            g_loss = self.combined.train_on_batch(X_train, [X_train, valid])\n",
    "\n",
    "            # Plot the progress\n",
    "            print (\"%d [D loss: %f, mean_acc: %.2f%% real_acc: %.2f%% fake_acc: %.2f%%] [G loss: %f, mse: %f]\" % (epoch, d_loss[0], 100*d_loss[1], 100*d_loss_real[1], 100*d_loss_fake[1], g_loss[0], g_loss[1]))\n",
    "\n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % sample_interval == 0:\n",
    "                self.sample_images(epoch)\n",
    "                self.generator.save_weights(savepath+'weights/generator_weights_'+str(epoch)+'.h5')\n",
    "                self.discriminator.save_weights(savepath+'weights/discriminator_weights_'+str(epoch)+'.h5')\n",
    "\n",
    "    def sample_images(self, epoch):\n",
    "        r, c             = 1, 10\n",
    "        X_train, y_train = next(myGenerator(10))\n",
    "        gen_imgs         = self.generator.predict(X_train)\n",
    "        \n",
    "        # Rescale images 0 - 1\n",
    "        temp     = (0.5 * gen_imgs + 0.5)*255\n",
    "        gen_imgs = temp.astype(int)\n",
    "        \n",
    "        combined = np.array([gen_imgs[0], gen_imgs[1], gen_imgs[2], gen_imgs[3], gen_imgs[4], gen_imgs[5], gen_imgs[6], gen_imgs[7], gen_imgs[8], gen_imgs[9]])\n",
    "        combined = np.hstack(combined.reshape(10, 384,384, 3))\n",
    "        imsave(savepath+\"images/\"+str(epoch)+\".png\", combined)\n",
    "        \n",
    "        combined = np.array([X_train[0], X_train[1], X_train[2], X_train[3], X_train[4], X_train[5], X_train[6], X_train[7], X_train[8], X_train[9]])\n",
    "        combined = np.hstack(combined.reshape(10, 384,384, 3))\n",
    "        imsave(savepath+\"images/\"+str(epoch)+\"_real.png\", combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# cgan = CGAN()\n",
    "# cgan.build_autoencoder()\n",
    "# cgan.train_generator_autoencoder(1000000, 10, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cgan.train(10000, batch_size=10, sample_interval=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:52: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, 3, kernel_initializer=\"he_normal\", strides=(2, 2))`\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:67: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, 3, kernel_initializer=\"he_normal\", strides=(2, 2))`\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:81: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, 3, kernel_initializer=\"he_normal\", strides=(2, 2))`\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:95: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, 3, kernel_initializer=\"he_normal\", strides=(2, 2))`\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:110: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(1, 3, kernel_initializer=\"he_normal\", strides=(2, 2))`\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:196: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"co..., inputs=Tensor(\"in...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 384, 384, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 384, 384, 16) 448         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 384, 384, 16) 0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 384, 384, 16) 64          leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 382, 382, 16) 2320        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 382, 382, 16) 0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 382, 382, 16) 64          leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 190, 190, 16) 2320        batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 190, 190, 16) 0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 190, 190, 16) 64          leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPadding2D (None, 192, 192, 16) 0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 192, 192, 32) 4640        zero_padding2d_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 192, 192, 32) 0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 192, 192, 32) 128         leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 190, 190, 32) 9248        batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, 190, 190, 32) 0           conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 190, 190, 32) 128         leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 94, 94, 32)   9248        batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)       (None, 94, 94, 32)   0           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 94, 94, 32)   128         leaky_re_lu_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 94, 94, 64)   18496       batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)       (None, 94, 94, 64)   0           conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 94, 94, 64)   256         leaky_re_lu_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 94, 94, 64)   36928       batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)       (None, 94, 94, 64)   0           conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 94, 94, 64)   256         leaky_re_lu_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 46, 46, 64)   36928       batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)       (None, 46, 46, 64)   0           conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 46, 46, 64)   256         leaky_re_lu_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 46, 46, 64)   36928       batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)      (None, 46, 46, 64)   0           conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 46, 46, 64)   256         leaky_re_lu_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 46, 46, 64)   36928       batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)      (None, 46, 46, 64)   0           conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 46, 46, 64)   256         leaky_re_lu_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 22, 22, 64)   36928       batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)      (None, 22, 22, 64)   0           conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 22, 22, 64)   256         leaky_re_lu_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 22, 22, 64)   36928       batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)      (None, 22, 22, 64)   0           conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 22, 22, 64)   256         leaky_re_lu_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 20, 20, 1)    577         batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)      (None, 20, 20, 1)    0           conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 20, 20, 1)    4           leaky_re_lu_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 9, 9, 1)      10          batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)      (None, 9, 9, 1)      0           conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 9, 9, 1)      4           leaky_re_lu_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2D)  (None, 18, 18, 1)    0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_2 (ZeroPadding2D (None, 20, 20, 1)    0           up_sampling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 20, 20, 1)    10          zero_padding2d_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_16 (LeakyReLU)      (None, 20, 20, 1)    0           conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 20, 20, 1)    4           leaky_re_lu_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2D)  (None, 40, 40, 1)    0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_3 (ZeroPadding2D (None, 46, 46, 1)    0           up_sampling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 46, 46, 64)   640         zero_padding2d_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_17 (LeakyReLU)      (None, 46, 46, 64)   0           conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 46, 46, 64)   256         leaky_re_lu_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 46, 46, 128)  0           batch_normalization_17[0][0]     \n",
      "                                                                 batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 46, 46, 64)   73792       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_18 (LeakyReLU)      (None, 46, 46, 64)   0           conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 46, 46, 64)   256         leaky_re_lu_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 46, 46, 64)   36928       batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_19 (LeakyReLU)      (None, 46, 46, 64)   0           conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 46, 46, 64)   256         leaky_re_lu_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2D)  (None, 92, 92, 64)   0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_4 (ZeroPadding2D (None, 94, 94, 64)   0           up_sampling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 94, 94, 64)   36928       zero_padding2d_4[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_20 (LeakyReLU)      (None, 94, 94, 64)   0           conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 94, 94, 64)   256         leaky_re_lu_20[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 94, 94, 128)  0           batch_normalization_8[0][0]      \n",
      "                                                                 batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 94, 94, 64)   73792       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_21 (LeakyReLU)      (None, 94, 94, 64)   0           conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 94, 94, 64)   256         leaky_re_lu_21[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 94, 94, 64)   36928       batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_22 (LeakyReLU)      (None, 94, 94, 64)   0           conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 94, 94, 64)   256         leaky_re_lu_22[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_4 (UpSampling2D)  (None, 188, 188, 64) 0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_5 (ZeroPadding2D (None, 190, 190, 64) 0           up_sampling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 190, 190, 96) 0           batch_normalization_5[0][0]      \n",
      "                                                                 zero_padding2d_5[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_6 (ZeroPadding2D (None, 192, 192, 96) 0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 192, 192, 32) 27680       zero_padding2d_6[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_23 (LeakyReLU)      (None, 192, 192, 32) 0           conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 192, 192, 32) 128         leaky_re_lu_23[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 192, 192, 32) 9248        batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_24 (LeakyReLU)      (None, 192, 192, 32) 0           conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 192, 192, 32) 128         leaky_re_lu_24[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_5 (UpSampling2D)  (None, 384, 384, 32) 0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 382, 382, 16) 4624        up_sampling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_25 (LeakyReLU)      (None, 382, 382, 16) 0           conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 382, 382, 16) 64          leaky_re_lu_25[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 382, 382, 32) 0           batch_normalization_2[0][0]      \n",
      "                                                                 batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_7 (ZeroPadding2D (None, 384, 384, 32) 0           concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 384, 384, 16) 4624        zero_padding2d_7[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_26 (LeakyReLU)      (None, 384, 384, 16) 0           conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 384, 384, 16) 64          leaky_re_lu_26[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 384, 384, 16) 2320        batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_27 (LeakyReLU)      (None, 384, 384, 16) 0           conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 384, 384, 16) 64          leaky_re_lu_27[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 384, 384, 3)  51          batch_normalization_27[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 580,804\n",
      "Trainable params: 578,622\n",
      "Non-trainable params: 2,182\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "cgan2 = CGAN()\n",
    "#cgan2.generator.load_weights('../data/weights/generator_weights_3000.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../data/sources/'\n",
    "def myGenerator(batch_size):\n",
    "    global tdx\n",
    "    while True:\n",
    "        index_list = random.sample(range(1, totalImages), batch_size)\n",
    "        alldata_y = []\n",
    "        for i in range(30, 120):#index_list:\n",
    "            frame = np.load(path+'nparrs_384/frame'+str(i)+'.npy')\n",
    "            alldata_y.append(frame[30])\n",
    "        alldata_y = np.array(alldata_y)\n",
    "        alldata_y = (alldata_y.astype(np.float32) - 127.5) / 127.5\n",
    "        yield alldata_y, alldata_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:15: DeprecationWarning: `imsave` is deprecated!\n",
      "`imsave` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imwrite`` instead.\n",
      "  from ipykernel import kernelapp as app\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:19: DeprecationWarning: `imsave` is deprecated!\n",
      "`imsave` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imwrite`` instead.\n"
     ]
    }
   ],
   "source": [
    "savepath = '../data/sources/'\n",
    "for i in range(1):\n",
    "    x = myGenerator(90)\n",
    "    X_train, ytest = next(x)\n",
    "\n",
    "    gen_imgs         = cgan.generator.predict(X_train)\n",
    "\n",
    "    # Rescale images 0 - 1\n",
    "    temp     = (0.5 * gen_imgs + 0.5)*255\n",
    "    X_train = (0.5 * X_train + 0.5)*255\n",
    "    gen_imgs = temp.astype(int)\n",
    "    \n",
    "    idx = 0\n",
    "    for j in gen_imgs:\n",
    "        imsave(savepath+'prediction/tile_segment_image'+str(idx)+'.png', j)\n",
    "        idx += 1\n",
    "    idx = 0\n",
    "    for j in X_train:\n",
    "        imsave(savepath+'real/tile_segment_image_real'+str(idx)+'.png', j)\n",
    "        idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.467476844787598\n",
      "1.785005807876587\n",
      "1.7195923328399658\n",
      "1.7406260967254639\n",
      "1.7078578472137451\n",
      "1.7259161472320557\n",
      "1.7722742557525635\n",
      "1.7466368675231934\n",
      "1.686011552810669\n",
      "1.7431378364562988\n",
      "2.009453558921814\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "a = []\n",
    "for i in range(10):\n",
    "    stime = time.time()\n",
    "    pred = cgan2.generator.predict(xtrain)\n",
    "    etime = time.time()\n",
    "    tp = etime-stime\n",
    "    a.append(tp)\n",
    "    print(tp)\n",
    "print(np.mean(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
