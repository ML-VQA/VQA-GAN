{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, multiply, GlobalAveragePooling2D, MaxPooling2D\n",
    "from keras.layers import BatchNormalization, Activation, Embedding, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from scipy.misc import imread\n",
    "from scipy.misc import imresize\n",
    "from scipy.misc import imsave\n",
    "import glob\n",
    "import random\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import applications\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import to_categorical\n",
    "#from keras.applications.resnet50 import preprocess_input\n",
    "\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "#from keras.applications.vgg19 import preprocess_input as preprocess_vgg\n",
    "from keras.applications.inception_v3 import preprocess_input as inception_preprocess\n",
    "\n",
    "#from keras.applications.inception_resnet_v2 import preprocess_input\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import keras\n",
    "import copy\n",
    "import cv2\n",
    "\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "\n",
    "import numpy as np \n",
    "import os\n",
    "import numpy as np\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras import backend as keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../data/'\n",
    "savepath = '../data/'\n",
    "#images = glob.glob(path+'new_data/*.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "tile_sizes = np.loadtxt(path+'tile_sizes.txt', dtype='int')\n",
    "tile_sizes = tile_sizes[350:400]\n",
    "images_sampled = {}\n",
    "for tile in tile_sizes:\n",
    "    for i in range(1, 201):\n",
    "        images_sampled.setdefault(tile[0]*30+i, []).append(tile[1])\n",
    "print(len(images_sampled.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myGenerator(batch_size):\n",
    "    s = 640\n",
    "    while True:\n",
    "        index_list = random.sample(images_sampled.keys(), batch_size)\n",
    "        alldata_x = []\n",
    "        alldata_y = []\n",
    "        for i in index_list:\n",
    "            frame = path+'sources/nparrs_384/frame'+str(i)+'.npy'\n",
    "            frame = np.load(frame)\n",
    "            tile_index = random.choice(images_sampled[i])\n",
    "            \n",
    "            tmp = imresize(frame[tile_index], (s, 480))\n",
    "            temp  = imresize(frame[tile_index], (48, 48))\n",
    "            temp  = imresize(temp, (s, 480))\n",
    "            \n",
    "            alldata_x.append(temp)\n",
    "            #alldata_y.append(frame[tile_index])\n",
    "            alldata_y.append(tmp)\n",
    "        \n",
    "        alldata_x = np.array(alldata_x)\n",
    "        alldata_y = np.array(alldata_y)\n",
    "        \n",
    "        alldata_y = (alldata_y.astype(np.float32) - 127.5) / 127.5\n",
    "        alldata_x = alldata_x.astype(np.float32)/255.0\n",
    "        \n",
    "        yield alldata_x, alldata_y\n",
    "\n",
    "#x = myGenerator(10)\n",
    "#xtrain, ytrain = next(x)\n",
    "#print('xtrain shape:',xtrain.shape)\n",
    "#print('ytrain shape:',ytrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CGAN():\n",
    "    def __init__(self):\n",
    "        # Input shape\n",
    "        s = 640\n",
    "        self.img_rows = s\n",
    "        self.img_cols = 480\n",
    "        self.channels = 3\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        #self.discriminator = self.build_discriminator()\n",
    "        #self.discriminator.compile(loss=['mse'],\n",
    "        #    optimizer=optimizer,\n",
    "        #    metrics=['accuracy'])\n",
    "        \n",
    "        #print(self.discriminator.summary())\n",
    "        \n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "        \n",
    "        print(self.generator.summary())\n",
    "        \n",
    "        noise = Input(shape=(self.img_rows, self.img_cols, 3))\n",
    "        img = self.generator(noise)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        #self.discriminator.trainable = False\n",
    "\n",
    "        # The discriminator takes generated image as input and determines validity\n",
    "        # and the label of that image\n",
    "        #valid = self.discriminator(img)\n",
    "        \n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains generator to fool discriminator\n",
    "        #self.combined = Model(noise, [img, valid])\n",
    "        #self.combined.compile(loss=['mse', 'mse'],\n",
    "        #    loss_weights=[0.9, 0.1],\n",
    "        #    optimizer=optimizer)\n",
    "\n",
    "    def build_generator(self):\n",
    "        filter_size = 8\n",
    "        input_size = (self.img_rows, self.img_cols,3)\n",
    "        inputs = Input(input_size)\n",
    "\n",
    "        conv1 = Conv2D(filter_size, 3, padding = 'same', kernel_initializer = 'he_normal')(inputs)\n",
    "        conv1 = LeakyReLU()(conv1)\n",
    "        conv1 = BatchNormalization(momentum=0.8)(conv1)\n",
    "\n",
    "        conv2 = Conv2D(filter_size, 3, padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
    "        conv2 = LeakyReLU()(conv2)\n",
    "        conv2 = BatchNormalization(momentum=0.8)(conv2)\n",
    "\n",
    "        conv3 = Conv2D(filter_size, 3, padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
    "        conv3 = LeakyReLU()(conv3)\n",
    "        conv3 = BatchNormalization(momentum=0.8)(conv3)\n",
    "\n",
    "        concat1 = add([conv1, conv3])\n",
    "\n",
    "        conv4 = Conv2D(filter_size, 3, padding = 'same', kernel_initializer = 'he_normal')(concat1)\n",
    "        conv4 = LeakyReLU()(conv4)\n",
    "        conv4 = BatchNormalization(momentum=0.8)(conv4)\n",
    "\n",
    "        conv5 = Conv2D(filter_size, 3, padding = 'same', kernel_initializer = 'he_normal')(conv4)\n",
    "        conv5 = LeakyReLU()(conv5)\n",
    "        conv5 = BatchNormalization(momentum=0.8)(conv5)\n",
    "\n",
    "        concat2 = add([conv5, concat1])\n",
    "\n",
    "        conv6 = Conv2D(filter_size, 3, padding = 'same', kernel_initializer = 'he_normal')(concat2)\n",
    "        conv6 = LeakyReLU()(conv6)\n",
    "        conv6 = BatchNormalization(momentum=0.8)(conv6)\n",
    "\n",
    "        conv7 = Conv2D(filter_size, 3, padding = 'same', kernel_initializer = 'he_normal')(conv6)\n",
    "        conv7 = LeakyReLU()(conv7)\n",
    "        conv7 = BatchNormalization(momentum=0.8)(conv7)\n",
    "\n",
    "        concat3 = add([conv7, concat2])\n",
    "\n",
    "        conv8 = Conv2D(filter_size, 3, padding = 'same', kernel_initializer = 'he_normal')(concat3)\n",
    "        conv8 = LeakyReLU()(conv8)\n",
    "        conv8 = BatchNormalization(momentum=0.8)(conv8)\n",
    "\n",
    "        conv9 = Conv2D(filter_size, 3, padding = 'same', kernel_initializer = 'he_normal')(conv8)\n",
    "        conv9 = LeakyReLU()(conv9)\n",
    "        conv9 = BatchNormalization(momentum=0.8)(conv9)\n",
    "        \n",
    "\n",
    "        conv10 = Conv2D(filter_size, 3, padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "        conv10 = LeakyReLU()(conv10)\n",
    "        conv10 = BatchNormalization(momentum=0.8)(conv10)\n",
    "\n",
    "        concat4 = add([conv10, conv1])\n",
    "        \n",
    "        conv11 = Conv2D(filter_size, 3, padding = 'same', kernel_initializer = 'he_normal')(concat4)\n",
    "        conv11 = LeakyReLU()(conv11)\n",
    "        conv11 = BatchNormalization(momentum=0.8)(conv11)\n",
    "        \n",
    "        conv12 = Conv2D(filter_size, 3, padding = 'same', kernel_initializer = 'he_normal')(conv11)\n",
    "        conv12 = PReLU()(conv12)\n",
    "        conv12 = BatchNormalization(momentum=0.8)(conv12)\n",
    "\n",
    "\n",
    "        out = Conv2D(3, 3, padding = 'same', kernel_initializer = 'he_normal')(conv12)\n",
    "        out = LeakyReLU()(out)\n",
    "        out = BatchNormalization(momentum=0.8)(out)\n",
    "\n",
    "\n",
    "        model = Model(input = inputs, output = out)\n",
    "        return model\n",
    "    \n",
    "    def build_autoencoder(self):\n",
    "        self.generator.compile(loss=['mse'],optimizer=self.optimizer)\n",
    "    \n",
    "    def train_generator_autoencoder(self, epochs, batch_size=128, sample_interval=10):\n",
    "        for epoch in range(epochs):\n",
    "            X_train, y_train = next(myGenerator(batch_size))\n",
    "            g_loss = self.generator.train_on_batch(X_train, y_train)\n",
    "            print (\"Epoch \", epoch, \" G loss \", g_loss)\n",
    "            #if epoch % sample_interval == 0:\n",
    "                #self.sample_images(epoch)\n",
    "                #self.generator.save_weights(savepath+'Per-segment-models/8/generator_weights_'+str(epoch)+'.h5')\n",
    "\n",
    "    def sample_images(self, epoch):\n",
    "        r, c             = 1, 10\n",
    "        X_train, y_train = next(myGenerator(10))\n",
    "        gen_imgs         = self.generator.predict(X_train)\n",
    "        \n",
    "        # Rescale images 0 - 1\n",
    "        temp     = (0.5 * gen_imgs + 0.5)*255\n",
    "        gen_imgs = temp.astype(int)\n",
    "        y_train = (0.5 * y_train + 0.5)*255\n",
    "        y_train = y_train.astype(int)\n",
    "        X_train = X_train*255\n",
    "        X_train = X_train.astype(int)\n",
    "        \n",
    "        combined = np.array([gen_imgs[0], gen_imgs[1], gen_imgs[2], gen_imgs[3], gen_imgs[4], gen_imgs[5], gen_imgs[6], gen_imgs[7], gen_imgs[8], gen_imgs[9]])\n",
    "        combined = np.hstack(combined.reshape(10, 384,384, 3))\n",
    "        imsave(savepath+\"images/\"+str(epoch)+\".png\", combined)\n",
    "        \n",
    "        combined = np.array([y_train[0], y_train[1], y_train[2], y_train[3], y_train[4], y_train[5], y_train[6], y_train[7], y_train[8], y_train[9]])\n",
    "        combined = np.hstack(combined.reshape(10, 384,384, 3))\n",
    "        imsave(savepath+\"images/\"+str(epoch)+\"_real.png\", combined)\n",
    "        \n",
    "        combined = np.array([X_train[0], X_train[1], X_train[2], X_train[3], X_train[4], X_train[5], X_train[6], X_train[7], X_train[8], X_train[9]])\n",
    "        combined = np.hstack(combined.reshape(10, 384,384, 3))\n",
    "        imsave(savepath+\"images/\"+str(epoch)+\"_lowres.png\", combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:109: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"ba..., inputs=Tensor(\"in...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 192, 384, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_66 (Conv2D)              (None, 192, 384, 8)  224         input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_61 (LeakyReLU)      (None, 192, 384, 8)  0           conv2d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_66 (BatchNo (None, 192, 384, 8)  32          leaky_re_lu_61[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_67 (Conv2D)              (None, 192, 384, 8)  584         batch_normalization_66[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_62 (LeakyReLU)      (None, 192, 384, 8)  0           conv2d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_67 (BatchNo (None, 192, 384, 8)  32          leaky_re_lu_62[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_68 (Conv2D)              (None, 192, 384, 8)  584         batch_normalization_67[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_63 (LeakyReLU)      (None, 192, 384, 8)  0           conv2d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_68 (BatchNo (None, 192, 384, 8)  32          leaky_re_lu_63[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_21 (Add)                    (None, 192, 384, 8)  0           batch_normalization_66[0][0]     \n",
      "                                                                 batch_normalization_68[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_69 (Conv2D)              (None, 192, 384, 8)  584         add_21[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_64 (LeakyReLU)      (None, 192, 384, 8)  0           conv2d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_69 (BatchNo (None, 192, 384, 8)  32          leaky_re_lu_64[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_70 (Conv2D)              (None, 192, 384, 8)  584         batch_normalization_69[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_65 (LeakyReLU)      (None, 192, 384, 8)  0           conv2d_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_70 (BatchNo (None, 192, 384, 8)  32          leaky_re_lu_65[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_22 (Add)                    (None, 192, 384, 8)  0           batch_normalization_70[0][0]     \n",
      "                                                                 add_21[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_71 (Conv2D)              (None, 192, 384, 8)  584         add_22[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_66 (LeakyReLU)      (None, 192, 384, 8)  0           conv2d_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_71 (BatchNo (None, 192, 384, 8)  32          leaky_re_lu_66[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_72 (Conv2D)              (None, 192, 384, 8)  584         batch_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_67 (LeakyReLU)      (None, 192, 384, 8)  0           conv2d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_72 (BatchNo (None, 192, 384, 8)  32          leaky_re_lu_67[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_23 (Add)                    (None, 192, 384, 8)  0           batch_normalization_72[0][0]     \n",
      "                                                                 add_22[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_73 (Conv2D)              (None, 192, 384, 8)  584         add_23[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_68 (LeakyReLU)      (None, 192, 384, 8)  0           conv2d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_73 (BatchNo (None, 192, 384, 8)  32          leaky_re_lu_68[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_74 (Conv2D)              (None, 192, 384, 8)  584         batch_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_69 (LeakyReLU)      (None, 192, 384, 8)  0           conv2d_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_74 (BatchNo (None, 192, 384, 8)  32          leaky_re_lu_69[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_75 (Conv2D)              (None, 192, 384, 8)  584         batch_normalization_74[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_70 (LeakyReLU)      (None, 192, 384, 8)  0           conv2d_75[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_75 (BatchNo (None, 192, 384, 8)  32          leaky_re_lu_70[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_24 (Add)                    (None, 192, 384, 8)  0           batch_normalization_75[0][0]     \n",
      "                                                                 batch_normalization_66[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_76 (Conv2D)              (None, 192, 384, 8)  584         add_24[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_71 (LeakyReLU)      (None, 192, 384, 8)  0           conv2d_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_76 (BatchNo (None, 192, 384, 8)  32          leaky_re_lu_71[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_77 (Conv2D)              (None, 192, 384, 8)  584         batch_normalization_76[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_6 (PReLU)               (None, 192, 384, 8)  589824      conv2d_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_77 (BatchNo (None, 192, 384, 8)  32          p_re_lu_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_78 (Conv2D)              (None, 192, 384, 3)  219         batch_normalization_77[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_72 (LeakyReLU)      (None, 192, 384, 3)  0           conv2d_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_78 (BatchNo (None, 192, 384, 3)  12          leaky_re_lu_72[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 597,087\n",
      "Trainable params: 596,889\n",
      "Non-trainable params: 198\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:12: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "  if sys.path[0] == '':\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:13: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "  del sys.path[0]\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:14: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0  G loss  1.4899393\n",
      "Epoch  1  G loss  0.92509854\n",
      "Epoch  2  G loss  1.2322229\n",
      "Epoch  3  G loss  0.8287456\n",
      "Epoch  4  G loss  1.0241556\n",
      "Epoch  5  G loss  1.3531548\n",
      "Epoch  6  G loss  0.7047219\n",
      "Epoch  7  G loss  1.0165621\n",
      "Epoch  8  G loss  1.1538848\n",
      "Epoch  9  G loss  1.3433421\n",
      "Epoch  10  G loss  0.8691329\n",
      "Epoch  11  G loss  0.6985925\n",
      "Epoch  12  G loss  0.65512615\n",
      "Epoch  13  G loss  0.5341756\n",
      "Epoch  14  G loss  0.52284056\n",
      "Epoch  15  G loss  0.7001317\n",
      "Epoch  16  G loss  1.2985369\n",
      "Epoch  17  G loss  1.013492\n",
      "Epoch  18  G loss  0.537446\n",
      "Epoch  19  G loss  0.8744843\n",
      "Epoch  20  G loss  0.97944736\n",
      "Epoch  21  G loss  0.44668537\n",
      "Epoch  22  G loss  0.372331\n",
      "Epoch  23  G loss  0.7470478\n",
      "Epoch  24  G loss  0.9563553\n"
     ]
    }
   ],
   "source": [
    "cgan = CGAN()\n",
    "cgan.build_autoencoder()\n",
    "cgan.train_generator_autoencoder(25, 2, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:12: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "  if sys.path[0] == '':\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:13: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "  del sys.path[0]\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:14: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.157881021499634\n",
      "0.16921091079711914\n",
      "0.18403935432434082\n",
      "0.1557934284210205\n",
      "0.15225601196289062\n",
      "0.17201876640319824\n",
      "0.16431331634521484\n",
      "0.15370607376098633\n",
      "0.1537480354309082\n",
      "0.15172481536865234\n",
      "0.15171456336975098\n",
      "0.1528174877166748\n",
      "0.1707000732421875\n",
      "0.1662282943725586\n",
      "0.15153241157531738\n",
      "0.15390515327453613\n",
      "0.15278077125549316\n",
      "0.15239548683166504\n",
      "0.15158820152282715\n",
      "0.15132713317871094\n",
      "0.15051984786987305\n",
      "0.15068674087524414\n",
      "0.15129590034484863\n",
      "0.15139412879943848\n",
      "0.1515052318572998\n",
      "0.15105152130126953\n",
      "0.15021991729736328\n",
      "0.1525721549987793\n",
      "0.18861985206604004\n",
      "0.15253019332885742\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "for i in range(30):\n",
    "    x = myGenerator(30)\n",
    "    xtest, ytest = next(x)\n",
    "    a = time.time()\n",
    "    pred = cgan.generator.predict(xtest)\n",
    "    b = time.time()\n",
    "    print(b-a)\n",
    "    pred = pred*255\n",
    "    pred = pred.astype(int)\n",
    "    #plt.imshow(pred[0])\n",
    "    #plt.show()\n",
    "    ytest = ytest*127.5+127.5\n",
    "    ytest = ytest.astype(int)\n",
    "    #plt.imshow(ytest[0])\n",
    "    #plt.show()\n",
    "    #imsave(path+'test/frame_pred'+str(i)+'.png', pred[0])\n",
    "    #imsave(path+'test/frame_real'+str(i)+'.png', ytest[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy \n",
    "import math\n",
    "import cv2\n",
    "\n",
    "def psnr(img1, img2):\n",
    "    mse = numpy.mean( (img1 - img2) ** 2 )\n",
    "    if mse == 0:\n",
    "        return 100\n",
    "    PIXEL_MAX = 255.0\n",
    "    return 20 * math.log10(PIXEL_MAX / math.sqrt(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for i in range(100):\n",
    "    img1 = cv2.imread(path+'test/frame_real'+str(i)+'.png')\n",
    "    img2 = cv2.imread(path+'test/frame_pred'+str(i)+'.png')\n",
    "    d = psnr(img1, img2)\n",
    "    data.append(d)\n",
    "    print(d)\n",
    "print('PSNR:', np.mean(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
