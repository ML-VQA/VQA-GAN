{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, multiply, GlobalAveragePooling2D, MaxPooling2D\n",
    "from keras.layers import BatchNormalization, Activation, Embedding, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from scipy.misc import imread\n",
    "from scipy.misc import imresize\n",
    "from scipy.misc import imsave\n",
    "import glob\n",
    "import random\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import applications\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import to_categorical\n",
    "#from keras.applications.resnet50 import preprocess_input\n",
    "\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "#from keras.applications.vgg19 import preprocess_input as preprocess_vgg\n",
    "from keras.applications.inception_v3 import preprocess_input as inception_preprocess\n",
    "\n",
    "#from keras.applications.inception_resnet_v2 import preprocess_input\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import keras\n",
    "import copy\n",
    "import cv2\n",
    "\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "\n",
    "import numpy as np \n",
    "import os\n",
    "import numpy as np\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras import backend as keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../data/'\n",
    "savepath = '../data/'\n",
    "images = glob.glob(path+'new_data/*.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_sizes = np.loadtxt(path+'tile_sizes.txt', dtype='int')\n",
    "tile_sizes = tile_sizes[350:400]\n",
    "images_sampled = {}\n",
    "for tile in tile_sizes:\n",
    "    for i in range(1, 31):\n",
    "        images_sampled.setdefault(tile[0]*30+i, []).append(tile[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myGenerator(batch_size):\n",
    "    while True:\n",
    "        index_list = random.sample(images_sampled.keys(), batch_size)\n",
    "        alldata_x = []\n",
    "        alldata_y = []\n",
    "        for i in index_list:\n",
    "            frame = path+'sources/nparrs_384/frame'+str(i)+'.npy'\n",
    "            frame = np.load(frame)\n",
    "            tile_index = random.choice(images_sampled[i])\n",
    "            \n",
    "            #tmp = imresize(frame[tile_index], (2*384, 2*384))\n",
    "            temp  = imresize(frame[tile_index], (48, 48))\n",
    "            temp  = imresize(temp, (384, 384))\n",
    "            \n",
    "            alldata_x.append(temp)\n",
    "            alldata_y.append(frame[tile_index])\n",
    "            #alldata_y.append(tmp)\n",
    "        \n",
    "        alldata_x = np.array(alldata_x)\n",
    "        alldata_y = np.array(alldata_y)\n",
    "        \n",
    "        alldata_y = (alldata_y.astype(np.float32) - 127.5) / 127.5\n",
    "        alldata_x = alldata_x.astype(np.float32)/255.0\n",
    "        \n",
    "        yield alldata_x, alldata_y\n",
    "\n",
    "#x = myGenerator(10)\n",
    "#xtrain, ytrain = next(x)\n",
    "#print('xtrain shape:',xtrain.shape)\n",
    "#print('ytrain shape:',ytrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CGAN():\n",
    "    def __init__(self):\n",
    "        # Input shape\n",
    "        self.img_rows = 384\n",
    "        self.img_cols = 384\n",
    "        self.channels = 3\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        #self.discriminator = self.build_discriminator()\n",
    "        #self.discriminator.compile(loss=['mse'],\n",
    "        #    optimizer=optimizer,\n",
    "        #    metrics=['accuracy'])\n",
    "        \n",
    "        #print(self.discriminator.summary())\n",
    "        \n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "        \n",
    "        print(self.generator.summary())\n",
    "        \n",
    "        noise = Input(shape=(self.img_rows, self.img_cols, 3))\n",
    "        img = self.generator(noise)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        #self.discriminator.trainable = False\n",
    "\n",
    "        # The discriminator takes generated image as input and determines validity\n",
    "        # and the label of that image\n",
    "        #valid = self.discriminator(img)\n",
    "        \n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains generator to fool discriminator\n",
    "        #self.combined = Model(noise, [img, valid])\n",
    "        #self.combined.compile(loss=['mse', 'mse'],\n",
    "        #    loss_weights=[0.9, 0.1],\n",
    "        #    optimizer=optimizer)\n",
    "\n",
    "    def build_generator(self):\n",
    "        filter_size = 32\n",
    "        input_size = (self.img_rows, self.img_cols,3)\n",
    "        inputs = Input(input_size)\n",
    "\n",
    "        conv1 = Conv2D(filter_size, 3, padding = 'same', kernel_initializer = 'he_normal')(inputs)\n",
    "        conv1 = LeakyReLU()(conv1)\n",
    "        conv1 = BatchNormalization(momentum=0.8)(conv1)\n",
    "\n",
    "        conv2 = Conv2D(filter_size, 3, padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
    "        conv2 = LeakyReLU()(conv2)\n",
    "        conv2 = BatchNormalization(momentum=0.8)(conv2)\n",
    "\n",
    "        conv3 = Conv2D(filter_size, 3, padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
    "        conv3 = LeakyReLU()(conv3)\n",
    "        conv3 = BatchNormalization(momentum=0.8)(conv3)\n",
    "\n",
    "        concat1 = add([conv1, conv3])\n",
    "\n",
    "        conv4 = Conv2D(filter_size, 3, padding = 'same', kernel_initializer = 'he_normal')(concat1)\n",
    "        conv4 = LeakyReLU()(conv4)\n",
    "        conv4 = BatchNormalization(momentum=0.8)(conv4)\n",
    "\n",
    "        conv5 = Conv2D(filter_size, 3, padding = 'same', kernel_initializer = 'he_normal')(conv4)\n",
    "        conv5 = LeakyReLU()(conv5)\n",
    "        conv5 = BatchNormalization(momentum=0.8)(conv5)\n",
    "\n",
    "        concat2 = add([conv5, concat1])\n",
    "\n",
    "        conv6 = Conv2D(filter_size, 3, padding = 'same', kernel_initializer = 'he_normal')(concat2)\n",
    "        conv6 = LeakyReLU()(conv6)\n",
    "        conv6 = BatchNormalization(momentum=0.8)(conv6)\n",
    "\n",
    "        conv7 = Conv2D(filter_size, 3, padding = 'same', kernel_initializer = 'he_normal')(conv6)\n",
    "        conv7 = LeakyReLU()(conv7)\n",
    "        conv7 = BatchNormalization(momentum=0.8)(conv7)\n",
    "\n",
    "        concat3 = add([conv7, concat2])\n",
    "\n",
    "        conv8 = Conv2D(filter_size, 3, padding = 'same', kernel_initializer = 'he_normal')(concat3)\n",
    "        conv8 = LeakyReLU()(conv8)\n",
    "        conv8 = BatchNormalization(momentum=0.8)(conv8)\n",
    "\n",
    "        conv9 = Conv2D(filter_size, 3, padding = 'same', kernel_initializer = 'he_normal')(conv8)\n",
    "        conv9 = LeakyReLU()(conv9)\n",
    "        conv9 = BatchNormalization(momentum=0.8)(conv9)\n",
    "        \n",
    "\n",
    "        conv10 = Conv2D(filter_size, 3, padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "        conv10 = LeakyReLU()(conv10)\n",
    "        conv10 = BatchNormalization(momentum=0.8)(conv10)\n",
    "\n",
    "        concat4 = add([conv10, conv1])\n",
    "        \n",
    "        conv11 = Conv2D(filter_size, 3, padding = 'same', kernel_initializer = 'he_normal')(concat4)\n",
    "        conv11 = LeakyReLU()(conv11)\n",
    "        conv11 = BatchNormalization(momentum=0.8)(conv11)\n",
    "        \n",
    "        conv12 = Conv2D(filter_size, 3, padding = 'same', kernel_initializer = 'he_normal')(conv11)\n",
    "        conv12 = PReLU()(conv12)\n",
    "        conv12 = BatchNormalization(momentum=0.8)(conv12)\n",
    "\n",
    "\n",
    "        out = Conv2D(3, 3, padding = 'same', kernel_initializer = 'he_normal')(conv12)\n",
    "        out = LeakyReLU()(out)\n",
    "        out = BatchNormalization(momentum=0.8)(out)\n",
    "\n",
    "\n",
    "        model = Model(input = inputs, output = out)\n",
    "        return model\n",
    "    \n",
    "    def build_autoencoder(self):\n",
    "        self.generator.compile(loss=['mse'],optimizer=self.optimizer)\n",
    "    \n",
    "    def train_generator_autoencoder(self, epochs, batch_size=128, sample_interval=10):\n",
    "        for epoch in range(epochs):\n",
    "            X_train, y_train = next(myGenerator(batch_size))\n",
    "            g_loss = self.generator.train_on_batch(X_train, y_train)\n",
    "            print (\"Epoch \", epoch, \" G loss \", g_loss)\n",
    "            #if epoch % sample_interval == 0:\n",
    "                #self.sample_images(epoch)\n",
    "                #self.generator.save_weights(savepath+'Per-segment-models/8/generator_weights_'+str(epoch)+'.h5')\n",
    "\n",
    "    def sample_images(self, epoch):\n",
    "        r, c             = 1, 10\n",
    "        X_train, y_train = next(myGenerator(10))\n",
    "        gen_imgs         = self.generator.predict(X_train)\n",
    "        \n",
    "        # Rescale images 0 - 1\n",
    "        temp     = (0.5 * gen_imgs + 0.5)*255\n",
    "        gen_imgs = temp.astype(int)\n",
    "        y_train = (0.5 * y_train + 0.5)*255\n",
    "        y_train = y_train.astype(int)\n",
    "        X_train = X_train*255\n",
    "        X_train = X_train.astype(int)\n",
    "        \n",
    "        combined = np.array([gen_imgs[0], gen_imgs[1], gen_imgs[2], gen_imgs[3], gen_imgs[4], gen_imgs[5], gen_imgs[6], gen_imgs[7], gen_imgs[8], gen_imgs[9]])\n",
    "        combined = np.hstack(combined.reshape(10, 384,384, 3))\n",
    "        imsave(savepath+\"images/\"+str(epoch)+\".png\", combined)\n",
    "        \n",
    "        combined = np.array([y_train[0], y_train[1], y_train[2], y_train[3], y_train[4], y_train[5], y_train[6], y_train[7], y_train[8], y_train[9]])\n",
    "        combined = np.hstack(combined.reshape(10, 384,384, 3))\n",
    "        imsave(savepath+\"images/\"+str(epoch)+\"_real.png\", combined)\n",
    "        \n",
    "        combined = np.array([X_train[0], X_train[1], X_train[2], X_train[3], X_train[4], X_train[5], X_train[6], X_train[7], X_train[8], X_train[9]])\n",
    "        combined = np.hstack(combined.reshape(10, 384,384, 3))\n",
    "        imsave(savepath+\"images/\"+str(epoch)+\"_lowres.png\", combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:108: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"ba..., inputs=Tensor(\"in...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 768, 768, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 768, 768, 32) 896         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 768, 768, 32) 0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 768, 768, 32) 128         leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 768, 768, 32) 9248        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 768, 768, 32) 0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 768, 768, 32) 128         leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 768, 768, 32) 9248        batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 768, 768, 32) 0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 768, 768, 32) 128         leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 768, 768, 32) 0           batch_normalization_1[0][0]      \n",
      "                                                                 batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 768, 768, 32) 9248        add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 768, 768, 32) 0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 768, 768, 32) 128         leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 768, 768, 32) 9248        batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, 768, 768, 32) 0           conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 768, 768, 32) 128         leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 768, 768, 32) 0           batch_normalization_5[0][0]      \n",
      "                                                                 add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 768, 768, 32) 9248        add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)       (None, 768, 768, 32) 0           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 768, 768, 32) 128         leaky_re_lu_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 768, 768, 32) 9248        batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)       (None, 768, 768, 32) 0           conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 768, 768, 32) 128         leaky_re_lu_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 768, 768, 32) 0           batch_normalization_7[0][0]      \n",
      "                                                                 add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 768, 768, 32) 9248        add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)       (None, 768, 768, 32) 0           conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 768, 768, 32) 128         leaky_re_lu_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 768, 768, 32) 9248        batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)       (None, 768, 768, 32) 0           conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 768, 768, 32) 128         leaky_re_lu_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 768, 768, 32) 9248        batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)      (None, 768, 768, 32) 0           conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 768, 768, 32) 128         leaky_re_lu_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 768, 768, 32) 0           batch_normalization_10[0][0]     \n",
      "                                                                 batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 768, 768, 32) 9248        add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)      (None, 768, 768, 32) 0           conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 768, 768, 32) 128         leaky_re_lu_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 768, 768, 32) 9248        batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_1 (PReLU)               (None, 768, 768, 32) 18874368    conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 768, 768, 32) 128         p_re_lu_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 768, 768, 3)  867         batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)      (None, 768, 768, 3)  0           conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 768, 768, 3)  12          leaky_re_lu_12[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 18,979,407\n",
      "Trainable params: 18,978,633\n",
      "Non-trainable params: 774\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:12: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "  if sys.path[0] == '':\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:13: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected input_1 to have shape (768, 768, 3) but got array with shape (384, 384, 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-1641636ea40f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcgan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCGAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_autoencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_generator_autoencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-2eb618d5a9d0>\u001b[0m in \u001b[0;36mtrain_generator_autoencoder\u001b[0;34m(self, epochs, batch_size, sample_interval)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmyGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0mg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" G loss \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0;31m#if epoch % sample_interval == 0:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1209\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1210\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1211\u001b[0;31m             class_weight=class_weight)\n\u001b[0m\u001b[1;32m   1212\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_uses_dynamic_learning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1213\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    136\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected input_1 to have shape (768, 768, 3) but got array with shape (384, 384, 3)"
     ]
    }
   ],
   "source": [
    "cgan = CGAN()\n",
    "cgan.build_autoencoder()\n",
    "cgan.train_generator_autoencoder(25, 1, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cgan2 = CGAN()\n",
    "cgan2.generator.load_weights('../data/superresolution_48x48/PerSegment-FilterSize8/weights/generator_weights_100000.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "for i in range(10):\n",
    "    x = myGenerator(1)\n",
    "    xtest, ytest = next(x)\n",
    "    a = time.time()\n",
    "    pred = cgan.generator.predict(xtest)\n",
    "    b = time.time()\n",
    "    print(b-a)\n",
    "    pred = pred*255\n",
    "    pred = pred.astype(int)\n",
    "    #plt.imshow(pred[0])\n",
    "    #plt.show()\n",
    "    ytest = ytest*127.5+127.5\n",
    "    ytest = ytest.astype(int)\n",
    "    #plt.imshow(ytest[0])\n",
    "    #plt.show()\n",
    "    #imsave(path+'test/frame_pred'+str(i)+'.png', pred[0])\n",
    "    #imsave(path+'test/frame_real'+str(i)+'.png', ytest[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy \n",
    "import math\n",
    "import cv2\n",
    "\n",
    "def psnr(img1, img2):\n",
    "    mse = numpy.mean( (img1 - img2) ** 2 )\n",
    "    if mse == 0:\n",
    "        return 100\n",
    "    PIXEL_MAX = 255.0\n",
    "    return 20 * math.log10(PIXEL_MAX / math.sqrt(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for i in range(100):\n",
    "    img1 = cv2.imread(path+'test/frame_real'+str(i)+'.png')\n",
    "    img2 = cv2.imread(path+'test/frame_pred'+str(i)+'.png')\n",
    "    d = psnr(img1, img2)\n",
    "    data.append(d)\n",
    "    print(d)\n",
    "print('PSNR:', np.mean(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
