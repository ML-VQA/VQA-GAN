{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, multiply, GlobalAveragePooling2D, MaxPooling2D\n",
    "from keras.layers import BatchNormalization, Activation, Embedding, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from scipy.misc import imread\n",
    "from scipy.misc import imresize\n",
    "from scipy.misc import imsave\n",
    "import glob\n",
    "import random\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import applications\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import to_categorical\n",
    "#from keras.applications.resnet50 import preprocess_input\n",
    "\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "#from keras.applications.vgg19 import preprocess_input as preprocess_vgg\n",
    "from keras.applications.inception_v3 import preprocess_input as inception_preprocess\n",
    "\n",
    "#from keras.applications.inception_resnet_v2 import preprocess_input\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import keras\n",
    "import copy\n",
    "import cv2\n",
    "\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "\n",
    "import numpy as np \n",
    "import os\n",
    "import numpy as np\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras import backend as keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../data/'\n",
    "savepath = '../data/'\n",
    "images = glob.glob(path+'new_data/*.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_sizes = np.loadtxt(path+'tile_sizes.txt', dtype='int')\n",
    "tile_sizes = tile_sizes[:150]\n",
    "images_sampled = {}\n",
    "for tile in tile_sizes:\n",
    "    for i in range(1, 31):\n",
    "        images_sampled.setdefault(tile[0]*30+i, []).append(tile[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myGenerator(batch_size):\n",
    "    while True:\n",
    "        index_list = random.sample(images_sampled.keys(), batch_size)\n",
    "        alldata_x = []\n",
    "        alldata_y = []\n",
    "        for i in index_list:\n",
    "            frame = path+'sources/nparrs_384/frame'+str(i)+'.npy'\n",
    "            frame = np.load(frame)\n",
    "            tile_index = random.choice(images_sampled[i])\n",
    "            \n",
    "            temp  = imresize(frame[tile_index], (48, 48))\n",
    "            temp  = imresize(temp, (384, 384))\n",
    "            \n",
    "            alldata_x.append(temp)\n",
    "            alldata_y.append(frame[tile_index])\n",
    "        \n",
    "        alldata_x = np.array(alldata_x)\n",
    "        alldata_y = np.array(alldata_y)\n",
    "        \n",
    "        alldata_y = (alldata_y.astype(np.float32) - 127.5) / 127.5\n",
    "        alldata_x = alldata_x.astype(np.float32)/255.0\n",
    "        \n",
    "        yield alldata_x, alldata_y\n",
    "\n",
    "#x = myGenerator(10)\n",
    "#xtrain, ytrain = next(x)\n",
    "#print('xtrain shape:',xtrain.shape)\n",
    "#print('ytrain shape:',ytrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CGAN():\n",
    "    def __init__(self):\n",
    "        # Input shape\n",
    "        self.img_rows = 384\n",
    "        self.img_cols = 384\n",
    "        self.channels = 3\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        #self.discriminator = self.build_discriminator()\n",
    "        #self.discriminator.compile(loss=['mse'],\n",
    "        #    optimizer=optimizer,\n",
    "        #    metrics=['accuracy'])\n",
    "        \n",
    "        #print(self.discriminator.summary())\n",
    "        \n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "        \n",
    "        print(self.generator.summary())\n",
    "        \n",
    "        noise = Input(shape=(384, 384, 3))\n",
    "        img = self.generator(noise)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        #self.discriminator.trainable = False\n",
    "\n",
    "        # The discriminator takes generated image as input and determines validity\n",
    "        # and the label of that image\n",
    "        #valid = self.discriminator(img)\n",
    "        \n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains generator to fool discriminator\n",
    "        #self.combined = Model(noise, [img, valid])\n",
    "        #self.combined.compile(loss=['mse', 'mse'],\n",
    "        #    loss_weights=[0.9, 0.1],\n",
    "        #    optimizer=optimizer)\n",
    "\n",
    "    def build_generator(self):\n",
    "        input_size = (384,384,3)\n",
    "        inputs = Input(input_size)\n",
    "\n",
    "\n",
    "        conv1 = Conv2D(8, 3, padding = 'same', kernel_initializer = 'he_normal')(inputs)\n",
    "        conv1 = LeakyReLU()(conv1)\n",
    "        conv1 = BatchNormalization(momentum=0.8)(conv1)\n",
    "\n",
    "\n",
    "\n",
    "        conv2 = Conv2D(8, 3, padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
    "        conv2 = LeakyReLU()(conv2)\n",
    "        conv2 = BatchNormalization(momentum=0.8)(conv2)\n",
    "\n",
    "        conv3 = Conv2D(8, 3, padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
    "        conv3 = LeakyReLU()(conv3)\n",
    "        conv3 = BatchNormalization(momentum=0.8)(conv3)\n",
    "\n",
    "        concat1 = add([conv1, conv3])\n",
    "\n",
    "\n",
    "\n",
    "        conv4 = Conv2D(8, 3, padding = 'same', kernel_initializer = 'he_normal')(concat1)\n",
    "        conv4 = LeakyReLU()(conv4)\n",
    "        conv4 = BatchNormalization(momentum=0.8)(conv4)\n",
    "\n",
    "        conv5 = Conv2D(8, 3, padding = 'same', kernel_initializer = 'he_normal')(conv4)\n",
    "        conv5 = LeakyReLU()(conv5)\n",
    "        conv5 = BatchNormalization(momentum=0.8)(conv5)\n",
    "\n",
    "        concat2 = add([conv5, concat1])\n",
    "\n",
    "\n",
    "\n",
    "        conv6 = Conv2D(8, 3, padding = 'same', kernel_initializer = 'he_normal')(concat2)\n",
    "        conv6 = LeakyReLU()(conv6)\n",
    "        conv6 = BatchNormalization(momentum=0.8)(conv6)\n",
    "\n",
    "        conv7 = Conv2D(8, 3, padding = 'same', kernel_initializer = 'he_normal')(conv6)\n",
    "        conv7 = LeakyReLU()(conv7)\n",
    "        conv7 = BatchNormalization(momentum=0.8)(conv7)\n",
    "\n",
    "        concat3 = add([conv7, concat2])\n",
    "\n",
    "\n",
    "\n",
    "        conv8 = Conv2D(8, 3, padding = 'same', kernel_initializer = 'he_normal')(concat3)\n",
    "        conv8 = LeakyReLU()(conv8)\n",
    "        conv8 = BatchNormalization(momentum=0.8)(conv8)\n",
    "\n",
    "        conv9 = Conv2D(8, 3, padding = 'same', kernel_initializer = 'he_normal')(conv8)\n",
    "        conv9 = LeakyReLU()(conv9)\n",
    "        conv9 = BatchNormalization(momentum=0.8)(conv9)\n",
    "        \n",
    "\n",
    "        conv10 = Conv2D(8, 3, padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "        conv10 = LeakyReLU()(conv10)\n",
    "        conv10 = BatchNormalization(momentum=0.8)(conv10)\n",
    "\n",
    "        concat4 = add([conv10, conv1])\n",
    "        \n",
    "        conv11 = Conv2D(8, 3, padding = 'same', kernel_initializer = 'he_normal')(concat4)\n",
    "        conv11 = LeakyReLU()(conv11)\n",
    "        conv11 = BatchNormalization(momentum=0.8)(conv11)\n",
    "        \n",
    "        conv12 = Conv2D(8, 3, padding = 'same', kernel_initializer = 'he_normal')(conv11)\n",
    "        conv12 = LeakyReLU()(conv12)\n",
    "        conv12 = BatchNormalization(momentum=0.8)(conv12)\n",
    "\n",
    "\n",
    "        out = Conv2D(3, 3, padding = 'same', kernel_initializer = 'he_normal')(conv12)\n",
    "        out = LeakyReLU()(out)\n",
    "        out = BatchNormalization(momentum=0.8)(out)\n",
    "\n",
    "\n",
    "        model = Model(input = inputs, output = out)\n",
    "        return model\n",
    "    \n",
    "    def build_autoencoder(self):\n",
    "        self.generator.compile(loss=['mse'],optimizer=self.optimizer)\n",
    "    \n",
    "    def train_generator_autoencoder(self, epochs, batch_size=128, sample_interval=10):\n",
    "        for epoch in range(epochs):\n",
    "            X_train, y_train = next(myGenerator(batch_size))\n",
    "            g_loss = self.generator.train_on_batch(X_train, y_train)\n",
    "            print (\"Epoch \", epoch, \" G loss \", g_loss)\n",
    "            if epoch % sample_interval == 0:\n",
    "                self.sample_images(epoch)\n",
    "                self.generator.save_weights(savepath+'weights/generator_weights_'+str(epoch)+'.h5')\n",
    "\n",
    "    def sample_images(self, epoch):\n",
    "        r, c             = 1, 10\n",
    "        X_train, y_train = next(myGenerator(10))\n",
    "        gen_imgs         = self.generator.predict(X_train)\n",
    "        \n",
    "        # Rescale images 0 - 1\n",
    "        temp     = (0.5 * gen_imgs + 0.5)*255\n",
    "        gen_imgs = temp.astype(int)\n",
    "        y_train = (0.5 * y_train + 0.5)*255\n",
    "        y_train = y_train.astype(int)\n",
    "        X_train = X_train*255\n",
    "        X_train = X_train.astype(int)\n",
    "        \n",
    "        combined = np.array([gen_imgs[0], gen_imgs[1], gen_imgs[2], gen_imgs[3], gen_imgs[4], gen_imgs[5], gen_imgs[6], gen_imgs[7], gen_imgs[8], gen_imgs[9]])\n",
    "        combined = np.hstack(combined.reshape(10, 384,384, 3))\n",
    "        imsave(savepath+\"images/\"+str(epoch)+\".png\", combined)\n",
    "        \n",
    "        combined = np.array([y_train[0], y_train[1], y_train[2], y_train[3], y_train[4], y_train[5], y_train[6], y_train[7], y_train[8], y_train[9]])\n",
    "        combined = np.hstack(combined.reshape(10, 384,384, 3))\n",
    "        imsave(savepath+\"images/\"+str(epoch)+\"_real.png\", combined)\n",
    "        \n",
    "        combined = np.array([X_train[0], X_train[1], X_train[2], X_train[3], X_train[4], X_train[5], X_train[6], X_train[7], X_train[8], X_train[9]])\n",
    "        combined = np.hstack(combined.reshape(10, 384,384, 3))\n",
    "        imsave(savepath+\"images/\"+str(epoch)+\"_lowres.png\", combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:116: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"ba...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_13 (InputLayer)           (None, 384, 384, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_73 (Conv2D)              (None, 384, 384, 8)  224         input_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_61 (LeakyReLU)      (None, 384, 384, 8)  0           conv2d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_73 (BatchNo (None, 384, 384, 8)  32          leaky_re_lu_61[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_74 (Conv2D)              (None, 384, 384, 8)  584         batch_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_62 (LeakyReLU)      (None, 384, 384, 8)  0           conv2d_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_74 (BatchNo (None, 384, 384, 8)  32          leaky_re_lu_62[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_75 (Conv2D)              (None, 384, 384, 8)  584         batch_normalization_74[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_63 (LeakyReLU)      (None, 384, 384, 8)  0           conv2d_75[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_75 (BatchNo (None, 384, 384, 8)  32          leaky_re_lu_63[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_24 (Add)                    (None, 384, 384, 8)  0           batch_normalization_73[0][0]     \n",
      "                                                                 batch_normalization_75[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_76 (Conv2D)              (None, 384, 384, 8)  584         add_24[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_64 (LeakyReLU)      (None, 384, 384, 8)  0           conv2d_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_76 (BatchNo (None, 384, 384, 8)  32          leaky_re_lu_64[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_77 (Conv2D)              (None, 384, 384, 8)  584         batch_normalization_76[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_65 (LeakyReLU)      (None, 384, 384, 8)  0           conv2d_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_77 (BatchNo (None, 384, 384, 8)  32          leaky_re_lu_65[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_25 (Add)                    (None, 384, 384, 8)  0           batch_normalization_77[0][0]     \n",
      "                                                                 add_24[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_78 (Conv2D)              (None, 384, 384, 8)  584         add_25[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_66 (LeakyReLU)      (None, 384, 384, 8)  0           conv2d_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_78 (BatchNo (None, 384, 384, 8)  32          leaky_re_lu_66[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_79 (Conv2D)              (None, 384, 384, 8)  584         batch_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_67 (LeakyReLU)      (None, 384, 384, 8)  0           conv2d_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_79 (BatchNo (None, 384, 384, 8)  32          leaky_re_lu_67[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_26 (Add)                    (None, 384, 384, 8)  0           batch_normalization_79[0][0]     \n",
      "                                                                 add_25[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_80 (Conv2D)              (None, 384, 384, 8)  584         add_26[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_68 (LeakyReLU)      (None, 384, 384, 8)  0           conv2d_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_80 (BatchNo (None, 384, 384, 8)  32          leaky_re_lu_68[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_81 (Conv2D)              (None, 384, 384, 8)  584         batch_normalization_80[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_69 (LeakyReLU)      (None, 384, 384, 8)  0           conv2d_81[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_81 (BatchNo (None, 384, 384, 8)  32          leaky_re_lu_69[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_82 (Conv2D)              (None, 384, 384, 8)  584         batch_normalization_81[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_70 (LeakyReLU)      (None, 384, 384, 8)  0           conv2d_82[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_82 (BatchNo (None, 384, 384, 8)  32          leaky_re_lu_70[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_27 (Add)                    (None, 384, 384, 8)  0           batch_normalization_82[0][0]     \n",
      "                                                                 batch_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_83 (Conv2D)              (None, 384, 384, 8)  584         add_27[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_71 (LeakyReLU)      (None, 384, 384, 8)  0           conv2d_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_83 (BatchNo (None, 384, 384, 8)  32          leaky_re_lu_71[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_84 (Conv2D)              (None, 384, 384, 8)  584         batch_normalization_83[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_13 (PReLU)              (None, 384, 384, 8)  1179648     conv2d_84[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_84 (BatchNo (None, 384, 384, 8)  32          p_re_lu_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_85 (Conv2D)              (None, 384, 384, 3)  219         batch_normalization_84[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_14 (PReLU)              (None, 384, 384, 3)  442368      conv2d_85[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_85 (BatchNo (None, 384, 384, 3)  12          p_re_lu_14[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 1,629,279\n",
      "Trainable params: 1,629,081\n",
      "Non-trainable params: 198\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:12: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0  G loss  1.9551148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:146: DeprecationWarning: `imsave` is deprecated!\n",
      "`imsave` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imwrite`` instead.\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:150: DeprecationWarning: `imsave` is deprecated!\n",
      "`imsave` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imwrite`` instead.\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:154: DeprecationWarning: `imsave` is deprecated!\n",
      "`imsave` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imwrite`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1  G loss  1.8498034\n",
      "Epoch  2  G loss  1.658897\n",
      "Epoch  3  G loss  1.8109728\n",
      "Epoch  4  G loss  1.744421\n",
      "Epoch  5  G loss  1.6266278\n",
      "Epoch  6  G loss  1.5710392\n",
      "Epoch  7  G loss  1.659986\n",
      "Epoch  8  G loss  1.5975208\n",
      "Epoch  9  G loss  1.5656619\n",
      "Epoch  10  G loss  1.429972\n",
      "Epoch  11  G loss  1.3973808\n",
      "Epoch  12  G loss  1.4935364\n",
      "Epoch  13  G loss  1.3724803\n",
      "Epoch  14  G loss  1.3661588\n",
      "Epoch  15  G loss  1.3849087\n",
      "Epoch  16  G loss  1.2177268\n",
      "Epoch  17  G loss  1.2719144\n",
      "Epoch  18  G loss  1.3102194\n",
      "Epoch  19  G loss  1.0186082\n",
      "Epoch  20  G loss  1.0668095\n",
      "Epoch  21  G loss  0.9718421\n",
      "Epoch  22  G loss  1.2281014\n",
      "Epoch  23  G loss  0.97665304\n",
      "Epoch  24  G loss  1.0915618\n",
      "Epoch  25  G loss  0.9335143\n",
      "Epoch  26  G loss  0.94390696\n",
      "Epoch  27  G loss  0.9265471\n",
      "Epoch  28  G loss  1.1802279\n",
      "Epoch  29  G loss  0.8995126\n",
      "Epoch  30  G loss  0.9896458\n",
      "Epoch  31  G loss  1.0126661\n",
      "Epoch  32  G loss  1.1960102\n",
      "Epoch  33  G loss  0.87481767\n",
      "Epoch  34  G loss  0.8406101\n",
      "Epoch  35  G loss  0.76470685\n",
      "Epoch  36  G loss  0.800275\n",
      "Epoch  37  G loss  0.7173535\n",
      "Epoch  38  G loss  0.8297673\n",
      "Epoch  39  G loss  0.87059766\n",
      "Epoch  40  G loss  0.91500634\n",
      "Epoch  41  G loss  0.7244985\n",
      "Epoch  42  G loss  0.7029875\n",
      "Epoch  43  G loss  0.657109\n",
      "Epoch  44  G loss  0.6657968\n",
      "Epoch  45  G loss  0.8000226\n",
      "Epoch  46  G loss  0.66540366\n",
      "Epoch  47  G loss  0.728388\n",
      "Epoch  48  G loss  0.86488485\n",
      "Epoch  49  G loss  0.67810017\n",
      "Epoch  50  G loss  0.66001916\n",
      "Epoch  51  G loss  0.71211094\n",
      "Epoch  52  G loss  0.92460203\n",
      "Epoch  53  G loss  0.68468183\n",
      "Epoch  54  G loss  0.8192444\n",
      "Epoch  55  G loss  0.67757314\n",
      "Epoch  56  G loss  0.6287096\n",
      "Epoch  57  G loss  0.82713604\n",
      "Epoch  58  G loss  0.69644517\n",
      "Epoch  59  G loss  0.8089752\n",
      "Epoch  60  G loss  0.6560133\n",
      "Epoch  61  G loss  0.66124105\n",
      "Epoch  62  G loss  0.58303666\n",
      "Epoch  63  G loss  0.64160097\n",
      "Epoch  64  G loss  0.6414562\n",
      "Epoch  65  G loss  0.62362313\n",
      "Epoch  66  G loss  0.68313485\n",
      "Epoch  67  G loss  0.6386271\n",
      "Epoch  68  G loss  0.6111801\n",
      "Epoch  69  G loss  0.6096563\n",
      "Epoch  70  G loss  0.82042664\n",
      "Epoch  71  G loss  0.57624006\n",
      "Epoch  72  G loss  0.602078\n",
      "Epoch  73  G loss  0.49364296\n",
      "Epoch  74  G loss  0.61442196\n",
      "Epoch  75  G loss  0.51417834\n",
      "Epoch  76  G loss  0.6554058\n",
      "Epoch  77  G loss  0.4799901\n",
      "Epoch  78  G loss  0.605987\n",
      "Epoch  79  G loss  0.4982334\n",
      "Epoch  80  G loss  0.4472007\n",
      "Epoch  81  G loss  0.37934563\n",
      "Epoch  82  G loss  0.35527083\n",
      "Epoch  83  G loss  0.57437265\n",
      "Epoch  84  G loss  0.47562894\n",
      "Epoch  85  G loss  0.5802035\n",
      "Epoch  86  G loss  0.5345703\n",
      "Epoch  87  G loss  0.4589953\n",
      "Epoch  88  G loss  0.4614825\n",
      "Epoch  89  G loss  0.641752\n",
      "Epoch  90  G loss  0.4757147\n",
      "Epoch  91  G loss  0.36602727\n",
      "Epoch  92  G loss  0.3650452\n",
      "Epoch  93  G loss  0.32445022\n",
      "Epoch  94  G loss  0.44844243\n",
      "Epoch  95  G loss  0.462832\n",
      "Epoch  96  G loss  0.33610773\n",
      "Epoch  97  G loss  0.6809123\n",
      "Epoch  98  G loss  0.6900692\n",
      "Epoch  99  G loss  0.5168511\n",
      "Epoch  100  G loss  0.65979195\n",
      "Epoch  101  G loss  0.39012277\n",
      "Epoch  102  G loss  0.43039396\n",
      "Epoch  103  G loss  0.5335376\n",
      "Epoch  104  G loss  0.62027377\n",
      "Epoch  105  G loss  0.3788905\n",
      "Epoch  106  G loss  0.6135655\n",
      "Epoch  107  G loss  0.40156746\n",
      "Epoch  108  G loss  0.39501026\n",
      "Epoch  109  G loss  0.5314292\n",
      "Epoch  110  G loss  0.6090383\n",
      "Epoch  111  G loss  0.3694074\n",
      "Epoch  112  G loss  0.54152554\n",
      "Epoch  113  G loss  0.3534665\n",
      "Epoch  114  G loss  0.48362732\n",
      "Epoch  115  G loss  0.35894808\n",
      "Epoch  116  G loss  0.34450832\n",
      "Epoch  117  G loss  0.7365779\n",
      "Epoch  118  G loss  0.39043632\n",
      "Epoch  119  G loss  0.45011267\n",
      "Epoch  120  G loss  0.3934587\n",
      "Epoch  121  G loss  0.31468788\n",
      "Epoch  122  G loss  0.33189654\n",
      "Epoch  123  G loss  0.5320373\n",
      "Epoch  124  G loss  0.40753093\n",
      "Epoch  125  G loss  0.43684697\n",
      "Epoch  126  G loss  0.56792\n",
      "Epoch  127  G loss  0.5345915\n",
      "Epoch  128  G loss  0.38865018\n",
      "Epoch  129  G loss  0.4019238\n",
      "Epoch  130  G loss  0.45968726\n",
      "Epoch  131  G loss  0.31259316\n",
      "Epoch  132  G loss  0.28978467\n",
      "Epoch  133  G loss  0.49589658\n",
      "Epoch  134  G loss  0.38297892\n",
      "Epoch  135  G loss  0.4030209\n",
      "Epoch  136  G loss  0.53126645\n",
      "Epoch  137  G loss  0.44992125\n",
      "Epoch  138  G loss  0.4716436\n",
      "Epoch  139  G loss  0.26620513\n",
      "Epoch  140  G loss  0.5410351\n",
      "Epoch  141  G loss  0.2955115\n",
      "Epoch  142  G loss  0.44448784\n",
      "Epoch  143  G loss  0.43030104\n",
      "Epoch  144  G loss  0.3921132\n",
      "Epoch  145  G loss  0.32925773\n",
      "Epoch  146  G loss  0.3224623\n",
      "Epoch  147  G loss  0.3757646\n",
      "Epoch  148  G loss  0.3504612\n",
      "Epoch  149  G loss  0.44916582\n",
      "Epoch  150  G loss  0.42969832\n",
      "Epoch  151  G loss  0.5584105\n",
      "Epoch  152  G loss  0.382399\n",
      "Epoch  153  G loss  0.24329399\n",
      "Epoch  154  G loss  0.28017667\n",
      "Epoch  155  G loss  0.51783377\n",
      "Epoch  156  G loss  0.66798496\n",
      "Epoch  157  G loss  0.32064044\n",
      "Epoch  158  G loss  0.24560976\n",
      "Epoch  159  G loss  0.35298645\n",
      "Epoch  160  G loss  0.52523917\n",
      "Epoch  161  G loss  0.259804\n",
      "Epoch  162  G loss  0.36091948\n",
      "Epoch  163  G loss  0.4020487\n",
      "Epoch  164  G loss  0.28300053\n",
      "Epoch  165  G loss  0.4540801\n",
      "Epoch  166  G loss  0.7693958\n",
      "Epoch  167  G loss  0.33345318\n",
      "Epoch  168  G loss  0.5563217\n",
      "Epoch  169  G loss  0.30440098\n",
      "Epoch  170  G loss  0.28235638\n",
      "Epoch  171  G loss  0.38848007\n",
      "Epoch  172  G loss  0.38786754\n",
      "Epoch  173  G loss  0.22502239\n",
      "Epoch  174  G loss  0.38055757\n",
      "Epoch  175  G loss  0.25231126\n",
      "Epoch  176  G loss  0.46426663\n",
      "Epoch  177  G loss  0.33068946\n",
      "Epoch  178  G loss  0.3691407\n",
      "Epoch  179  G loss  0.36564597\n",
      "Epoch  180  G loss  0.5764954\n",
      "Epoch  181  G loss  0.2799851\n",
      "Epoch  182  G loss  0.42047596\n",
      "Epoch  183  G loss  0.4416889\n",
      "Epoch  184  G loss  0.33574128\n",
      "Epoch  185  G loss  0.2879204\n",
      "Epoch  186  G loss  0.3286238\n",
      "Epoch  187  G loss  0.34351316\n",
      "Epoch  188  G loss  0.27147245\n",
      "Epoch  189  G loss  0.472594\n",
      "Epoch  190  G loss  0.324805\n",
      "Epoch  191  G loss  0.49997973\n",
      "Epoch  192  G loss  0.3951458\n",
      "Epoch  193  G loss  0.40597463\n",
      "Epoch  194  G loss  0.38400033\n",
      "Epoch  195  G loss  0.27325407\n",
      "Epoch  196  G loss  0.26616785\n",
      "Epoch  197  G loss  0.32404444\n",
      "Epoch  198  G loss  0.4922572\n",
      "Epoch  199  G loss  0.25920752\n",
      "Epoch  200  G loss  0.3032025\n",
      "Epoch  201  G loss  0.30080327\n",
      "Epoch  202  G loss  0.36241087\n",
      "Epoch  203  G loss  0.312183\n",
      "Epoch  204  G loss  0.28489515\n",
      "Epoch  205  G loss  0.44608393\n",
      "Epoch  206  G loss  0.26970294\n",
      "Epoch  207  G loss  0.31644562\n",
      "Epoch  208  G loss  0.22936283\n",
      "Epoch  209  G loss  0.63262624\n",
      "Epoch  210  G loss  0.25980294\n",
      "Epoch  211  G loss  0.29424357\n",
      "Epoch  212  G loss  0.25344622\n",
      "Epoch  213  G loss  0.32091603\n",
      "Epoch  214  G loss  0.32327107\n",
      "Epoch  215  G loss  0.26555482\n",
      "Epoch  216  G loss  0.25432017\n",
      "Epoch  217  G loss  0.31785807\n",
      "Epoch  218  G loss  0.33095244\n",
      "Epoch  219  G loss  0.2556202\n",
      "Epoch  220  G loss  0.29279295\n",
      "Epoch  221  G loss  0.32910547\n",
      "Epoch  222  G loss  0.5712498\n",
      "Epoch  223  G loss  0.3068342\n",
      "Epoch  224  G loss  0.4319075\n",
      "Epoch  225  G loss  0.39788666\n",
      "Epoch  226  G loss  0.22835934\n",
      "Epoch  227  G loss  0.26845118\n",
      "Epoch  228  G loss  0.29209942\n",
      "Epoch  229  G loss  0.34917167\n",
      "Epoch  230  G loss  0.28358898\n",
      "Epoch  231  G loss  0.25770485\n",
      "Epoch  232  G loss  0.21974237\n",
      "Epoch  233  G loss  0.2550265\n",
      "Epoch  234  G loss  0.2970874\n",
      "Epoch  235  G loss  0.33496776\n",
      "Epoch  236  G loss  0.4137318\n",
      "Epoch  237  G loss  0.34238347\n",
      "Epoch  238  G loss  0.29993463\n",
      "Epoch  239  G loss  0.34171423\n",
      "Epoch  240  G loss  0.26656768\n",
      "Epoch  241  G loss  0.3424046\n",
      "Epoch  242  G loss  0.37401685\n",
      "Epoch  243  G loss  0.27115673\n",
      "Epoch  244  G loss  0.36017895\n",
      "Epoch  245  G loss  0.2818422\n",
      "Epoch  246  G loss  0.35726508\n",
      "Epoch  247  G loss  0.3328009\n",
      "Epoch  248  G loss  0.29594478\n",
      "Epoch  249  G loss  0.2773169\n",
      "Epoch  250  G loss  0.31929302\n",
      "Epoch  251  G loss  0.31179875\n",
      "Epoch  252  G loss  0.2796915\n",
      "Epoch  253  G loss  0.26588824\n",
      "Epoch  254  G loss  0.30850962\n",
      "Epoch  255  G loss  0.31487575\n",
      "Epoch  256  G loss  0.3774326\n",
      "Epoch  257  G loss  0.3410041\n",
      "Epoch  258  G loss  0.3030574\n",
      "Epoch  259  G loss  0.31006438\n",
      "Epoch  260  G loss  0.22621135\n",
      "Epoch  261  G loss  0.33053222\n",
      "Epoch  262  G loss  0.2871993\n",
      "Epoch  263  G loss  0.25966063\n",
      "Epoch  264  G loss  0.33389902\n",
      "Epoch  265  G loss  0.35569307\n",
      "Epoch  266  G loss  0.2934879\n",
      "Epoch  267  G loss  0.25673088\n",
      "Epoch  268  G loss  0.33716342\n",
      "Epoch  269  G loss  0.34320697\n",
      "Epoch  270  G loss  0.33494464\n",
      "Epoch  271  G loss  0.2787332\n",
      "Epoch  272  G loss  0.27341947\n",
      "Epoch  273  G loss  0.35799074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  274  G loss  0.264428\n",
      "Epoch  275  G loss  0.31585678\n",
      "Epoch  276  G loss  0.2373513\n",
      "Epoch  277  G loss  0.28391886\n",
      "Epoch  278  G loss  0.4085518\n",
      "Epoch  279  G loss  0.31721953\n",
      "Epoch  280  G loss  0.29079446\n",
      "Epoch  281  G loss  0.31298438\n",
      "Epoch  282  G loss  0.30456588\n",
      "Epoch  283  G loss  0.40999714\n",
      "Epoch  284  G loss  0.26034772\n",
      "Epoch  285  G loss  0.32287613\n",
      "Epoch  286  G loss  0.29703447\n",
      "Epoch  287  G loss  0.5027997\n",
      "Epoch  288  G loss  0.187117\n",
      "Epoch  289  G loss  0.26256517\n",
      "Epoch  290  G loss  0.2516081\n",
      "Epoch  291  G loss  0.26514044\n",
      "Epoch  292  G loss  0.26461968\n",
      "Epoch  293  G loss  0.39526877\n",
      "Epoch  294  G loss  0.34233478\n",
      "Epoch  295  G loss  0.23660807\n",
      "Epoch  296  G loss  0.23145412\n",
      "Epoch  297  G loss  0.2249341\n",
      "Epoch  298  G loss  0.31170994\n",
      "Epoch  299  G loss  0.4361769\n",
      "Epoch  300  G loss  0.24173401\n",
      "Epoch  301  G loss  0.29566902\n",
      "Epoch  302  G loss  0.4189341\n",
      "Epoch  303  G loss  0.3570286\n",
      "Epoch  304  G loss  0.30294982\n",
      "Epoch  305  G loss  0.64090794\n",
      "Epoch  306  G loss  0.20090486\n",
      "Epoch  307  G loss  0.21117242\n",
      "Epoch  308  G loss  0.20831466\n",
      "Epoch  309  G loss  0.39054275\n",
      "Epoch  310  G loss  0.262426\n",
      "Epoch  311  G loss  0.1928613\n",
      "Epoch  312  G loss  0.3063114\n",
      "Epoch  313  G loss  0.34975758\n",
      "Epoch  314  G loss  0.32511488\n",
      "Epoch  315  G loss  0.40093255\n",
      "Epoch  316  G loss  0.4911212\n",
      "Epoch  317  G loss  0.44171384\n",
      "Epoch  318  G loss  0.48858723\n",
      "Epoch  319  G loss  0.25420308\n",
      "Epoch  320  G loss  0.2710568\n",
      "Epoch  321  G loss  0.38493133\n",
      "Epoch  322  G loss  0.251097\n",
      "Epoch  323  G loss  0.26057008\n",
      "Epoch  324  G loss  0.53693235\n",
      "Epoch  325  G loss  0.27615932\n",
      "Epoch  326  G loss  0.23918085\n",
      "Epoch  327  G loss  1.2167474\n",
      "Epoch  328  G loss  0.20053525\n",
      "Epoch  329  G loss  0.26403862\n",
      "Epoch  330  G loss  0.3106685\n",
      "Epoch  331  G loss  0.23625493\n",
      "Epoch  332  G loss  0.27179137\n",
      "Epoch  333  G loss  0.33107534\n",
      "Epoch  334  G loss  0.248053\n",
      "Epoch  335  G loss  0.31348518\n",
      "Epoch  336  G loss  0.28547546\n",
      "Epoch  337  G loss  0.31324688\n",
      "Epoch  338  G loss  0.2883048\n",
      "Epoch  339  G loss  0.25362933\n",
      "Epoch  340  G loss  0.25190112\n",
      "Epoch  341  G loss  0.3480195\n",
      "Epoch  342  G loss  0.31168064\n",
      "Epoch  343  G loss  0.33406124\n",
      "Epoch  344  G loss  0.22709747\n",
      "Epoch  345  G loss  0.26832736\n",
      "Epoch  346  G loss  0.23927373\n",
      "Epoch  347  G loss  0.24873133\n",
      "Epoch  348  G loss  0.22349882\n",
      "Epoch  349  G loss  0.21098946\n",
      "Epoch  350  G loss  0.23149954\n",
      "Epoch  351  G loss  0.21701582\n",
      "Epoch  352  G loss  0.23956789\n",
      "Epoch  353  G loss  0.18959779\n",
      "Epoch  354  G loss  0.24569094\n",
      "Epoch  355  G loss  0.22857296\n",
      "Epoch  356  G loss  0.27758688\n",
      "Epoch  357  G loss  0.24666111\n",
      "Epoch  358  G loss  0.29305518\n",
      "Epoch  359  G loss  0.3124757\n",
      "Epoch  360  G loss  0.21064003\n",
      "Epoch  361  G loss  0.29411277\n",
      "Epoch  362  G loss  0.257012\n",
      "Epoch  363  G loss  0.26265416\n",
      "Epoch  364  G loss  0.31917325\n",
      "Epoch  365  G loss  0.23594855\n",
      "Epoch  366  G loss  0.29882333\n",
      "Epoch  367  G loss  0.28136694\n",
      "Epoch  368  G loss  0.3179184\n",
      "Epoch  369  G loss  0.29589704\n",
      "Epoch  370  G loss  0.24287303\n",
      "Epoch  371  G loss  0.30698067\n",
      "Epoch  372  G loss  0.18045722\n",
      "Epoch  373  G loss  0.46992937\n",
      "Epoch  374  G loss  0.19512028\n",
      "Epoch  375  G loss  0.2735224\n",
      "Epoch  376  G loss  0.32879677\n",
      "Epoch  377  G loss  0.3360791\n",
      "Epoch  378  G loss  0.19776304\n",
      "Epoch  379  G loss  0.2782612\n",
      "Epoch  380  G loss  0.33240566\n",
      "Epoch  381  G loss  0.25820458\n",
      "Epoch  382  G loss  0.29937044\n",
      "Epoch  383  G loss  0.1925243\n",
      "Epoch  384  G loss  0.2655772\n",
      "Epoch  385  G loss  0.23562403\n",
      "Epoch  386  G loss  0.23445709\n",
      "Epoch  387  G loss  0.21239178\n",
      "Epoch  388  G loss  0.2966607\n",
      "Epoch  389  G loss  0.27213672\n",
      "Epoch  390  G loss  0.36069143\n",
      "Epoch  391  G loss  0.25304055\n",
      "Epoch  392  G loss  0.26831535\n",
      "Epoch  393  G loss  0.27163735\n",
      "Epoch  394  G loss  0.18819201\n",
      "Epoch  395  G loss  0.25631344\n",
      "Epoch  396  G loss  0.24841143\n",
      "Epoch  397  G loss  0.25486857\n",
      "Epoch  398  G loss  0.38618675\n",
      "Epoch  399  G loss  0.21351404\n",
      "Epoch  400  G loss  0.2611281\n",
      "Epoch  401  G loss  0.17853214\n",
      "Epoch  402  G loss  0.20516734\n",
      "Epoch  403  G loss  0.31173113\n",
      "Epoch  404  G loss  0.234496\n",
      "Epoch  405  G loss  0.31233898\n",
      "Epoch  406  G loss  0.24723752\n",
      "Epoch  407  G loss  0.1969949\n",
      "Epoch  408  G loss  0.28267512\n",
      "Epoch  409  G loss  0.22737741\n",
      "Epoch  410  G loss  0.42728424\n",
      "Epoch  411  G loss  0.24513821\n",
      "Epoch  412  G loss  0.30403894\n",
      "Epoch  413  G loss  0.28244087\n",
      "Epoch  414  G loss  0.36446846\n",
      "Epoch  415  G loss  0.228662\n",
      "Epoch  416  G loss  0.22866459\n",
      "Epoch  417  G loss  0.2549052\n",
      "Epoch  418  G loss  0.21931534\n",
      "Epoch  419  G loss  0.2542546\n",
      "Epoch  420  G loss  0.2998354\n",
      "Epoch  421  G loss  0.24891631\n",
      "Epoch  422  G loss  0.19108969\n",
      "Epoch  423  G loss  0.4917449\n",
      "Epoch  424  G loss  0.2417248\n",
      "Epoch  425  G loss  0.34043622\n",
      "Epoch  426  G loss  0.220793\n",
      "Epoch  427  G loss  0.23181051\n",
      "Epoch  428  G loss  0.37829402\n",
      "Epoch  429  G loss  0.33540258\n",
      "Epoch  430  G loss  0.15681462\n",
      "Epoch  431  G loss  0.20617627\n",
      "Epoch  432  G loss  0.16382134\n",
      "Epoch  433  G loss  0.2604485\n",
      "Epoch  434  G loss  0.30173922\n",
      "Epoch  435  G loss  0.23269986\n",
      "Epoch  436  G loss  0.2311794\n",
      "Epoch  437  G loss  0.22248487\n",
      "Epoch  438  G loss  0.25933656\n",
      "Epoch  439  G loss  0.24803118\n",
      "Epoch  440  G loss  0.4209783\n",
      "Epoch  441  G loss  0.23386574\n",
      "Epoch  442  G loss  0.19544621\n",
      "Epoch  443  G loss  0.2478894\n",
      "Epoch  444  G loss  0.21047081\n",
      "Epoch  445  G loss  0.26146054\n",
      "Epoch  446  G loss  0.30673197\n",
      "Epoch  447  G loss  0.19122864\n",
      "Epoch  448  G loss  0.2287835\n",
      "Epoch  449  G loss  0.2493062\n",
      "Epoch  450  G loss  0.21655583\n",
      "Epoch  451  G loss  0.23117042\n",
      "Epoch  452  G loss  0.20268829\n",
      "Epoch  453  G loss  0.38324964\n",
      "Epoch  454  G loss  0.19050084\n",
      "Epoch  455  G loss  0.32613027\n",
      "Epoch  456  G loss  0.2643217\n",
      "Epoch  457  G loss  0.21067114\n",
      "Epoch  458  G loss  0.3032178\n",
      "Epoch  459  G loss  0.23098136\n",
      "Epoch  460  G loss  0.20654118\n",
      "Epoch  461  G loss  0.17875455\n",
      "Epoch  462  G loss  0.23975904\n",
      "Epoch  463  G loss  0.20533605\n",
      "Epoch  464  G loss  0.2207241\n",
      "Epoch  465  G loss  0.23526919\n",
      "Epoch  466  G loss  0.22107369\n",
      "Epoch  467  G loss  0.21657287\n",
      "Epoch  468  G loss  0.2106049\n",
      "Epoch  469  G loss  0.23539846\n",
      "Epoch  470  G loss  0.54187155\n",
      "Epoch  471  G loss  0.23389141\n",
      "Epoch  472  G loss  0.23507388\n",
      "Epoch  473  G loss  0.280524\n",
      "Epoch  474  G loss  0.2181365\n",
      "Epoch  475  G loss  0.28333464\n",
      "Epoch  476  G loss  0.24430977\n",
      "Epoch  477  G loss  0.31948853\n",
      "Epoch  478  G loss  0.24590397\n",
      "Epoch  479  G loss  0.25947893\n",
      "Epoch  480  G loss  0.25381005\n",
      "Epoch  481  G loss  0.22582467\n",
      "Epoch  482  G loss  0.29240814\n",
      "Epoch  483  G loss  0.26732084\n",
      "Epoch  484  G loss  0.2555163\n",
      "Epoch  485  G loss  0.24733895\n",
      "Epoch  486  G loss  0.24470262\n",
      "Epoch  487  G loss  0.33935818\n",
      "Epoch  488  G loss  0.21366669\n",
      "Epoch  489  G loss  0.1899402\n",
      "Epoch  490  G loss  0.20810027\n",
      "Epoch  491  G loss  0.30820966\n",
      "Epoch  492  G loss  0.20594347\n",
      "Epoch  493  G loss  0.38477913\n",
      "Epoch  494  G loss  0.18840194\n",
      "Epoch  495  G loss  0.21783894\n",
      "Epoch  496  G loss  0.21498322\n",
      "Epoch  497  G loss  0.22453916\n",
      "Epoch  498  G loss  0.27302426\n",
      "Epoch  499  G loss  0.20662868\n",
      "Epoch  500  G loss  0.17944615\n",
      "Epoch  501  G loss  0.25418663\n",
      "Epoch  502  G loss  0.19670475\n",
      "Epoch  503  G loss  0.23476768\n",
      "Epoch  504  G loss  0.19629307\n",
      "Epoch  505  G loss  0.20460407\n",
      "Epoch  506  G loss  0.2500199\n",
      "Epoch  507  G loss  0.41719875\n",
      "Epoch  508  G loss  0.23162343\n",
      "Epoch  509  G loss  0.19460233\n",
      "Epoch  510  G loss  0.2012573\n",
      "Epoch  511  G loss  0.22831118\n",
      "Epoch  512  G loss  0.3084363\n",
      "Epoch  513  G loss  0.2201463\n",
      "Epoch  514  G loss  0.13173695\n",
      "Epoch  515  G loss  0.26860723\n",
      "Epoch  516  G loss  0.4419686\n",
      "Epoch  517  G loss  0.2147852\n",
      "Epoch  518  G loss  0.20150168\n",
      "Epoch  519  G loss  0.20672274\n",
      "Epoch  520  G loss  0.41366073\n",
      "Epoch  521  G loss  0.292188\n",
      "Epoch  522  G loss  0.21966954\n",
      "Epoch  523  G loss  0.26151723\n",
      "Epoch  524  G loss  0.19051953\n",
      "Epoch  525  G loss  0.21893896\n",
      "Epoch  526  G loss  0.2403729\n",
      "Epoch  527  G loss  0.18590824\n",
      "Epoch  528  G loss  0.21070705\n",
      "Epoch  529  G loss  0.3435724\n",
      "Epoch  530  G loss  0.254154\n",
      "Epoch  531  G loss  0.20450686\n",
      "Epoch  532  G loss  0.23432831\n",
      "Epoch  533  G loss  0.18222034\n",
      "Epoch  534  G loss  0.23026137\n",
      "Epoch  535  G loss  0.34991217\n",
      "Epoch  536  G loss  0.20887087\n",
      "Epoch  537  G loss  0.30663314\n",
      "Epoch  538  G loss  0.22327012\n",
      "Epoch  539  G loss  0.16258143\n",
      "Epoch  540  G loss  0.21810676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  541  G loss  0.36536238\n",
      "Epoch  542  G loss  0.2148654\n",
      "Epoch  543  G loss  0.23889565\n",
      "Epoch  544  G loss  0.2648321\n",
      "Epoch  545  G loss  0.19361635\n",
      "Epoch  546  G loss  0.24043356\n",
      "Epoch  547  G loss  0.2799215\n",
      "Epoch  548  G loss  0.21397483\n",
      "Epoch  549  G loss  0.14187595\n",
      "Epoch  550  G loss  0.20456593\n",
      "Epoch  551  G loss  0.21901812\n",
      "Epoch  552  G loss  0.20309205\n",
      "Epoch  553  G loss  0.36468616\n",
      "Epoch  554  G loss  0.25226435\n",
      "Epoch  555  G loss  0.27541706\n",
      "Epoch  556  G loss  0.20055169\n",
      "Epoch  557  G loss  0.2395997\n",
      "Epoch  558  G loss  0.22133756\n",
      "Epoch  559  G loss  0.22553532\n",
      "Epoch  560  G loss  0.17010808\n",
      "Epoch  561  G loss  0.21037169\n",
      "Epoch  562  G loss  0.28436232\n",
      "Epoch  563  G loss  0.1973337\n",
      "Epoch  564  G loss  0.24236245\n",
      "Epoch  565  G loss  0.17848937\n",
      "Epoch  566  G loss  0.16094218\n",
      "Epoch  567  G loss  0.28501987\n",
      "Epoch  568  G loss  0.19740193\n",
      "Epoch  569  G loss  0.19691014\n",
      "Epoch  570  G loss  0.17123878\n",
      "Epoch  571  G loss  0.21543156\n",
      "Epoch  572  G loss  0.18215938\n",
      "Epoch  573  G loss  0.28731447\n",
      "Epoch  574  G loss  0.1875178\n",
      "Epoch  575  G loss  0.21578713\n",
      "Epoch  576  G loss  0.16321449\n",
      "Epoch  577  G loss  0.19148763\n",
      "Epoch  578  G loss  0.1442862\n",
      "Epoch  579  G loss  0.20540936\n",
      "Epoch  580  G loss  0.22291942\n",
      "Epoch  581  G loss  0.24963789\n",
      "Epoch  582  G loss  0.18467999\n",
      "Epoch  583  G loss  0.49776998\n",
      "Epoch  584  G loss  0.18498975\n",
      "Epoch  585  G loss  0.20330139\n",
      "Epoch  586  G loss  0.2584952\n",
      "Epoch  587  G loss  0.20332973\n",
      "Epoch  588  G loss  0.21340178\n",
      "Epoch  589  G loss  0.19871138\n",
      "Epoch  590  G loss  0.21578448\n",
      "Epoch  591  G loss  0.20825262\n",
      "Epoch  592  G loss  0.1515335\n",
      "Epoch  593  G loss  0.20353483\n",
      "Epoch  594  G loss  0.27974513\n",
      "Epoch  595  G loss  0.16830005\n",
      "Epoch  596  G loss  0.27354527\n",
      "Epoch  597  G loss  0.1964895\n",
      "Epoch  598  G loss  0.16405308\n",
      "Epoch  599  G loss  0.24394508\n",
      "Epoch  600  G loss  0.25914395\n",
      "Epoch  601  G loss  0.22690342\n",
      "Epoch  602  G loss  0.19092506\n",
      "Epoch  603  G loss  0.1757118\n",
      "Epoch  604  G loss  0.1684304\n",
      "Epoch  605  G loss  0.2162251\n",
      "Epoch  606  G loss  0.2045898\n",
      "Epoch  607  G loss  0.26277435\n",
      "Epoch  608  G loss  0.21890318\n",
      "Epoch  609  G loss  0.25187272\n",
      "Epoch  610  G loss  0.27522537\n",
      "Epoch  611  G loss  0.17129858\n",
      "Epoch  612  G loss  0.12268367\n",
      "Epoch  613  G loss  0.36597762\n",
      "Epoch  614  G loss  0.15342487\n",
      "Epoch  615  G loss  0.84807235\n",
      "Epoch  616  G loss  0.1856796\n",
      "Epoch  617  G loss  0.24619286\n",
      "Epoch  618  G loss  0.26627156\n",
      "Epoch  619  G loss  0.23098375\n",
      "Epoch  620  G loss  0.206492\n",
      "Epoch  621  G loss  0.39657548\n",
      "Epoch  622  G loss  0.16895111\n",
      "Epoch  623  G loss  0.19192825\n",
      "Epoch  624  G loss  0.18222533\n",
      "Epoch  625  G loss  0.18999867\n",
      "Epoch  626  G loss  0.26124713\n",
      "Epoch  627  G loss  0.19240649\n",
      "Epoch  628  G loss  0.21501802\n",
      "Epoch  629  G loss  0.13496017\n",
      "Epoch  630  G loss  0.19429131\n",
      "Epoch  631  G loss  0.17085259\n",
      "Epoch  632  G loss  0.1603833\n",
      "Epoch  633  G loss  0.21572627\n",
      "Epoch  634  G loss  0.20747705\n",
      "Epoch  635  G loss  0.20790851\n",
      "Epoch  636  G loss  0.20381522\n",
      "Epoch  637  G loss  0.1513402\n",
      "Epoch  638  G loss  0.2786561\n",
      "Epoch  639  G loss  0.21480115\n",
      "Epoch  640  G loss  0.24261642\n",
      "Epoch  641  G loss  0.29118398\n",
      "Epoch  642  G loss  0.21204586\n",
      "Epoch  643  G loss  0.25688913\n",
      "Epoch  644  G loss  0.16741669\n",
      "Epoch  645  G loss  0.187851\n",
      "Epoch  646  G loss  0.16753547\n",
      "Epoch  647  G loss  0.14610204\n",
      "Epoch  648  G loss  0.19559534\n",
      "Epoch  649  G loss  0.17916615\n",
      "Epoch  650  G loss  0.23978166\n",
      "Epoch  651  G loss  0.21056609\n",
      "Epoch  652  G loss  0.2336461\n",
      "Epoch  653  G loss  0.21168773\n",
      "Epoch  654  G loss  0.34886876\n",
      "Epoch  655  G loss  0.19655533\n",
      "Epoch  656  G loss  0.20345187\n",
      "Epoch  657  G loss  0.27356127\n",
      "Epoch  658  G loss  0.15787975\n",
      "Epoch  659  G loss  0.19306223\n",
      "Epoch  660  G loss  0.18666817\n",
      "Epoch  661  G loss  0.19634539\n",
      "Epoch  662  G loss  0.15773381\n",
      "Epoch  663  G loss  0.15620647\n",
      "Epoch  664  G loss  0.22276783\n",
      "Epoch  665  G loss  0.18876082\n",
      "Epoch  666  G loss  0.22412767\n",
      "Epoch  667  G loss  0.15446164\n",
      "Epoch  668  G loss  0.20025015\n",
      "Epoch  669  G loss  0.2519447\n",
      "Epoch  670  G loss  0.1491303\n",
      "Epoch  671  G loss  0.16795988\n",
      "Epoch  672  G loss  0.4136788\n",
      "Epoch  673  G loss  0.38284782\n",
      "Epoch  674  G loss  0.24860592\n",
      "Epoch  675  G loss  0.23548429\n",
      "Epoch  676  G loss  0.17480437\n",
      "Epoch  677  G loss  0.22063105\n",
      "Epoch  678  G loss  0.2145658\n",
      "Epoch  679  G loss  0.20241201\n",
      "Epoch  680  G loss  0.15030117\n",
      "Epoch  681  G loss  0.1827184\n",
      "Epoch  682  G loss  0.18773057\n",
      "Epoch  683  G loss  0.141587\n",
      "Epoch  684  G loss  0.1592799\n",
      "Epoch  685  G loss  0.13796733\n",
      "Epoch  686  G loss  0.23008187\n",
      "Epoch  687  G loss  0.17555279\n",
      "Epoch  688  G loss  0.20080042\n",
      "Epoch  689  G loss  0.17712945\n",
      "Epoch  690  G loss  0.12407098\n",
      "Epoch  691  G loss  0.13967228\n",
      "Epoch  692  G loss  0.14523412\n",
      "Epoch  693  G loss  0.11731436\n",
      "Epoch  694  G loss  0.23899566\n",
      "Epoch  695  G loss  0.12726055\n",
      "Epoch  696  G loss  0.18605037\n",
      "Epoch  697  G loss  0.14187412\n",
      "Epoch  698  G loss  0.13927357\n",
      "Epoch  699  G loss  0.09465919\n",
      "Epoch  700  G loss  0.15597714\n",
      "Epoch  701  G loss  0.15683737\n",
      "Epoch  702  G loss  0.12403524\n",
      "Epoch  703  G loss  0.12980245\n",
      "Epoch  704  G loss  0.14683169\n",
      "Epoch  705  G loss  0.1136669\n",
      "Epoch  706  G loss  0.17630064\n",
      "Epoch  707  G loss  0.1493621\n",
      "Epoch  708  G loss  0.27679604\n",
      "Epoch  709  G loss  0.21207118\n",
      "Epoch  710  G loss  0.16408214\n",
      "Epoch  711  G loss  0.17637913\n",
      "Epoch  712  G loss  0.16050096\n",
      "Epoch  713  G loss  0.24983793\n",
      "Epoch  714  G loss  0.15089555\n",
      "Epoch  715  G loss  0.13549143\n",
      "Epoch  716  G loss  0.092937686\n",
      "Epoch  717  G loss  0.20168906\n",
      "Epoch  718  G loss  0.1236498\n",
      "Epoch  719  G loss  0.11477671\n",
      "Epoch  720  G loss  0.14521587\n",
      "Epoch  721  G loss  0.12298465\n",
      "Epoch  722  G loss  0.20057558\n",
      "Epoch  723  G loss  0.11835303\n",
      "Epoch  724  G loss  0.11249739\n",
      "Epoch  725  G loss  0.13919775\n",
      "Epoch  726  G loss  0.1565923\n",
      "Epoch  727  G loss  0.106768005\n",
      "Epoch  728  G loss  0.11985904\n",
      "Epoch  729  G loss  0.10458814\n",
      "Epoch  730  G loss  0.13274348\n",
      "Epoch  731  G loss  0.091979116\n",
      "Epoch  732  G loss  0.34357002\n",
      "Epoch  733  G loss  0.097425915\n",
      "Epoch  734  G loss  0.15548807\n",
      "Epoch  735  G loss  0.1415035\n",
      "Epoch  736  G loss  0.110081375\n",
      "Epoch  737  G loss  0.15567324\n",
      "Epoch  738  G loss  0.1309012\n",
      "Epoch  739  G loss  0.094683826\n",
      "Epoch  740  G loss  0.15821324\n",
      "Epoch  741  G loss  0.15257573\n",
      "Epoch  742  G loss  0.14975502\n",
      "Epoch  743  G loss  0.1980151\n",
      "Epoch  744  G loss  0.13253962\n",
      "Epoch  745  G loss  0.15484601\n",
      "Epoch  746  G loss  0.2531894\n",
      "Epoch  747  G loss  0.107187055\n",
      "Epoch  748  G loss  0.109820604\n",
      "Epoch  749  G loss  0.12992394\n",
      "Epoch  750  G loss  0.09478039\n",
      "Epoch  751  G loss  0.1255106\n",
      "Epoch  752  G loss  0.08653215\n",
      "Epoch  753  G loss  0.09942182\n",
      "Epoch  754  G loss  0.17459083\n",
      "Epoch  755  G loss  0.1619931\n",
      "Epoch  756  G loss  0.118436396\n",
      "Epoch  757  G loss  0.20991237\n",
      "Epoch  758  G loss  0.124525435\n",
      "Epoch  759  G loss  0.0801709\n",
      "Epoch  760  G loss  0.17267783\n",
      "Epoch  761  G loss  0.12698527\n",
      "Epoch  762  G loss  0.12667958\n",
      "Epoch  763  G loss  0.08502531\n",
      "Epoch  764  G loss  0.13870908\n",
      "Epoch  765  G loss  0.108552925\n",
      "Epoch  766  G loss  0.14847301\n",
      "Epoch  767  G loss  0.1309132\n",
      "Epoch  768  G loss  0.14508289\n",
      "Epoch  769  G loss  0.16276744\n",
      "Epoch  770  G loss  0.13329238\n",
      "Epoch  771  G loss  0.20899393\n",
      "Epoch  772  G loss  0.114599705\n",
      "Epoch  773  G loss  0.17571922\n",
      "Epoch  774  G loss  0.1445958\n",
      "Epoch  775  G loss  0.12534206\n",
      "Epoch  776  G loss  0.17862844\n",
      "Epoch  777  G loss  0.15132001\n",
      "Epoch  778  G loss  0.16253741\n",
      "Epoch  779  G loss  0.10784888\n",
      "Epoch  780  G loss  0.10666308\n",
      "Epoch  781  G loss  0.114574194\n",
      "Epoch  782  G loss  0.093140654\n",
      "Epoch  783  G loss  0.09816883\n",
      "Epoch  784  G loss  0.15174896\n",
      "Epoch  785  G loss  0.3066636\n",
      "Epoch  786  G loss  0.14498904\n",
      "Epoch  787  G loss  0.14281626\n",
      "Epoch  788  G loss  0.1277862\n",
      "Epoch  789  G loss  0.10402584\n",
      "Epoch  790  G loss  0.18386625\n",
      "Epoch  791  G loss  0.10982549\n",
      "Epoch  792  G loss  0.08736777\n",
      "Epoch  793  G loss  0.26742837\n",
      "Epoch  794  G loss  0.11726434\n",
      "Epoch  795  G loss  0.1119776\n",
      "Epoch  796  G loss  0.14389275\n",
      "Epoch  797  G loss  0.15019758\n",
      "Epoch  798  G loss  0.3825786\n",
      "Epoch  799  G loss  0.2108506\n",
      "Epoch  800  G loss  0.12520571\n",
      "Epoch  801  G loss  0.3900119\n",
      "Epoch  802  G loss  0.10586101\n",
      "Epoch  803  G loss  0.09837387\n",
      "Epoch  804  G loss  0.28448597\n",
      "Epoch  805  G loss  0.10397347\n",
      "Epoch  806  G loss  0.11956906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  807  G loss  0.1388364\n",
      "Epoch  808  G loss  0.18226938\n",
      "Epoch  809  G loss  0.08968431\n",
      "Epoch  810  G loss  0.14389426\n",
      "Epoch  811  G loss  0.13337195\n",
      "Epoch  812  G loss  0.40711322\n",
      "Epoch  813  G loss  0.21595581\n",
      "Epoch  814  G loss  0.13638799\n",
      "Epoch  815  G loss  0.14147471\n",
      "Epoch  816  G loss  0.35633758\n",
      "Epoch  817  G loss  0.13394201\n",
      "Epoch  818  G loss  0.1021317\n",
      "Epoch  819  G loss  0.09122317\n",
      "Epoch  820  G loss  0.16066249\n",
      "Epoch  821  G loss  0.18623507\n",
      "Epoch  822  G loss  0.11429423\n",
      "Epoch  823  G loss  0.09489575\n",
      "Epoch  824  G loss  0.20918916\n",
      "Epoch  825  G loss  0.12040477\n",
      "Epoch  826  G loss  0.11933715\n",
      "Epoch  827  G loss  0.21124668\n",
      "Epoch  828  G loss  0.0777617\n",
      "Epoch  829  G loss  0.14727278\n",
      "Epoch  830  G loss  0.11825364\n",
      "Epoch  831  G loss  0.16112863\n",
      "Epoch  832  G loss  0.106106065\n",
      "Epoch  833  G loss  0.15231268\n",
      "Epoch  834  G loss  0.55150324\n",
      "Epoch  835  G loss  0.15008913\n",
      "Epoch  836  G loss  0.10054328\n",
      "Epoch  837  G loss  0.11046016\n",
      "Epoch  838  G loss  0.14779146\n",
      "Epoch  839  G loss  0.11720371\n",
      "Epoch  840  G loss  0.1567728\n",
      "Epoch  841  G loss  0.13259584\n",
      "Epoch  842  G loss  0.092576355\n",
      "Epoch  843  G loss  0.103367954\n",
      "Epoch  844  G loss  0.15661347\n",
      "Epoch  845  G loss  0.15870084\n",
      "Epoch  846  G loss  0.11988873\n",
      "Epoch  847  G loss  0.12828596\n",
      "Epoch  848  G loss  0.18410207\n",
      "Epoch  849  G loss  0.12142277\n",
      "Epoch  850  G loss  0.13324149\n",
      "Epoch  851  G loss  0.2792929\n",
      "Epoch  852  G loss  0.2006774\n",
      "Epoch  853  G loss  0.11398929\n",
      "Epoch  854  G loss  0.17534022\n",
      "Epoch  855  G loss  0.08628712\n",
      "Epoch  856  G loss  0.10214711\n",
      "Epoch  857  G loss  0.13456684\n",
      "Epoch  858  G loss  0.08137215\n",
      "Epoch  859  G loss  0.104306616\n",
      "Epoch  860  G loss  0.14909157\n",
      "Epoch  861  G loss  0.08900648\n",
      "Epoch  862  G loss  0.12729283\n",
      "Epoch  863  G loss  0.11103038\n",
      "Epoch  864  G loss  0.2561995\n",
      "Epoch  865  G loss  0.18954222\n",
      "Epoch  866  G loss  0.17133307\n",
      "Epoch  867  G loss  0.08668542\n",
      "Epoch  868  G loss  0.1581181\n",
      "Epoch  869  G loss  0.5640888\n",
      "Epoch  870  G loss  0.088075764\n",
      "Epoch  871  G loss  0.24008958\n",
      "Epoch  872  G loss  0.08630129\n",
      "Epoch  873  G loss  0.16711555\n",
      "Epoch  874  G loss  0.23618479\n",
      "Epoch  875  G loss  0.10251939\n",
      "Epoch  876  G loss  0.1069158\n",
      "Epoch  877  G loss  0.18130672\n",
      "Epoch  878  G loss  0.09717607\n",
      "Epoch  879  G loss  0.19921237\n",
      "Epoch  880  G loss  0.113421135\n",
      "Epoch  881  G loss  0.1711152\n",
      "Epoch  882  G loss  0.24150956\n",
      "Epoch  883  G loss  0.14512734\n",
      "Epoch  884  G loss  0.18769364\n",
      "Epoch  885  G loss  0.14456058\n",
      "Epoch  886  G loss  0.14945537\n",
      "Epoch  887  G loss  0.14407295\n",
      "Epoch  888  G loss  0.16169484\n",
      "Epoch  889  G loss  0.17661738\n",
      "Epoch  890  G loss  0.15420763\n",
      "Epoch  891  G loss  0.11809787\n",
      "Epoch  892  G loss  0.11786901\n",
      "Epoch  893  G loss  0.09743186\n",
      "Epoch  894  G loss  0.09133625\n",
      "Epoch  895  G loss  0.101416945\n",
      "Epoch  896  G loss  0.123191595\n",
      "Epoch  897  G loss  0.090070106\n",
      "Epoch  898  G loss  0.09066832\n",
      "Epoch  899  G loss  0.13651885\n",
      "Epoch  900  G loss  0.07997154\n",
      "Epoch  901  G loss  0.124205135\n",
      "Epoch  902  G loss  0.2238599\n",
      "Epoch  903  G loss  0.08698218\n",
      "Epoch  904  G loss  0.13358097\n",
      "Epoch  905  G loss  0.15479162\n",
      "Epoch  906  G loss  0.07501434\n",
      "Epoch  907  G loss  0.15290366\n",
      "Epoch  908  G loss  0.18403286\n",
      "Epoch  909  G loss  0.18260969\n",
      "Epoch  910  G loss  0.2391692\n",
      "Epoch  911  G loss  0.17589718\n",
      "Epoch  912  G loss  0.12035873\n",
      "Epoch  913  G loss  0.14559922\n",
      "Epoch  914  G loss  0.11893704\n",
      "Epoch  915  G loss  0.14944299\n",
      "Epoch  916  G loss  0.119183995\n",
      "Epoch  917  G loss  0.12251866\n",
      "Epoch  918  G loss  0.18162338\n",
      "Epoch  919  G loss  0.0944468\n",
      "Epoch  920  G loss  0.19949289\n",
      "Epoch  921  G loss  0.11570034\n",
      "Epoch  922  G loss  0.12650742\n",
      "Epoch  923  G loss  0.09959174\n",
      "Epoch  924  G loss  0.10872766\n",
      "Epoch  925  G loss  0.07199762\n",
      "Epoch  926  G loss  0.10065013\n",
      "Epoch  927  G loss  0.087587975\n",
      "Epoch  928  G loss  0.14317647\n",
      "Epoch  929  G loss  0.07244589\n",
      "Epoch  930  G loss  0.35183048\n",
      "Epoch  931  G loss  0.06963094\n",
      "Epoch  932  G loss  0.16148753\n",
      "Epoch  933  G loss  0.11400043\n",
      "Epoch  934  G loss  0.12430722\n",
      "Epoch  935  G loss  0.08049634\n",
      "Epoch  936  G loss  0.14916293\n",
      "Epoch  937  G loss  0.11383603\n",
      "Epoch  938  G loss  0.10542496\n",
      "Epoch  939  G loss  0.14626439\n",
      "Epoch  940  G loss  0.08789221\n",
      "Epoch  941  G loss  0.12820733\n",
      "Epoch  942  G loss  0.24356015\n",
      "Epoch  943  G loss  0.124167316\n",
      "Epoch  944  G loss  0.09359014\n",
      "Epoch  945  G loss  0.39679363\n",
      "Epoch  946  G loss  0.2231692\n",
      "Epoch  947  G loss  0.12377292\n",
      "Epoch  948  G loss  0.21055906\n",
      "Epoch  949  G loss  0.10483555\n",
      "Epoch  950  G loss  0.12401786\n",
      "Epoch  951  G loss  0.094718255\n",
      "Epoch  952  G loss  0.12137434\n",
      "Epoch  953  G loss  0.1122883\n",
      "Epoch  954  G loss  0.12231698\n",
      "Epoch  955  G loss  0.087201096\n",
      "Epoch  956  G loss  0.16501783\n",
      "Epoch  957  G loss  0.09380182\n",
      "Epoch  958  G loss  0.0988277\n",
      "Epoch  959  G loss  0.082626395\n",
      "Epoch  960  G loss  0.09849616\n",
      "Epoch  961  G loss  0.13171673\n",
      "Epoch  962  G loss  0.11915112\n",
      "Epoch  963  G loss  0.08831792\n",
      "Epoch  964  G loss  0.13796426\n",
      "Epoch  965  G loss  0.07794998\n",
      "Epoch  966  G loss  0.10985895\n",
      "Epoch  967  G loss  0.09537986\n",
      "Epoch  968  G loss  0.06919916\n",
      "Epoch  969  G loss  0.11927587\n",
      "Epoch  970  G loss  0.10493288\n",
      "Epoch  971  G loss  0.09542302\n",
      "Epoch  972  G loss  0.11705994\n",
      "Epoch  973  G loss  0.12153617\n",
      "Epoch  974  G loss  0.114035934\n",
      "Epoch  975  G loss  0.17677931\n",
      "Epoch  976  G loss  0.11487276\n",
      "Epoch  977  G loss  0.10278896\n",
      "Epoch  978  G loss  0.14913215\n",
      "Epoch  979  G loss  0.18389137\n",
      "Epoch  980  G loss  0.12220535\n",
      "Epoch  981  G loss  0.12825978\n",
      "Epoch  982  G loss  0.14089425\n",
      "Epoch  983  G loss  0.17733653\n",
      "Epoch  984  G loss  0.11145913\n",
      "Epoch  985  G loss  0.13500379\n",
      "Epoch  986  G loss  0.09869561\n",
      "Epoch  987  G loss  0.08276229\n",
      "Epoch  988  G loss  0.17972045\n",
      "Epoch  989  G loss  0.116547644\n",
      "Epoch  990  G loss  0.19064756\n",
      "Epoch  991  G loss  0.18026876\n",
      "Epoch  992  G loss  0.105793454\n",
      "Epoch  993  G loss  0.09398655\n",
      "Epoch  994  G loss  0.0968676\n",
      "Epoch  995  G loss  0.07760818\n",
      "Epoch  996  G loss  0.10034019\n",
      "Epoch  997  G loss  0.13454504\n",
      "Epoch  998  G loss  0.09040159\n",
      "Epoch  999  G loss  0.103636526\n",
      "Epoch  1000  G loss  0.088814996\n",
      "Epoch  1001  G loss  0.086390354\n",
      "Epoch  1002  G loss  0.15664689\n",
      "Epoch  1003  G loss  0.112446554\n",
      "Epoch  1004  G loss  0.07865014\n",
      "Epoch  1005  G loss  0.08963412\n",
      "Epoch  1006  G loss  0.1687014\n",
      "Epoch  1007  G loss  0.083892375\n",
      "Epoch  1008  G loss  0.19219102\n",
      "Epoch  1009  G loss  0.075636335\n",
      "Epoch  1010  G loss  0.140482\n",
      "Epoch  1011  G loss  0.2333108\n",
      "Epoch  1012  G loss  0.19204979\n",
      "Epoch  1013  G loss  0.14175251\n",
      "Epoch  1014  G loss  0.07294937\n",
      "Epoch  1015  G loss  0.10663745\n",
      "Epoch  1016  G loss  0.08036288\n",
      "Epoch  1017  G loss  0.13148773\n",
      "Epoch  1018  G loss  0.18024808\n",
      "Epoch  1019  G loss  0.14661418\n",
      "Epoch  1020  G loss  0.11963167\n",
      "Epoch  1021  G loss  0.061991315\n",
      "Epoch  1022  G loss  0.10868966\n",
      "Epoch  1023  G loss  0.11602137\n",
      "Epoch  1024  G loss  0.20793684\n",
      "Epoch  1025  G loss  0.14002018\n",
      "Epoch  1026  G loss  0.12681635\n",
      "Epoch  1027  G loss  0.0862546\n",
      "Epoch  1028  G loss  0.12212352\n",
      "Epoch  1029  G loss  0.09157253\n",
      "Epoch  1030  G loss  0.16387004\n",
      "Epoch  1031  G loss  0.1424378\n",
      "Epoch  1032  G loss  0.09425935\n",
      "Epoch  1033  G loss  0.11686369\n",
      "Epoch  1034  G loss  0.08417197\n",
      "Epoch  1035  G loss  0.11115954\n",
      "Epoch  1036  G loss  0.11712027\n",
      "Epoch  1037  G loss  0.085025646\n",
      "Epoch  1038  G loss  0.083931245\n",
      "Epoch  1039  G loss  0.08734271\n",
      "Epoch  1040  G loss  0.080073126\n",
      "Epoch  1041  G loss  0.08472407\n",
      "Epoch  1042  G loss  0.065220535\n",
      "Epoch  1043  G loss  0.08059981\n",
      "Epoch  1044  G loss  0.1317007\n",
      "Epoch  1045  G loss  0.08391925\n",
      "Epoch  1046  G loss  0.095611244\n",
      "Epoch  1047  G loss  0.08594067\n",
      "Epoch  1048  G loss  0.07580376\n",
      "Epoch  1049  G loss  0.097901724\n",
      "Epoch  1050  G loss  0.09552458\n",
      "Epoch  1051  G loss  0.12884995\n",
      "Epoch  1052  G loss  0.0777404\n",
      "Epoch  1053  G loss  0.16685681\n",
      "Epoch  1054  G loss  0.091736615\n",
      "Epoch  1055  G loss  0.110536754\n",
      "Epoch  1056  G loss  0.14336036\n",
      "Epoch  1057  G loss  0.08729175\n",
      "Epoch  1058  G loss  0.13635461\n",
      "Epoch  1059  G loss  0.12364495\n",
      "Epoch  1060  G loss  0.099838935\n",
      "Epoch  1061  G loss  0.11488054\n",
      "Epoch  1062  G loss  0.110756196\n",
      "Epoch  1063  G loss  0.08269629\n",
      "Epoch  1064  G loss  0.056961205\n",
      "Epoch  1065  G loss  0.25991628\n",
      "Epoch  1066  G loss  0.070378125\n",
      "Epoch  1067  G loss  0.082824565\n",
      "Epoch  1068  G loss  0.11686188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1069  G loss  0.07940729\n",
      "Epoch  1070  G loss  0.070020325\n",
      "Epoch  1071  G loss  0.1219815\n",
      "Epoch  1072  G loss  0.07731789\n",
      "Epoch  1073  G loss  0.09351811\n",
      "Epoch  1074  G loss  0.14272945\n",
      "Epoch  1075  G loss  0.11062497\n",
      "Epoch  1076  G loss  0.20995645\n",
      "Epoch  1077  G loss  0.08426472\n",
      "Epoch  1078  G loss  0.10587996\n",
      "Epoch  1079  G loss  0.13290887\n",
      "Epoch  1080  G loss  0.1868434\n",
      "Epoch  1081  G loss  0.11981416\n",
      "Epoch  1082  G loss  0.089167215\n",
      "Epoch  1083  G loss  0.08081806\n",
      "Epoch  1084  G loss  0.08379784\n",
      "Epoch  1085  G loss  0.15765937\n",
      "Epoch  1086  G loss  0.13984945\n",
      "Epoch  1087  G loss  0.088662386\n",
      "Epoch  1088  G loss  0.16182368\n",
      "Epoch  1089  G loss  0.14414917\n",
      "Epoch  1090  G loss  0.08236719\n",
      "Epoch  1091  G loss  0.077134974\n",
      "Epoch  1092  G loss  0.07604338\n",
      "Epoch  1093  G loss  0.06908281\n",
      "Epoch  1094  G loss  0.07996224\n",
      "Epoch  1095  G loss  0.07981559\n",
      "Epoch  1096  G loss  0.13920955\n",
      "Epoch  1097  G loss  0.115369685\n",
      "Epoch  1098  G loss  0.094635285\n",
      "Epoch  1099  G loss  0.108254336\n",
      "Epoch  1100  G loss  0.07233437\n",
      "Epoch  1101  G loss  0.0870139\n",
      "Epoch  1102  G loss  0.089779\n",
      "Epoch  1103  G loss  0.08816153\n",
      "Epoch  1104  G loss  0.18496914\n",
      "Epoch  1105  G loss  0.07456747\n",
      "Epoch  1106  G loss  0.12816595\n",
      "Epoch  1107  G loss  0.08789712\n",
      "Epoch  1108  G loss  0.12904601\n",
      "Epoch  1109  G loss  0.09824423\n",
      "Epoch  1110  G loss  0.1548711\n",
      "Epoch  1111  G loss  0.107287206\n",
      "Epoch  1112  G loss  0.08485969\n",
      "Epoch  1113  G loss  0.13597575\n",
      "Epoch  1114  G loss  0.09197271\n",
      "Epoch  1115  G loss  0.24666531\n",
      "Epoch  1116  G loss  0.11940563\n",
      "Epoch  1117  G loss  0.13627724\n",
      "Epoch  1118  G loss  0.12610464\n",
      "Epoch  1119  G loss  0.10720283\n",
      "Epoch  1120  G loss  0.083907515\n",
      "Epoch  1121  G loss  0.09806967\n",
      "Epoch  1122  G loss  0.07120327\n",
      "Epoch  1123  G loss  0.12337176\n",
      "Epoch  1124  G loss  0.16806209\n",
      "Epoch  1125  G loss  0.087823786\n",
      "Epoch  1126  G loss  0.088666946\n",
      "Epoch  1127  G loss  0.09534023\n",
      "Epoch  1128  G loss  0.096632175\n",
      "Epoch  1129  G loss  0.3016453\n",
      "Epoch  1130  G loss  0.1340846\n",
      "Epoch  1131  G loss  0.06737491\n",
      "Epoch  1132  G loss  0.10119003\n",
      "Epoch  1133  G loss  0.069712885\n",
      "Epoch  1134  G loss  0.07471447\n",
      "Epoch  1135  G loss  0.085333824\n",
      "Epoch  1136  G loss  0.08662292\n",
      "Epoch  1137  G loss  0.1653477\n",
      "Epoch  1138  G loss  0.080559194\n",
      "Epoch  1139  G loss  0.076467976\n",
      "Epoch  1140  G loss  0.101441704\n",
      "Epoch  1141  G loss  0.09917289\n",
      "Epoch  1142  G loss  0.104617834\n",
      "Epoch  1143  G loss  0.079027385\n",
      "Epoch  1144  G loss  0.064053394\n",
      "Epoch  1145  G loss  0.08393451\n",
      "Epoch  1146  G loss  0.10265553\n",
      "Epoch  1147  G loss  0.0576517\n",
      "Epoch  1148  G loss  0.0730043\n",
      "Epoch  1149  G loss  0.0504363\n",
      "Epoch  1150  G loss  0.07909752\n",
      "Epoch  1151  G loss  0.15655698\n",
      "Epoch  1152  G loss  0.0626685\n",
      "Epoch  1153  G loss  0.07272687\n",
      "Epoch  1154  G loss  0.09007275\n",
      "Epoch  1155  G loss  0.11136151\n",
      "Epoch  1156  G loss  0.11326765\n",
      "Epoch  1157  G loss  0.08992327\n",
      "Epoch  1158  G loss  0.13585882\n",
      "Epoch  1159  G loss  0.08875239\n",
      "Epoch  1160  G loss  0.108148776\n",
      "Epoch  1161  G loss  0.064491294\n",
      "Epoch  1162  G loss  0.18296905\n",
      "Epoch  1163  G loss  0.2230038\n",
      "Epoch  1164  G loss  0.056130614\n",
      "Epoch  1165  G loss  0.08730843\n",
      "Epoch  1166  G loss  0.106515646\n",
      "Epoch  1167  G loss  0.09978956\n",
      "Epoch  1168  G loss  0.09412223\n",
      "Epoch  1169  G loss  0.0689562\n",
      "Epoch  1170  G loss  0.099479765\n",
      "Epoch  1171  G loss  0.08570927\n",
      "Epoch  1172  G loss  0.048835367\n",
      "Epoch  1173  G loss  0.09155762\n",
      "Epoch  1174  G loss  0.17313229\n",
      "Epoch  1175  G loss  0.08007783\n",
      "Epoch  1176  G loss  0.06749637\n",
      "Epoch  1177  G loss  0.08936723\n",
      "Epoch  1178  G loss  0.18446064\n",
      "Epoch  1179  G loss  0.089089386\n",
      "Epoch  1180  G loss  0.075052254\n",
      "Epoch  1181  G loss  0.3461636\n",
      "Epoch  1182  G loss  0.108246185\n",
      "Epoch  1183  G loss  0.082743205\n",
      "Epoch  1184  G loss  0.15381943\n",
      "Epoch  1185  G loss  0.10817381\n",
      "Epoch  1186  G loss  0.07175846\n",
      "Epoch  1187  G loss  0.07740588\n",
      "Epoch  1188  G loss  0.08289854\n",
      "Epoch  1189  G loss  0.076033406\n",
      "Epoch  1190  G loss  0.071834706\n",
      "Epoch  1191  G loss  0.1513625\n",
      "Epoch  1192  G loss  0.16413128\n",
      "Epoch  1193  G loss  0.17629708\n",
      "Epoch  1194  G loss  0.074222185\n",
      "Epoch  1195  G loss  0.10008753\n",
      "Epoch  1196  G loss  0.08631939\n",
      "Epoch  1197  G loss  0.08289129\n",
      "Epoch  1198  G loss  0.08972799\n",
      "Epoch  1199  G loss  0.10052371\n",
      "Epoch  1200  G loss  0.09809577\n",
      "Epoch  1201  G loss  0.080426626\n",
      "Epoch  1202  G loss  0.088610314\n",
      "Epoch  1203  G loss  0.07453329\n",
      "Epoch  1204  G loss  0.12774591\n",
      "Epoch  1205  G loss  0.066627316\n",
      "Epoch  1206  G loss  0.09500468\n",
      "Epoch  1207  G loss  0.09675946\n",
      "Epoch  1208  G loss  0.15675113\n",
      "Epoch  1209  G loss  0.05702448\n",
      "Epoch  1210  G loss  0.08109722\n",
      "Epoch  1211  G loss  0.10154798\n",
      "Epoch  1212  G loss  0.077038\n",
      "Epoch  1213  G loss  0.09543407\n",
      "Epoch  1214  G loss  0.07955546\n",
      "Epoch  1215  G loss  0.047260303\n",
      "Epoch  1216  G loss  0.09002938\n",
      "Epoch  1217  G loss  0.18086721\n",
      "Epoch  1218  G loss  0.073081024\n",
      "Epoch  1219  G loss  0.0861421\n",
      "Epoch  1220  G loss  0.06612697\n",
      "Epoch  1221  G loss  0.08792809\n",
      "Epoch  1222  G loss  0.060535762\n",
      "Epoch  1223  G loss  0.10478529\n",
      "Epoch  1224  G loss  0.098320186\n",
      "Epoch  1225  G loss  0.054675955\n",
      "Epoch  1226  G loss  0.18781692\n",
      "Epoch  1227  G loss  0.07533634\n",
      "Epoch  1228  G loss  0.13566922\n",
      "Epoch  1229  G loss  0.12172902\n",
      "Epoch  1230  G loss  0.05186942\n",
      "Epoch  1231  G loss  0.079187416\n",
      "Epoch  1232  G loss  0.06278333\n",
      "Epoch  1233  G loss  0.079110414\n",
      "Epoch  1234  G loss  0.06872706\n",
      "Epoch  1235  G loss  0.072703294\n",
      "Epoch  1236  G loss  0.2533901\n",
      "Epoch  1237  G loss  0.05251356\n",
      "Epoch  1238  G loss  0.1468039\n",
      "Epoch  1239  G loss  0.12700328\n",
      "Epoch  1240  G loss  0.10322865\n",
      "Epoch  1241  G loss  0.08286864\n",
      "Epoch  1242  G loss  0.087263964\n",
      "Epoch  1243  G loss  0.12238499\n",
      "Epoch  1244  G loss  0.06847749\n",
      "Epoch  1245  G loss  0.13230462\n",
      "Epoch  1246  G loss  0.06782005\n",
      "Epoch  1247  G loss  0.2884662\n",
      "Epoch  1248  G loss  0.105824195\n",
      "Epoch  1249  G loss  0.07399497\n",
      "Epoch  1250  G loss  0.05197187\n",
      "Epoch  1251  G loss  0.062355746\n",
      "Epoch  1252  G loss  0.08728734\n",
      "Epoch  1253  G loss  0.24256326\n",
      "Epoch  1254  G loss  0.21273167\n",
      "Epoch  1255  G loss  0.080989964\n",
      "Epoch  1256  G loss  0.0653655\n",
      "Epoch  1257  G loss  0.070373386\n",
      "Epoch  1258  G loss  0.09764972\n",
      "Epoch  1259  G loss  0.06681169\n",
      "Epoch  1260  G loss  0.2697258\n",
      "Epoch  1261  G loss  0.08950385\n",
      "Epoch  1262  G loss  0.1068948\n",
      "Epoch  1263  G loss  0.1027157\n",
      "Epoch  1264  G loss  0.08490438\n",
      "Epoch  1265  G loss  0.12978335\n",
      "Epoch  1266  G loss  0.052400798\n",
      "Epoch  1267  G loss  0.097333916\n",
      "Epoch  1268  G loss  0.052290704\n",
      "Epoch  1269  G loss  0.06838264\n",
      "Epoch  1270  G loss  0.10086402\n",
      "Epoch  1271  G loss  0.11452649\n",
      "Epoch  1272  G loss  0.08730457\n",
      "Epoch  1273  G loss  0.2506688\n",
      "Epoch  1274  G loss  0.1511978\n",
      "Epoch  1275  G loss  0.14756761\n",
      "Epoch  1276  G loss  0.053044822\n",
      "Epoch  1277  G loss  0.071283154\n",
      "Epoch  1278  G loss  0.061706606\n",
      "Epoch  1279  G loss  0.061656047\n",
      "Epoch  1280  G loss  0.10317877\n",
      "Epoch  1281  G loss  0.05243045\n",
      "Epoch  1282  G loss  0.052128464\n",
      "Epoch  1283  G loss  0.089785576\n",
      "Epoch  1284  G loss  0.06474653\n",
      "Epoch  1285  G loss  0.15548299\n",
      "Epoch  1286  G loss  0.121509075\n",
      "Epoch  1287  G loss  0.07684069\n",
      "Epoch  1288  G loss  0.0810468\n",
      "Epoch  1289  G loss  0.07167242\n",
      "Epoch  1290  G loss  0.07121745\n",
      "Epoch  1291  G loss  0.15096326\n",
      "Epoch  1292  G loss  0.065578885\n",
      "Epoch  1293  G loss  0.09331864\n",
      "Epoch  1294  G loss  0.3077134\n",
      "Epoch  1295  G loss  0.12405173\n",
      "Epoch  1296  G loss  0.07531019\n",
      "Epoch  1297  G loss  0.12364787\n",
      "Epoch  1298  G loss  0.08960428\n",
      "Epoch  1299  G loss  0.15444657\n",
      "Epoch  1300  G loss  0.0681035\n",
      "Epoch  1301  G loss  0.10060808\n",
      "Epoch  1302  G loss  0.0866765\n",
      "Epoch  1303  G loss  0.19617264\n",
      "Epoch  1304  G loss  0.08743011\n",
      "Epoch  1305  G loss  0.10591922\n",
      "Epoch  1306  G loss  0.11667097\n",
      "Epoch  1307  G loss  0.0617467\n",
      "Epoch  1308  G loss  0.07705917\n",
      "Epoch  1309  G loss  0.05819121\n",
      "Epoch  1310  G loss  0.07622857\n",
      "Epoch  1311  G loss  0.09012421\n",
      "Epoch  1312  G loss  0.06031086\n",
      "Epoch  1313  G loss  0.14472882\n",
      "Epoch  1314  G loss  0.061368074\n",
      "Epoch  1315  G loss  0.04670897\n",
      "Epoch  1316  G loss  0.066812806\n",
      "Epoch  1317  G loss  0.09664974\n",
      "Epoch  1318  G loss  0.07482151\n",
      "Epoch  1319  G loss  0.08922088\n",
      "Epoch  1320  G loss  0.07508396\n",
      "Epoch  1321  G loss  0.100278646\n",
      "Epoch  1322  G loss  0.09506152\n",
      "Epoch  1323  G loss  0.08513886\n",
      "Epoch  1324  G loss  0.1152651\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1325  G loss  0.21593785\n",
      "Epoch  1326  G loss  0.08909249\n",
      "Epoch  1327  G loss  0.054980252\n",
      "Epoch  1328  G loss  0.10470269\n",
      "Epoch  1329  G loss  0.06641614\n",
      "Epoch  1330  G loss  0.054050714\n",
      "Epoch  1331  G loss  0.07235835\n",
      "Epoch  1332  G loss  0.053936627\n",
      "Epoch  1333  G loss  0.146625\n",
      "Epoch  1334  G loss  0.080070354\n",
      "Epoch  1335  G loss  0.09044433\n",
      "Epoch  1336  G loss  0.07657871\n",
      "Epoch  1337  G loss  0.050933022\n",
      "Epoch  1338  G loss  0.2037686\n",
      "Epoch  1339  G loss  0.14364207\n",
      "Epoch  1340  G loss  0.08178564\n",
      "Epoch  1341  G loss  0.12726392\n",
      "Epoch  1342  G loss  0.1301322\n",
      "Epoch  1343  G loss  0.123297565\n",
      "Epoch  1344  G loss  0.13072872\n",
      "Epoch  1345  G loss  0.06311918\n",
      "Epoch  1346  G loss  0.05550636\n",
      "Epoch  1347  G loss  0.21219736\n",
      "Epoch  1348  G loss  0.09313989\n",
      "Epoch  1349  G loss  0.110368915\n",
      "Epoch  1350  G loss  0.094100855\n",
      "Epoch  1351  G loss  0.079185784\n",
      "Epoch  1352  G loss  0.073290125\n",
      "Epoch  1353  G loss  0.10263594\n",
      "Epoch  1354  G loss  0.06322859\n",
      "Epoch  1355  G loss  0.22598183\n",
      "Epoch  1356  G loss  0.08362039\n",
      "Epoch  1357  G loss  0.19831271\n",
      "Epoch  1358  G loss  0.087412596\n",
      "Epoch  1359  G loss  0.068949245\n",
      "Epoch  1360  G loss  0.071721084\n",
      "Epoch  1361  G loss  0.099121004\n",
      "Epoch  1362  G loss  0.05920227\n",
      "Epoch  1363  G loss  0.10478876\n",
      "Epoch  1364  G loss  0.11466322\n",
      "Epoch  1365  G loss  0.081889346\n",
      "Epoch  1366  G loss  0.07623005\n",
      "Epoch  1367  G loss  0.083435625\n",
      "Epoch  1368  G loss  0.072872885\n",
      "Epoch  1369  G loss  0.093458615\n",
      "Epoch  1370  G loss  0.07831582\n",
      "Epoch  1371  G loss  0.15115918\n",
      "Epoch  1372  G loss  0.1212588\n",
      "Epoch  1373  G loss  0.12168489\n",
      "Epoch  1374  G loss  0.11449204\n",
      "Epoch  1375  G loss  0.34403023\n",
      "Epoch  1376  G loss  0.09342227\n",
      "Epoch  1377  G loss  0.08492839\n",
      "Epoch  1378  G loss  0.07203954\n",
      "Epoch  1379  G loss  0.18698893\n",
      "Epoch  1380  G loss  0.08267672\n",
      "Epoch  1381  G loss  0.07151123\n",
      "Epoch  1382  G loss  0.12343821\n",
      "Epoch  1383  G loss  0.083520144\n",
      "Epoch  1384  G loss  0.086762965\n",
      "Epoch  1385  G loss  0.06616086\n",
      "Epoch  1386  G loss  0.105455875\n",
      "Epoch  1387  G loss  0.05896467\n",
      "Epoch  1388  G loss  0.057952505\n",
      "Epoch  1389  G loss  0.05755644\n",
      "Epoch  1390  G loss  0.09881598\n",
      "Epoch  1391  G loss  0.13138664\n",
      "Epoch  1392  G loss  0.08768257\n",
      "Epoch  1393  G loss  0.077309646\n",
      "Epoch  1394  G loss  0.07518267\n",
      "Epoch  1395  G loss  0.06524791\n",
      "Epoch  1396  G loss  0.06600346\n",
      "Epoch  1397  G loss  0.08637146\n",
      "Epoch  1398  G loss  0.057901073\n",
      "Epoch  1399  G loss  0.079934545\n",
      "Epoch  1400  G loss  0.111308254\n",
      "Epoch  1401  G loss  0.06572441\n",
      "Epoch  1402  G loss  0.054092754\n",
      "Epoch  1403  G loss  0.073870495\n",
      "Epoch  1404  G loss  0.13646571\n",
      "Epoch  1405  G loss  0.073584035\n",
      "Epoch  1406  G loss  0.068163075\n",
      "Epoch  1407  G loss  0.12828057\n",
      "Epoch  1408  G loss  0.11832476\n",
      "Epoch  1409  G loss  0.10202619\n",
      "Epoch  1410  G loss  0.06142794\n",
      "Epoch  1411  G loss  0.09324636\n",
      "Epoch  1412  G loss  0.10159167\n",
      "Epoch  1413  G loss  0.08237826\n",
      "Epoch  1414  G loss  0.041540977\n",
      "Epoch  1415  G loss  0.096541196\n",
      "Epoch  1416  G loss  0.059305403\n",
      "Epoch  1417  G loss  0.10148483\n",
      "Epoch  1418  G loss  0.13289674\n",
      "Epoch  1419  G loss  0.08548496\n",
      "Epoch  1420  G loss  0.06641417\n",
      "Epoch  1421  G loss  0.04735467\n",
      "Epoch  1422  G loss  0.078109644\n",
      "Epoch  1423  G loss  0.1048323\n",
      "Epoch  1424  G loss  0.06479933\n",
      "Epoch  1425  G loss  0.08461153\n",
      "Epoch  1426  G loss  0.089223124\n",
      "Epoch  1427  G loss  0.056492697\n",
      "Epoch  1428  G loss  0.079831146\n",
      "Epoch  1429  G loss  0.06671453\n",
      "Epoch  1430  G loss  0.060886484\n",
      "Epoch  1431  G loss  0.091083296\n",
      "Epoch  1432  G loss  0.21776539\n",
      "Epoch  1433  G loss  0.076937735\n",
      "Epoch  1434  G loss  0.12493056\n",
      "Epoch  1435  G loss  0.2731047\n",
      "Epoch  1436  G loss  0.056979958\n",
      "Epoch  1437  G loss  0.053522307\n",
      "Epoch  1438  G loss  0.06765049\n",
      "Epoch  1439  G loss  0.1897155\n",
      "Epoch  1440  G loss  0.06915059\n",
      "Epoch  1441  G loss  0.060792148\n",
      "Epoch  1442  G loss  0.05459829\n",
      "Epoch  1443  G loss  0.05651495\n",
      "Epoch  1444  G loss  0.13980167\n",
      "Epoch  1445  G loss  0.1434786\n",
      "Epoch  1446  G loss  0.061559513\n",
      "Epoch  1447  G loss  0.09502553\n",
      "Epoch  1448  G loss  0.11721611\n",
      "Epoch  1449  G loss  0.060539175\n",
      "Epoch  1450  G loss  0.08865061\n",
      "Epoch  1451  G loss  0.18574543\n",
      "Epoch  1452  G loss  0.06335941\n",
      "Epoch  1453  G loss  0.1366682\n",
      "Epoch  1454  G loss  0.0539069\n",
      "Epoch  1455  G loss  0.079916574\n",
      "Epoch  1456  G loss  0.07111085\n",
      "Epoch  1457  G loss  0.064818084\n",
      "Epoch  1458  G loss  0.063560106\n",
      "Epoch  1459  G loss  0.19043754\n",
      "Epoch  1460  G loss  0.15522043\n",
      "Epoch  1461  G loss  0.06573824\n",
      "Epoch  1462  G loss  0.15271652\n",
      "Epoch  1463  G loss  0.09476626\n",
      "Epoch  1464  G loss  0.06868404\n",
      "Epoch  1465  G loss  0.15089162\n",
      "Epoch  1466  G loss  0.08501178\n",
      "Epoch  1467  G loss  0.073467955\n",
      "Epoch  1468  G loss  0.07810744\n",
      "Epoch  1469  G loss  0.049553465\n",
      "Epoch  1470  G loss  0.0601951\n",
      "Epoch  1471  G loss  0.09469027\n",
      "Epoch  1472  G loss  0.24497306\n",
      "Epoch  1473  G loss  0.09627181\n",
      "Epoch  1474  G loss  0.06801966\n",
      "Epoch  1475  G loss  0.057116676\n",
      "Epoch  1476  G loss  0.05066401\n",
      "Epoch  1477  G loss  0.07309496\n",
      "Epoch  1478  G loss  0.15421483\n",
      "Epoch  1479  G loss  0.05264924\n",
      "Epoch  1480  G loss  0.05548669\n",
      "Epoch  1481  G loss  0.0962838\n",
      "Epoch  1482  G loss  0.08791577\n",
      "Epoch  1483  G loss  0.093167625\n",
      "Epoch  1484  G loss  0.083997875\n",
      "Epoch  1485  G loss  0.05211802\n",
      "Epoch  1486  G loss  0.16767092\n",
      "Epoch  1487  G loss  0.13194357\n",
      "Epoch  1488  G loss  0.10039715\n",
      "Epoch  1489  G loss  0.08730983\n",
      "Epoch  1490  G loss  0.056091424\n",
      "Epoch  1491  G loss  0.08488027\n",
      "Epoch  1492  G loss  0.14567499\n",
      "Epoch  1493  G loss  0.05728915\n",
      "Epoch  1494  G loss  0.08921316\n",
      "Epoch  1495  G loss  0.09145814\n",
      "Epoch  1496  G loss  0.07034509\n",
      "Epoch  1497  G loss  0.09709257\n",
      "Epoch  1498  G loss  0.06790347\n",
      "Epoch  1499  G loss  0.08115443\n",
      "Epoch  1500  G loss  0.05786593\n",
      "Epoch  1501  G loss  0.047993105\n",
      "Epoch  1502  G loss  0.0680506\n",
      "Epoch  1503  G loss  0.046504255\n",
      "Epoch  1504  G loss  0.06692386\n",
      "Epoch  1505  G loss  0.075166516\n",
      "Epoch  1506  G loss  0.11356906\n",
      "Epoch  1507  G loss  0.1004087\n",
      "Epoch  1508  G loss  0.07620048\n",
      "Epoch  1509  G loss  0.11455987\n",
      "Epoch  1510  G loss  0.036519993\n",
      "Epoch  1511  G loss  0.093165405\n",
      "Epoch  1512  G loss  0.06115693\n",
      "Epoch  1513  G loss  0.06803841\n",
      "Epoch  1514  G loss  0.061734516\n",
      "Epoch  1515  G loss  0.06674095\n",
      "Epoch  1516  G loss  0.044421468\n",
      "Epoch  1517  G loss  0.081884086\n",
      "Epoch  1518  G loss  0.5300579\n",
      "Epoch  1519  G loss  0.093130805\n",
      "Epoch  1520  G loss  0.06599734\n",
      "Epoch  1521  G loss  0.06578278\n",
      "Epoch  1522  G loss  0.06341945\n",
      "Epoch  1523  G loss  0.076738514\n",
      "Epoch  1524  G loss  0.09394344\n",
      "Epoch  1525  G loss  0.058664095\n",
      "Epoch  1526  G loss  0.09441442\n",
      "Epoch  1527  G loss  0.09643042\n",
      "Epoch  1528  G loss  0.056030434\n",
      "Epoch  1529  G loss  0.09648631\n",
      "Epoch  1530  G loss  0.047709797\n",
      "Epoch  1531  G loss  0.05387337\n",
      "Epoch  1532  G loss  0.08075984\n",
      "Epoch  1533  G loss  0.08102857\n",
      "Epoch  1534  G loss  0.053708892\n",
      "Epoch  1535  G loss  0.04551579\n",
      "Epoch  1536  G loss  0.047402967\n",
      "Epoch  1537  G loss  0.1342981\n",
      "Epoch  1538  G loss  0.060911056\n",
      "Epoch  1539  G loss  0.075138\n",
      "Epoch  1540  G loss  0.061467186\n",
      "Epoch  1541  G loss  0.049725074\n",
      "Epoch  1542  G loss  0.06751213\n",
      "Epoch  1543  G loss  0.06110846\n",
      "Epoch  1544  G loss  0.058457065\n",
      "Epoch  1545  G loss  0.13594069\n",
      "Epoch  1546  G loss  0.04320069\n",
      "Epoch  1547  G loss  0.11869184\n",
      "Epoch  1548  G loss  0.07009163\n",
      "Epoch  1549  G loss  0.07004861\n",
      "Epoch  1550  G loss  0.065958075\n",
      "Epoch  1551  G loss  0.05080433\n",
      "Epoch  1552  G loss  0.091504\n",
      "Epoch  1553  G loss  0.08948477\n",
      "Epoch  1554  G loss  0.11829462\n",
      "Epoch  1555  G loss  0.049618077\n",
      "Epoch  1556  G loss  0.06682233\n",
      "Epoch  1557  G loss  0.06644366\n",
      "Epoch  1558  G loss  0.044486467\n",
      "Epoch  1559  G loss  0.061336327\n",
      "Epoch  1560  G loss  0.023612743\n",
      "Epoch  1561  G loss  0.08329233\n",
      "Epoch  1562  G loss  0.042970404\n",
      "Epoch  1563  G loss  0.06888179\n",
      "Epoch  1564  G loss  0.07708125\n",
      "Epoch  1565  G loss  0.25944868\n",
      "Epoch  1566  G loss  0.056408275\n",
      "Epoch  1567  G loss  0.04557019\n",
      "Epoch  1568  G loss  0.047127973\n",
      "Epoch  1569  G loss  0.06513629\n",
      "Epoch  1570  G loss  0.04801594\n",
      "Epoch  1571  G loss  0.09669324\n",
      "Epoch  1572  G loss  0.05061397\n",
      "Epoch  1573  G loss  0.07153504\n",
      "Epoch  1574  G loss  0.038840625\n",
      "Epoch  1575  G loss  0.03957719\n",
      "Epoch  1576  G loss  0.069889374\n",
      "Epoch  1577  G loss  0.064131886\n",
      "Epoch  1578  G loss  0.18004298\n",
      "Epoch  1579  G loss  0.03788468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1580  G loss  0.069975674\n",
      "Epoch  1581  G loss  0.040343393\n",
      "Epoch  1582  G loss  0.11484948\n",
      "Epoch  1583  G loss  0.058341492\n",
      "Epoch  1584  G loss  0.13859497\n",
      "Epoch  1585  G loss  0.07189576\n",
      "Epoch  1586  G loss  0.09086362\n",
      "Epoch  1587  G loss  0.053566694\n",
      "Epoch  1588  G loss  0.041416682\n",
      "Epoch  1589  G loss  0.07482679\n",
      "Epoch  1590  G loss  0.054482166\n",
      "Epoch  1591  G loss  0.047655758\n",
      "Epoch  1592  G loss  0.050932985\n",
      "Epoch  1593  G loss  0.10331342\n",
      "Epoch  1594  G loss  0.04915156\n",
      "Epoch  1595  G loss  0.07031876\n",
      "Epoch  1596  G loss  0.08059084\n",
      "Epoch  1597  G loss  0.043665066\n",
      "Epoch  1598  G loss  0.05890886\n",
      "Epoch  1599  G loss  0.0983333\n",
      "Epoch  1600  G loss  0.053593233\n",
      "Epoch  1601  G loss  0.1639763\n",
      "Epoch  1602  G loss  0.052826013\n",
      "Epoch  1603  G loss  0.14762165\n",
      "Epoch  1604  G loss  0.048326295\n",
      "Epoch  1605  G loss  0.07508009\n",
      "Epoch  1606  G loss  0.056330565\n",
      "Epoch  1607  G loss  0.08201418\n",
      "Epoch  1608  G loss  0.051911306\n",
      "Epoch  1609  G loss  0.062061206\n",
      "Epoch  1610  G loss  0.066292435\n",
      "Epoch  1611  G loss  0.07965875\n",
      "Epoch  1612  G loss  0.075733654\n",
      "Epoch  1613  G loss  0.08311734\n",
      "Epoch  1614  G loss  0.15881576\n",
      "Epoch  1615  G loss  0.1207563\n",
      "Epoch  1616  G loss  0.12027254\n",
      "Epoch  1617  G loss  0.05604322\n",
      "Epoch  1618  G loss  0.036442667\n",
      "Epoch  1619  G loss  0.046133906\n",
      "Epoch  1620  G loss  0.048960116\n",
      "Epoch  1621  G loss  0.06616211\n",
      "Epoch  1622  G loss  0.044593934\n",
      "Epoch  1623  G loss  0.11943654\n",
      "Epoch  1624  G loss  0.06568917\n",
      "Epoch  1625  G loss  0.09152716\n",
      "Epoch  1626  G loss  0.07765128\n",
      "Epoch  1627  G loss  0.07390977\n",
      "Epoch  1628  G loss  0.04914416\n",
      "Epoch  1629  G loss  0.0653153\n",
      "Epoch  1630  G loss  0.050312757\n",
      "Epoch  1631  G loss  0.05875\n",
      "Epoch  1632  G loss  0.048060685\n",
      "Epoch  1633  G loss  0.058964025\n",
      "Epoch  1634  G loss  0.0544887\n",
      "Epoch  1635  G loss  0.19719018\n",
      "Epoch  1636  G loss  0.061948564\n",
      "Epoch  1637  G loss  0.055116773\n",
      "Epoch  1638  G loss  0.06962504\n",
      "Epoch  1639  G loss  0.09538128\n",
      "Epoch  1640  G loss  0.10185792\n",
      "Epoch  1641  G loss  0.2092127\n",
      "Epoch  1642  G loss  0.07347464\n",
      "Epoch  1643  G loss  0.2506683\n",
      "Epoch  1644  G loss  0.17542358\n",
      "Epoch  1645  G loss  0.074812524\n",
      "Epoch  1646  G loss  0.052537564\n",
      "Epoch  1647  G loss  0.10992689\n",
      "Epoch  1648  G loss  0.061509818\n",
      "Epoch  1649  G loss  0.10685918\n",
      "Epoch  1650  G loss  0.058754217\n",
      "Epoch  1651  G loss  0.039895207\n",
      "Epoch  1652  G loss  0.052531842\n",
      "Epoch  1653  G loss  0.043591473\n",
      "Epoch  1654  G loss  0.058544934\n",
      "Epoch  1655  G loss  0.08246376\n",
      "Epoch  1656  G loss  0.15014748\n",
      "Epoch  1657  G loss  0.070204504\n",
      "Epoch  1658  G loss  0.051737662\n",
      "Epoch  1659  G loss  0.04975919\n",
      "Epoch  1660  G loss  0.07117298\n",
      "Epoch  1661  G loss  0.18700767\n",
      "Epoch  1662  G loss  0.043454736\n",
      "Epoch  1663  G loss  0.06206615\n",
      "Epoch  1664  G loss  0.15010852\n",
      "Epoch  1665  G loss  0.26414195\n",
      "Epoch  1666  G loss  0.042921398\n",
      "Epoch  1667  G loss  0.030101294\n",
      "Epoch  1668  G loss  0.04968551\n",
      "Epoch  1669  G loss  0.13114524\n",
      "Epoch  1670  G loss  0.042333186\n",
      "Epoch  1671  G loss  0.04091527\n",
      "Epoch  1672  G loss  0.2967792\n",
      "Epoch  1673  G loss  0.09841574\n",
      "Epoch  1674  G loss  0.036344238\n",
      "Epoch  1675  G loss  0.033469014\n",
      "Epoch  1676  G loss  0.055604797\n",
      "Epoch  1677  G loss  0.09282189\n",
      "Epoch  1678  G loss  0.14601071\n",
      "Epoch  1679  G loss  0.029708752\n",
      "Epoch  1680  G loss  0.034966428\n",
      "Epoch  1681  G loss  0.08523791\n",
      "Epoch  1682  G loss  0.09761753\n",
      "Epoch  1683  G loss  0.06218962\n",
      "Epoch  1684  G loss  0.09273026\n",
      "Epoch  1685  G loss  0.07867825\n",
      "Epoch  1686  G loss  0.048319116\n",
      "Epoch  1687  G loss  0.0785093\n",
      "Epoch  1688  G loss  0.029122626\n",
      "Epoch  1689  G loss  0.052672654\n",
      "Epoch  1690  G loss  0.07221254\n",
      "Epoch  1691  G loss  0.10714203\n",
      "Epoch  1692  G loss  0.0477494\n",
      "Epoch  1693  G loss  0.07485573\n",
      "Epoch  1694  G loss  0.15211964\n",
      "Epoch  1695  G loss  0.052257717\n",
      "Epoch  1696  G loss  0.06250194\n",
      "Epoch  1697  G loss  0.05688003\n",
      "Epoch  1698  G loss  0.0886269\n",
      "Epoch  1699  G loss  0.10965216\n",
      "Epoch  1700  G loss  0.099197626\n",
      "Epoch  1701  G loss  0.11920449\n",
      "Epoch  1702  G loss  0.051085744\n",
      "Epoch  1703  G loss  0.06886281\n",
      "Epoch  1704  G loss  0.06549973\n",
      "Epoch  1705  G loss  0.06631612\n",
      "Epoch  1706  G loss  0.14104815\n",
      "Epoch  1707  G loss  0.07197905\n",
      "Epoch  1708  G loss  0.062534906\n",
      "Epoch  1709  G loss  0.060632944\n",
      "Epoch  1710  G loss  0.093364716\n",
      "Epoch  1711  G loss  0.06905358\n",
      "Epoch  1712  G loss  0.06139766\n",
      "Epoch  1713  G loss  0.104235835\n",
      "Epoch  1714  G loss  0.055520307\n",
      "Epoch  1715  G loss  0.067547694\n",
      "Epoch  1716  G loss  0.050367538\n",
      "Epoch  1717  G loss  0.059123263\n",
      "Epoch  1718  G loss  0.09070835\n",
      "Epoch  1719  G loss  0.05011375\n",
      "Epoch  1720  G loss  0.04538094\n",
      "Epoch  1721  G loss  0.03488322\n",
      "Epoch  1722  G loss  0.049330186\n",
      "Epoch  1723  G loss  0.109242946\n",
      "Epoch  1724  G loss  0.12748669\n",
      "Epoch  1725  G loss  0.074706934\n",
      "Epoch  1726  G loss  0.040796306\n",
      "Epoch  1727  G loss  0.036827516\n",
      "Epoch  1728  G loss  0.09932557\n",
      "Epoch  1729  G loss  0.067271456\n",
      "Epoch  1730  G loss  0.3159478\n",
      "Epoch  1731  G loss  0.047015727\n",
      "Epoch  1732  G loss  0.09737972\n",
      "Epoch  1733  G loss  0.10393757\n",
      "Epoch  1734  G loss  0.05583178\n",
      "Epoch  1735  G loss  0.14123206\n",
      "Epoch  1736  G loss  0.032013535\n",
      "Epoch  1737  G loss  0.039130032\n",
      "Epoch  1738  G loss  0.04673587\n",
      "Epoch  1739  G loss  0.07333439\n",
      "Epoch  1740  G loss  0.060840786\n",
      "Epoch  1741  G loss  0.071435995\n",
      "Epoch  1742  G loss  0.06679957\n",
      "Epoch  1743  G loss  0.057336465\n",
      "Epoch  1744  G loss  0.029998923\n",
      "Epoch  1745  G loss  0.033240024\n",
      "Epoch  1746  G loss  0.034445837\n",
      "Epoch  1747  G loss  0.05370605\n",
      "Epoch  1748  G loss  0.09393748\n",
      "Epoch  1749  G loss  0.04215284\n",
      "Epoch  1750  G loss  0.17137276\n",
      "Epoch  1751  G loss  0.068524115\n",
      "Epoch  1752  G loss  0.035765916\n",
      "Epoch  1753  G loss  0.07238791\n",
      "Epoch  1754  G loss  0.07482528\n",
      "Epoch  1755  G loss  0.07898486\n",
      "Epoch  1756  G loss  0.044626176\n",
      "Epoch  1757  G loss  0.06415075\n",
      "Epoch  1758  G loss  0.045425992\n",
      "Epoch  1759  G loss  0.08902078\n",
      "Epoch  1760  G loss  0.032104146\n",
      "Epoch  1761  G loss  0.07485867\n",
      "Epoch  1762  G loss  0.06820394\n",
      "Epoch  1763  G loss  0.047555353\n",
      "Epoch  1764  G loss  0.047458876\n",
      "Epoch  1765  G loss  0.04750972\n",
      "Epoch  1766  G loss  0.0853683\n",
      "Epoch  1767  G loss  0.06762112\n",
      "Epoch  1768  G loss  0.11643442\n",
      "Epoch  1769  G loss  0.0435012\n",
      "Epoch  1770  G loss  0.0915285\n",
      "Epoch  1771  G loss  0.07212367\n",
      "Epoch  1772  G loss  0.05213483\n",
      "Epoch  1773  G loss  0.0407952\n",
      "Epoch  1774  G loss  0.050876867\n",
      "Epoch  1775  G loss  0.3234875\n",
      "Epoch  1776  G loss  0.1280575\n",
      "Epoch  1777  G loss  0.04537982\n",
      "Epoch  1778  G loss  0.03585188\n",
      "Epoch  1779  G loss  0.18457381\n",
      "Epoch  1780  G loss  0.06787937\n",
      "Epoch  1781  G loss  0.043567743\n",
      "Epoch  1782  G loss  0.1593879\n",
      "Epoch  1783  G loss  0.124097966\n",
      "Epoch  1784  G loss  0.06911539\n",
      "Epoch  1785  G loss  0.050886836\n",
      "Epoch  1786  G loss  0.057581633\n",
      "Epoch  1787  G loss  0.07616907\n",
      "Epoch  1788  G loss  0.05232687\n",
      "Epoch  1789  G loss  0.05189379\n",
      "Epoch  1790  G loss  0.07607097\n",
      "Epoch  1791  G loss  0.07173417\n",
      "Epoch  1792  G loss  0.07835706\n",
      "Epoch  1793  G loss  0.04239993\n",
      "Epoch  1794  G loss  0.037361633\n",
      "Epoch  1795  G loss  0.10460061\n",
      "Epoch  1796  G loss  0.15522559\n",
      "Epoch  1797  G loss  0.07378182\n",
      "Epoch  1798  G loss  0.026982889\n",
      "Epoch  1799  G loss  0.051677927\n",
      "Epoch  1800  G loss  0.036511783\n",
      "Epoch  1801  G loss  0.062155336\n",
      "Epoch  1802  G loss  0.07895235\n",
      "Epoch  1803  G loss  0.035241183\n",
      "Epoch  1804  G loss  0.05186523\n",
      "Epoch  1805  G loss  0.07744373\n",
      "Epoch  1806  G loss  0.13297169\n",
      "Epoch  1807  G loss  0.05305035\n",
      "Epoch  1808  G loss  0.045231212\n",
      "Epoch  1809  G loss  0.027174963\n",
      "Epoch  1810  G loss  0.0387056\n",
      "Epoch  1811  G loss  0.1551372\n",
      "Epoch  1812  G loss  0.070827216\n",
      "Epoch  1813  G loss  0.11135471\n",
      "Epoch  1814  G loss  0.042314827\n",
      "Epoch  1815  G loss  0.055906877\n",
      "Epoch  1816  G loss  0.08139677\n",
      "Epoch  1817  G loss  0.07529051\n",
      "Epoch  1818  G loss  0.078289084\n",
      "Epoch  1819  G loss  0.048438966\n",
      "Epoch  1820  G loss  0.15788288\n",
      "Epoch  1821  G loss  0.035403986\n",
      "Epoch  1822  G loss  0.08312005\n",
      "Epoch  1823  G loss  0.05239777\n",
      "Epoch  1824  G loss  0.06970326\n",
      "Epoch  1825  G loss  0.06518533\n",
      "Epoch  1826  G loss  0.38167024\n",
      "Epoch  1827  G loss  0.11460809\n",
      "Epoch  1828  G loss  0.08224515\n",
      "Epoch  1829  G loss  0.06826908\n",
      "Epoch  1830  G loss  0.13582353\n",
      "Epoch  1831  G loss  0.07524036\n",
      "Epoch  1832  G loss  0.053128436\n",
      "Epoch  1833  G loss  0.07777056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1834  G loss  0.06571121\n",
      "Epoch  1835  G loss  0.06422476\n",
      "Epoch  1836  G loss  0.042863984\n",
      "Epoch  1837  G loss  0.035764124\n",
      "Epoch  1838  G loss  0.044603426\n",
      "Epoch  1839  G loss  0.20233244\n",
      "Epoch  1840  G loss  0.07061104\n",
      "Epoch  1841  G loss  0.05077256\n",
      "Epoch  1842  G loss  0.052015096\n",
      "Epoch  1843  G loss  0.068909906\n",
      "Epoch  1844  G loss  0.0507201\n",
      "Epoch  1845  G loss  0.03651144\n",
      "Epoch  1846  G loss  0.058264434\n",
      "Epoch  1847  G loss  0.10397789\n",
      "Epoch  1848  G loss  0.051917735\n",
      "Epoch  1849  G loss  0.07616819\n",
      "Epoch  1850  G loss  0.044682622\n",
      "Epoch  1851  G loss  0.105405174\n",
      "Epoch  1852  G loss  0.05685549\n",
      "Epoch  1853  G loss  0.09315806\n",
      "Epoch  1854  G loss  0.07098788\n",
      "Epoch  1855  G loss  0.041699946\n",
      "Epoch  1856  G loss  0.04328234\n",
      "Epoch  1857  G loss  0.08109148\n",
      "Epoch  1858  G loss  0.11774624\n",
      "Epoch  1859  G loss  0.107056834\n",
      "Epoch  1860  G loss  0.04111294\n",
      "Epoch  1861  G loss  0.05280779\n",
      "Epoch  1862  G loss  0.03811842\n",
      "Epoch  1863  G loss  0.06277612\n",
      "Epoch  1864  G loss  0.043955054\n",
      "Epoch  1865  G loss  0.11905048\n",
      "Epoch  1866  G loss  0.02994532\n",
      "Epoch  1867  G loss  0.057183985\n",
      "Epoch  1868  G loss  0.07405581\n",
      "Epoch  1869  G loss  0.08201762\n",
      "Epoch  1870  G loss  0.05374047\n",
      "Epoch  1871  G loss  0.05483402\n",
      "Epoch  1872  G loss  0.05908944\n",
      "Epoch  1873  G loss  0.039220188\n",
      "Epoch  1874  G loss  0.17040755\n",
      "Epoch  1875  G loss  0.0489798\n",
      "Epoch  1876  G loss  0.057291582\n",
      "Epoch  1877  G loss  0.06643329\n",
      "Epoch  1878  G loss  0.078282826\n",
      "Epoch  1879  G loss  0.043941516\n",
      "Epoch  1880  G loss  0.03637764\n",
      "Epoch  1881  G loss  0.03024034\n",
      "Epoch  1882  G loss  0.03453539\n",
      "Epoch  1883  G loss  0.08396226\n",
      "Epoch  1884  G loss  0.05765586\n",
      "Epoch  1885  G loss  0.05249305\n",
      "Epoch  1886  G loss  0.086556256\n",
      "Epoch  1887  G loss  0.03403753\n",
      "Epoch  1888  G loss  0.04413027\n",
      "Epoch  1889  G loss  0.059692364\n",
      "Epoch  1890  G loss  0.04655215\n",
      "Epoch  1891  G loss  0.049015492\n",
      "Epoch  1892  G loss  0.039610222\n",
      "Epoch  1893  G loss  0.058764953\n",
      "Epoch  1894  G loss  0.079958566\n",
      "Epoch  1895  G loss  0.14395688\n",
      "Epoch  1896  G loss  0.033348057\n",
      "Epoch  1897  G loss  0.10486715\n",
      "Epoch  1898  G loss  0.07975772\n",
      "Epoch  1899  G loss  0.03461946\n",
      "Epoch  1900  G loss  0.022687562\n",
      "Epoch  1901  G loss  0.08128152\n",
      "Epoch  1902  G loss  0.08156634\n",
      "Epoch  1903  G loss  0.030117752\n",
      "Epoch  1904  G loss  0.03833003\n",
      "Epoch  1905  G loss  0.10044715\n",
      "Epoch  1906  G loss  0.0831903\n",
      "Epoch  1907  G loss  0.070895195\n",
      "Epoch  1908  G loss  0.045596957\n",
      "Epoch  1909  G loss  0.05550498\n",
      "Epoch  1910  G loss  0.089422345\n",
      "Epoch  1911  G loss  0.069246896\n",
      "Epoch  1912  G loss  0.058705986\n",
      "Epoch  1913  G loss  0.04502896\n",
      "Epoch  1914  G loss  0.02891816\n",
      "Epoch  1915  G loss  0.07399484\n",
      "Epoch  1916  G loss  0.04088743\n",
      "Epoch  1917  G loss  0.06067873\n",
      "Epoch  1918  G loss  0.078923196\n",
      "Epoch  1919  G loss  0.06314951\n",
      "Epoch  1920  G loss  0.03902794\n",
      "Epoch  1921  G loss  0.07779477\n",
      "Epoch  1922  G loss  0.04363343\n",
      "Epoch  1923  G loss  0.07220181\n",
      "Epoch  1924  G loss  0.037404995\n",
      "Epoch  1925  G loss  0.09733864\n",
      "Epoch  1926  G loss  0.04617961\n",
      "Epoch  1927  G loss  0.042383254\n",
      "Epoch  1928  G loss  0.055898175\n",
      "Epoch  1929  G loss  0.049457807\n",
      "Epoch  1930  G loss  0.06528851\n",
      "Epoch  1931  G loss  0.054747354\n",
      "Epoch  1932  G loss  0.05269127\n",
      "Epoch  1933  G loss  0.02805505\n",
      "Epoch  1934  G loss  0.06252608\n",
      "Epoch  1935  G loss  0.077225216\n",
      "Epoch  1936  G loss  0.021713339\n",
      "Epoch  1937  G loss  0.022929972\n",
      "Epoch  1938  G loss  0.06421631\n",
      "Epoch  1939  G loss  0.0774852\n",
      "Epoch  1940  G loss  0.14799269\n",
      "Epoch  1941  G loss  0.14503393\n",
      "Epoch  1942  G loss  0.054047246\n",
      "Epoch  1943  G loss  0.11311311\n",
      "Epoch  1944  G loss  0.073135875\n",
      "Epoch  1945  G loss  0.052051198\n",
      "Epoch  1946  G loss  0.06380058\n",
      "Epoch  1947  G loss  0.063472144\n",
      "Epoch  1948  G loss  0.032519683\n",
      "Epoch  1949  G loss  0.029580267\n",
      "Epoch  1950  G loss  0.04098528\n",
      "Epoch  1951  G loss  0.03613249\n",
      "Epoch  1952  G loss  0.029846817\n",
      "Epoch  1953  G loss  0.15050432\n",
      "Epoch  1954  G loss  0.04460525\n",
      "Epoch  1955  G loss  0.0385047\n",
      "Epoch  1956  G loss  0.03234459\n",
      "Epoch  1957  G loss  0.036460362\n",
      "Epoch  1958  G loss  0.12108994\n",
      "Epoch  1959  G loss  0.043602195\n",
      "Epoch  1960  G loss  0.027738392\n",
      "Epoch  1961  G loss  0.056309413\n",
      "Epoch  1962  G loss  0.045944195\n",
      "Epoch  1963  G loss  0.029110372\n",
      "Epoch  1964  G loss  0.04958592\n",
      "Epoch  1965  G loss  0.029738603\n",
      "Epoch  1966  G loss  0.06858179\n",
      "Epoch  1967  G loss  0.05182621\n",
      "Epoch  1968  G loss  0.074242786\n",
      "Epoch  1969  G loss  0.06344449\n",
      "Epoch  1970  G loss  0.029674955\n",
      "Epoch  1971  G loss  0.050733954\n",
      "Epoch  1972  G loss  0.07325267\n",
      "Epoch  1973  G loss  0.051700998\n",
      "Epoch  1974  G loss  0.041010648\n",
      "Epoch  1975  G loss  0.12421608\n",
      "Epoch  1976  G loss  0.045541745\n",
      "Epoch  1977  G loss  0.047591355\n",
      "Epoch  1978  G loss  0.065906465\n",
      "Epoch  1979  G loss  0.06316528\n",
      "Epoch  1980  G loss  0.03815055\n",
      "Epoch  1981  G loss  0.05204912\n",
      "Epoch  1982  G loss  0.08587911\n",
      "Epoch  1983  G loss  0.026248539\n",
      "Epoch  1984  G loss  0.04394102\n",
      "Epoch  1985  G loss  0.054084998\n",
      "Epoch  1986  G loss  0.043206528\n",
      "Epoch  1987  G loss  0.11735282\n",
      "Epoch  1988  G loss  0.051086795\n",
      "Epoch  1989  G loss  0.057807803\n",
      "Epoch  1990  G loss  0.04937504\n",
      "Epoch  1991  G loss  0.06770605\n",
      "Epoch  1992  G loss  0.039594296\n",
      "Epoch  1993  G loss  0.067940906\n",
      "Epoch  1994  G loss  0.09085602\n",
      "Epoch  1995  G loss  0.116239406\n",
      "Epoch  1996  G loss  0.11083994\n",
      "Epoch  1997  G loss  0.07114616\n",
      "Epoch  1998  G loss  0.03562094\n",
      "Epoch  1999  G loss  0.090562284\n",
      "Epoch  2000  G loss  0.0990121\n",
      "Epoch  2001  G loss  0.059747603\n",
      "Epoch  2002  G loss  0.054111663\n",
      "Epoch  2003  G loss  0.07040139\n",
      "Epoch  2004  G loss  0.029660597\n",
      "Epoch  2005  G loss  0.04500468\n",
      "Epoch  2006  G loss  0.03773672\n",
      "Epoch  2007  G loss  0.038898166\n",
      "Epoch  2008  G loss  0.05007096\n",
      "Epoch  2009  G loss  0.070108145\n",
      "Epoch  2010  G loss  0.04391693\n",
      "Epoch  2011  G loss  0.040461585\n",
      "Epoch  2012  G loss  0.025629802\n",
      "Epoch  2013  G loss  0.08164827\n",
      "Epoch  2014  G loss  0.12113893\n",
      "Epoch  2015  G loss  0.056860905\n",
      "Epoch  2016  G loss  0.05907866\n",
      "Epoch  2017  G loss  0.039673045\n",
      "Epoch  2018  G loss  0.039148536\n",
      "Epoch  2019  G loss  0.043982103\n",
      "Epoch  2020  G loss  0.043606322\n",
      "Epoch  2021  G loss  0.038244788\n",
      "Epoch  2022  G loss  0.06891959\n",
      "Epoch  2023  G loss  0.051120576\n",
      "Epoch  2024  G loss  0.042241216\n",
      "Epoch  2025  G loss  0.028323987\n",
      "Epoch  2026  G loss  0.12511271\n",
      "Epoch  2027  G loss  0.13696578\n",
      "Epoch  2028  G loss  0.029802047\n",
      "Epoch  2029  G loss  0.08945281\n",
      "Epoch  2030  G loss  0.024211898\n",
      "Epoch  2031  G loss  0.0671516\n",
      "Epoch  2032  G loss  0.044056226\n",
      "Epoch  2033  G loss  0.1017042\n",
      "Epoch  2034  G loss  0.09067255\n",
      "Epoch  2035  G loss  0.0682044\n",
      "Epoch  2036  G loss  0.042291846\n",
      "Epoch  2037  G loss  0.037230898\n",
      "Epoch  2038  G loss  0.03239135\n",
      "Epoch  2039  G loss  0.052556504\n",
      "Epoch  2040  G loss  0.029677758\n",
      "Epoch  2041  G loss  0.050314743\n",
      "Epoch  2042  G loss  0.04359619\n",
      "Epoch  2043  G loss  0.09329569\n",
      "Epoch  2044  G loss  0.031137874\n",
      "Epoch  2045  G loss  0.099919885\n",
      "Epoch  2046  G loss  0.027254537\n",
      "Epoch  2047  G loss  0.035655495\n",
      "Epoch  2048  G loss  0.08685511\n",
      "Epoch  2049  G loss  0.14329068\n",
      "Epoch  2050  G loss  0.042459805\n",
      "Epoch  2051  G loss  0.07734258\n",
      "Epoch  2052  G loss  0.03269844\n",
      "Epoch  2053  G loss  0.0615938\n",
      "Epoch  2054  G loss  0.19608034\n",
      "Epoch  2055  G loss  0.06836251\n",
      "Epoch  2056  G loss  0.045513555\n",
      "Epoch  2057  G loss  0.030822566\n",
      "Epoch  2058  G loss  0.13908117\n",
      "Epoch  2059  G loss  0.06789874\n",
      "Epoch  2060  G loss  0.04580466\n",
      "Epoch  2061  G loss  0.049642563\n",
      "Epoch  2062  G loss  0.05090941\n",
      "Epoch  2063  G loss  0.04196268\n",
      "Epoch  2064  G loss  0.044841845\n",
      "Epoch  2065  G loss  0.16821688\n",
      "Epoch  2066  G loss  0.023735156\n",
      "Epoch  2067  G loss  0.064503014\n",
      "Epoch  2068  G loss  0.051414814\n",
      "Epoch  2069  G loss  0.027905067\n",
      "Epoch  2070  G loss  0.08034121\n",
      "Epoch  2071  G loss  0.033769593\n",
      "Epoch  2072  G loss  0.09417173\n",
      "Epoch  2073  G loss  0.030785948\n",
      "Epoch  2074  G loss  0.18724054\n",
      "Epoch  2075  G loss  0.029336244\n",
      "Epoch  2076  G loss  0.058296204\n",
      "Epoch  2077  G loss  0.11187273\n",
      "Epoch  2078  G loss  0.04983173\n",
      "Epoch  2079  G loss  0.025762787\n",
      "Epoch  2080  G loss  0.079857714\n",
      "Epoch  2081  G loss  0.04006809\n",
      "Epoch  2082  G loss  0.111406304\n",
      "Epoch  2083  G loss  0.047741782\n",
      "Epoch  2084  G loss  0.12192359\n",
      "Epoch  2085  G loss  0.14756174\n",
      "Epoch  2086  G loss  0.09540967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  2087  G loss  0.055875283\n",
      "Epoch  2088  G loss  0.0503546\n",
      "Epoch  2089  G loss  0.0836944\n",
      "Epoch  2090  G loss  0.055065792\n",
      "Epoch  2091  G loss  0.027232958\n",
      "Epoch  2092  G loss  0.07454407\n",
      "Epoch  2093  G loss  0.0572537\n",
      "Epoch  2094  G loss  0.040595014\n",
      "Epoch  2095  G loss  0.08578601\n",
      "Epoch  2096  G loss  0.17316854\n",
      "Epoch  2097  G loss  0.039858177\n",
      "Epoch  2098  G loss  0.0569931\n",
      "Epoch  2099  G loss  0.035731614\n",
      "Epoch  2100  G loss  0.08190834\n",
      "Epoch  2101  G loss  0.05521113\n",
      "Epoch  2102  G loss  0.026579589\n",
      "Epoch  2103  G loss  0.04867272\n",
      "Epoch  2104  G loss  0.10591334\n",
      "Epoch  2105  G loss  0.044896662\n",
      "Epoch  2106  G loss  0.041334916\n",
      "Epoch  2107  G loss  0.028064566\n",
      "Epoch  2108  G loss  0.04355352\n",
      "Epoch  2109  G loss  0.036332514\n",
      "Epoch  2110  G loss  0.042661484\n",
      "Epoch  2111  G loss  0.025490401\n",
      "Epoch  2112  G loss  0.035887707\n",
      "Epoch  2113  G loss  0.030192798\n",
      "Epoch  2114  G loss  0.13280912\n",
      "Epoch  2115  G loss  0.048695255\n",
      "Epoch  2116  G loss  0.124648385\n",
      "Epoch  2117  G loss  0.05087776\n",
      "Epoch  2118  G loss  0.11106046\n",
      "Epoch  2119  G loss  0.043408867\n",
      "Epoch  2120  G loss  0.058055002\n",
      "Epoch  2121  G loss  0.05293268\n",
      "Epoch  2122  G loss  0.03510763\n",
      "Epoch  2123  G loss  0.05510899\n",
      "Epoch  2124  G loss  0.040280003\n",
      "Epoch  2125  G loss  0.024832262\n",
      "Epoch  2126  G loss  0.024186293\n",
      "Epoch  2127  G loss  0.029412508\n",
      "Epoch  2128  G loss  0.049500093\n",
      "Epoch  2129  G loss  0.030565286\n",
      "Epoch  2130  G loss  0.04675312\n",
      "Epoch  2131  G loss  0.043691795\n",
      "Epoch  2132  G loss  0.05392148\n",
      "Epoch  2133  G loss  0.031793486\n",
      "Epoch  2134  G loss  0.057219137\n",
      "Epoch  2135  G loss  0.060579915\n",
      "Epoch  2136  G loss  0.12611203\n",
      "Epoch  2137  G loss  0.07022506\n",
      "Epoch  2138  G loss  0.02407142\n",
      "Epoch  2139  G loss  0.03596204\n",
      "Epoch  2140  G loss  0.04332729\n",
      "Epoch  2141  G loss  0.042407945\n",
      "Epoch  2142  G loss  0.04382029\n",
      "Epoch  2143  G loss  0.055009644\n",
      "Epoch  2144  G loss  0.10863557\n",
      "Epoch  2145  G loss  0.051401984\n",
      "Epoch  2146  G loss  0.08970368\n",
      "Epoch  2147  G loss  0.1479991\n",
      "Epoch  2148  G loss  0.04284389\n",
      "Epoch  2149  G loss  0.042371873\n",
      "Epoch  2150  G loss  0.04933548\n",
      "Epoch  2151  G loss  0.025579365\n",
      "Epoch  2152  G loss  0.053847592\n",
      "Epoch  2153  G loss  0.027450891\n",
      "Epoch  2154  G loss  0.030774906\n",
      "Epoch  2155  G loss  0.050807986\n",
      "Epoch  2156  G loss  0.034658257\n",
      "Epoch  2157  G loss  0.040475894\n",
      "Epoch  2158  G loss  0.061686274\n",
      "Epoch  2159  G loss  0.028944664\n",
      "Epoch  2160  G loss  0.04798578\n",
      "Epoch  2161  G loss  0.037803944\n",
      "Epoch  2162  G loss  0.08597061\n",
      "Epoch  2163  G loss  0.033069447\n",
      "Epoch  2164  G loss  0.11346518\n",
      "Epoch  2165  G loss  0.08905283\n",
      "Epoch  2166  G loss  0.14679979\n",
      "Epoch  2167  G loss  0.062089663\n",
      "Epoch  2168  G loss  0.088667475\n",
      "Epoch  2169  G loss  0.049897593\n",
      "Epoch  2170  G loss  0.06773346\n",
      "Epoch  2171  G loss  0.05938531\n",
      "Epoch  2172  G loss  0.03593272\n",
      "Epoch  2173  G loss  0.027066356\n",
      "Epoch  2174  G loss  0.050620392\n",
      "Epoch  2175  G loss  0.10629418\n",
      "Epoch  2176  G loss  0.039136246\n",
      "Epoch  2177  G loss  0.022021478\n",
      "Epoch  2178  G loss  0.051416487\n",
      "Epoch  2179  G loss  0.032218885\n",
      "Epoch  2180  G loss  0.02889146\n",
      "Epoch  2181  G loss  0.04768304\n",
      "Epoch  2182  G loss  0.04057743\n",
      "Epoch  2183  G loss  0.04703318\n",
      "Epoch  2184  G loss  0.082533196\n",
      "Epoch  2185  G loss  0.02886265\n",
      "Epoch  2186  G loss  0.086868465\n",
      "Epoch  2187  G loss  0.038022626\n",
      "Epoch  2188  G loss  0.061189353\n",
      "Epoch  2189  G loss  0.045224976\n",
      "Epoch  2190  G loss  0.057120446\n",
      "Epoch  2191  G loss  0.05387948\n",
      "Epoch  2192  G loss  0.037713144\n",
      "Epoch  2193  G loss  0.029466035\n",
      "Epoch  2194  G loss  0.046590034\n",
      "Epoch  2195  G loss  0.043939114\n",
      "Epoch  2196  G loss  0.111539364\n",
      "Epoch  2197  G loss  0.048027605\n",
      "Epoch  2198  G loss  0.038383838\n",
      "Epoch  2199  G loss  0.053322643\n",
      "Epoch  2200  G loss  0.02805528\n",
      "Epoch  2201  G loss  0.03549141\n",
      "Epoch  2202  G loss  0.051587343\n",
      "Epoch  2203  G loss  0.090955734\n",
      "Epoch  2204  G loss  0.054599404\n",
      "Epoch  2205  G loss  0.024457112\n",
      "Epoch  2206  G loss  0.050976336\n",
      "Epoch  2207  G loss  0.11093668\n",
      "Epoch  2208  G loss  0.028043473\n",
      "Epoch  2209  G loss  0.038837086\n",
      "Epoch  2210  G loss  0.039769318\n",
      "Epoch  2211  G loss  0.045522198\n",
      "Epoch  2212  G loss  0.055140942\n",
      "Epoch  2213  G loss  0.029882358\n",
      "Epoch  2214  G loss  0.044969548\n",
      "Epoch  2215  G loss  0.043936204\n",
      "Epoch  2216  G loss  0.031019619\n",
      "Epoch  2217  G loss  0.03961334\n",
      "Epoch  2218  G loss  0.029844558\n",
      "Epoch  2219  G loss  0.043210804\n",
      "Epoch  2220  G loss  0.057610285\n",
      "Epoch  2221  G loss  0.059401464\n",
      "Epoch  2222  G loss  0.027437368\n",
      "Epoch  2223  G loss  0.036449436\n",
      "Epoch  2224  G loss  0.028022116\n",
      "Epoch  2225  G loss  0.056022555\n",
      "Epoch  2226  G loss  0.026004905\n",
      "Epoch  2227  G loss  0.060689878\n",
      "Epoch  2228  G loss  0.0925236\n",
      "Epoch  2229  G loss  0.04904309\n",
      "Epoch  2230  G loss  0.07348552\n",
      "Epoch  2231  G loss  0.0193059\n",
      "Epoch  2232  G loss  0.054003995\n",
      "Epoch  2233  G loss  0.042464882\n",
      "Epoch  2234  G loss  0.1407517\n",
      "Epoch  2235  G loss  0.040648818\n",
      "Epoch  2236  G loss  0.052272018\n",
      "Epoch  2237  G loss  0.048639715\n",
      "Epoch  2238  G loss  0.03866482\n",
      "Epoch  2239  G loss  0.05097401\n",
      "Epoch  2240  G loss  0.047600668\n",
      "Epoch  2241  G loss  0.018569358\n",
      "Epoch  2242  G loss  0.09700968\n",
      "Epoch  2243  G loss  0.03746409\n",
      "Epoch  2244  G loss  0.06274719\n",
      "Epoch  2245  G loss  0.036909785\n",
      "Epoch  2246  G loss  0.058577254\n",
      "Epoch  2247  G loss  0.12160734\n",
      "Epoch  2248  G loss  0.09040763\n",
      "Epoch  2249  G loss  0.030649649\n",
      "Epoch  2250  G loss  0.049098626\n",
      "Epoch  2251  G loss  0.02429024\n",
      "Epoch  2252  G loss  0.054665368\n",
      "Epoch  2253  G loss  0.030268947\n",
      "Epoch  2254  G loss  0.050029\n",
      "Epoch  2255  G loss  0.08513514\n",
      "Epoch  2256  G loss  0.16294585\n",
      "Epoch  2257  G loss  0.024359977\n",
      "Epoch  2258  G loss  0.03009305\n",
      "Epoch  2259  G loss  0.06600338\n",
      "Epoch  2260  G loss  0.023678057\n",
      "Epoch  2261  G loss  0.047713324\n",
      "Epoch  2262  G loss  0.05278365\n",
      "Epoch  2263  G loss  0.14262615\n",
      "Epoch  2264  G loss  0.029293953\n",
      "Epoch  2265  G loss  0.031462323\n",
      "Epoch  2266  G loss  0.051124275\n",
      "Epoch  2267  G loss  0.024014624\n",
      "Epoch  2268  G loss  0.031423565\n",
      "Epoch  2269  G loss  0.05922385\n",
      "Epoch  2270  G loss  0.09598038\n",
      "Epoch  2271  G loss  0.02598477\n",
      "Epoch  2272  G loss  0.024161568\n",
      "Epoch  2273  G loss  0.05797403\n",
      "Epoch  2274  G loss  0.02507379\n",
      "Epoch  2275  G loss  0.052634988\n",
      "Epoch  2276  G loss  0.04169348\n",
      "Epoch  2277  G loss  0.031102503\n",
      "Epoch  2278  G loss  0.031341363\n",
      "Epoch  2279  G loss  0.09914017\n",
      "Epoch  2280  G loss  0.026745575\n",
      "Epoch  2281  G loss  0.02141239\n",
      "Epoch  2282  G loss  0.06909803\n",
      "Epoch  2283  G loss  0.023841701\n",
      "Epoch  2284  G loss  0.062621295\n",
      "Epoch  2285  G loss  0.0845628\n",
      "Epoch  2286  G loss  0.08693255\n",
      "Epoch  2287  G loss  0.05881359\n",
      "Epoch  2288  G loss  0.19264805\n",
      "Epoch  2289  G loss  0.09479455\n",
      "Epoch  2290  G loss  0.056563526\n",
      "Epoch  2291  G loss  0.032650664\n",
      "Epoch  2292  G loss  0.016918989\n",
      "Epoch  2293  G loss  0.08499768\n",
      "Epoch  2294  G loss  0.0634175\n",
      "Epoch  2295  G loss  0.053074125\n",
      "Epoch  2296  G loss  0.020072179\n",
      "Epoch  2297  G loss  0.036243517\n",
      "Epoch  2298  G loss  0.027443955\n",
      "Epoch  2299  G loss  0.022066416\n",
      "Epoch  2300  G loss  0.07462322\n",
      "Epoch  2301  G loss  0.09896243\n",
      "Epoch  2302  G loss  0.04850629\n",
      "Epoch  2303  G loss  0.05565739\n",
      "Epoch  2304  G loss  0.063606836\n",
      "Epoch  2305  G loss  0.029205993\n",
      "Epoch  2306  G loss  0.056626797\n",
      "Epoch  2307  G loss  0.038500957\n",
      "Epoch  2308  G loss  0.020462262\n",
      "Epoch  2309  G loss  0.020479055\n",
      "Epoch  2310  G loss  0.026992938\n",
      "Epoch  2311  G loss  0.023596838\n",
      "Epoch  2312  G loss  0.04506114\n",
      "Epoch  2313  G loss  0.10358772\n",
      "Epoch  2314  G loss  0.090048514\n",
      "Epoch  2315  G loss  0.041564893\n",
      "Epoch  2316  G loss  0.18569815\n",
      "Epoch  2317  G loss  0.03606596\n",
      "Epoch  2318  G loss  0.028766334\n",
      "Epoch  2319  G loss  0.0297951\n",
      "Epoch  2320  G loss  0.08165874\n",
      "Epoch  2321  G loss  0.054171298\n",
      "Epoch  2322  G loss  0.037599023\n",
      "Epoch  2323  G loss  0.019166349\n",
      "Epoch  2324  G loss  0.030008487\n",
      "Epoch  2325  G loss  0.0502114\n",
      "Epoch  2326  G loss  0.0374703\n",
      "Epoch  2327  G loss  0.04987583\n",
      "Epoch  2328  G loss  0.032750215\n",
      "Epoch  2329  G loss  0.037874024\n",
      "Epoch  2330  G loss  0.07087683\n",
      "Epoch  2331  G loss  0.02771647\n",
      "Epoch  2332  G loss  0.023748437\n",
      "Epoch  2333  G loss  0.027352491\n",
      "Epoch  2334  G loss  0.04342592\n",
      "Epoch  2335  G loss  0.020778898\n",
      "Epoch  2336  G loss  0.033321228\n",
      "Epoch  2337  G loss  0.038573373\n",
      "Epoch  2338  G loss  0.03284814\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  2339  G loss  0.09801397\n",
      "Epoch  2340  G loss  0.029821187\n",
      "Epoch  2341  G loss  0.02136259\n",
      "Epoch  2342  G loss  0.0341586\n",
      "Epoch  2343  G loss  0.029102266\n",
      "Epoch  2344  G loss  0.07739315\n",
      "Epoch  2345  G loss  0.029334119\n",
      "Epoch  2346  G loss  0.17840303\n",
      "Epoch  2347  G loss  0.036942635\n",
      "Epoch  2348  G loss  0.06390669\n",
      "Epoch  2349  G loss  0.028603489\n",
      "Epoch  2350  G loss  0.11921552\n",
      "Epoch  2351  G loss  0.06901559\n",
      "Epoch  2352  G loss  0.03630409\n",
      "Epoch  2353  G loss  0.027946865\n",
      "Epoch  2354  G loss  0.03113582\n",
      "Epoch  2355  G loss  0.025738975\n",
      "Epoch  2356  G loss  0.050302222\n",
      "Epoch  2357  G loss  0.074193716\n",
      "Epoch  2358  G loss  0.045872886\n",
      "Epoch  2359  G loss  0.06806677\n",
      "Epoch  2360  G loss  0.030454652\n",
      "Epoch  2361  G loss  0.018308545\n",
      "Epoch  2362  G loss  0.0971866\n",
      "Epoch  2363  G loss  0.027878953\n",
      "Epoch  2364  G loss  0.09464758\n",
      "Epoch  2365  G loss  0.059728812\n",
      "Epoch  2366  G loss  0.02741322\n",
      "Epoch  2367  G loss  0.041704405\n",
      "Epoch  2368  G loss  0.018323988\n",
      "Epoch  2369  G loss  0.07141618\n",
      "Epoch  2370  G loss  0.052302327\n",
      "Epoch  2371  G loss  0.032161552\n",
      "Epoch  2372  G loss  0.05128765\n",
      "Epoch  2373  G loss  0.041450944\n",
      "Epoch  2374  G loss  0.020252476\n",
      "Epoch  2375  G loss  0.15082754\n",
      "Epoch  2376  G loss  0.082715206\n",
      "Epoch  2377  G loss  0.15749045\n",
      "Epoch  2378  G loss  0.030659193\n",
      "Epoch  2379  G loss  0.038203\n",
      "Epoch  2380  G loss  0.028686648\n",
      "Epoch  2381  G loss  0.032000247\n",
      "Epoch  2382  G loss  0.06085193\n",
      "Epoch  2383  G loss  0.038394343\n",
      "Epoch  2384  G loss  0.033203617\n",
      "Epoch  2385  G loss  0.066043295\n",
      "Epoch  2386  G loss  0.07115591\n",
      "Epoch  2387  G loss  0.264208\n",
      "Epoch  2388  G loss  0.055261146\n",
      "Epoch  2389  G loss  0.04779284\n",
      "Epoch  2390  G loss  0.048543986\n",
      "Epoch  2391  G loss  0.081684165\n",
      "Epoch  2392  G loss  0.045116305\n",
      "Epoch  2393  G loss  0.05504605\n",
      "Epoch  2394  G loss  0.05100897\n",
      "Epoch  2395  G loss  0.023637995\n",
      "Epoch  2396  G loss  0.024837956\n",
      "Epoch  2397  G loss  0.095080145\n",
      "Epoch  2398  G loss  0.06868584\n",
      "Epoch  2399  G loss  0.07428882\n",
      "Epoch  2400  G loss  0.02744022\n",
      "Epoch  2401  G loss  0.031151215\n",
      "Epoch  2402  G loss  0.02800245\n",
      "Epoch  2403  G loss  0.036737643\n",
      "Epoch  2404  G loss  0.03904008\n",
      "Epoch  2405  G loss  0.03256224\n",
      "Epoch  2406  G loss  0.036053028\n",
      "Epoch  2407  G loss  0.1269318\n",
      "Epoch  2408  G loss  0.03690436\n",
      "Epoch  2409  G loss  0.019523706\n",
      "Epoch  2410  G loss  0.058086526\n",
      "Epoch  2411  G loss  0.041061435\n",
      "Epoch  2412  G loss  0.032350626\n",
      "Epoch  2413  G loss  0.049749356\n",
      "Epoch  2414  G loss  0.02786097\n",
      "Epoch  2415  G loss  0.0651649\n",
      "Epoch  2416  G loss  0.11686969\n",
      "Epoch  2417  G loss  0.043117065\n",
      "Epoch  2418  G loss  0.03013097\n",
      "Epoch  2419  G loss  0.026053721\n",
      "Epoch  2420  G loss  0.04031998\n",
      "Epoch  2421  G loss  0.09379381\n",
      "Epoch  2422  G loss  0.02520688\n",
      "Epoch  2423  G loss  0.0711467\n",
      "Epoch  2424  G loss  0.060577285\n",
      "Epoch  2425  G loss  0.053254258\n",
      "Epoch  2426  G loss  0.041769877\n",
      "Epoch  2427  G loss  0.075603224\n",
      "Epoch  2428  G loss  0.03650872\n",
      "Epoch  2429  G loss  0.07237479\n",
      "Epoch  2430  G loss  0.03456791\n",
      "Epoch  2431  G loss  0.06390271\n",
      "Epoch  2432  G loss  0.021834379\n",
      "Epoch  2433  G loss  0.06008057\n",
      "Epoch  2434  G loss  0.05492571\n",
      "Epoch  2435  G loss  0.02929707\n",
      "Epoch  2436  G loss  0.033092145\n",
      "Epoch  2437  G loss  0.059416678\n",
      "Epoch  2438  G loss  0.07871445\n",
      "Epoch  2439  G loss  0.03082708\n",
      "Epoch  2440  G loss  0.028045498\n",
      "Epoch  2441  G loss  0.07380849\n",
      "Epoch  2442  G loss  0.025745263\n",
      "Epoch  2443  G loss  0.02259957\n",
      "Epoch  2444  G loss  0.06427575\n",
      "Epoch  2445  G loss  0.05219984\n",
      "Epoch  2446  G loss  0.03499339\n",
      "Epoch  2447  G loss  0.024205526\n",
      "Epoch  2448  G loss  0.064376436\n",
      "Epoch  2449  G loss  0.030617585\n",
      "Epoch  2450  G loss  0.03187644\n",
      "Epoch  2451  G loss  0.03439878\n",
      "Epoch  2452  G loss  0.13006638\n",
      "Epoch  2453  G loss  0.06358383\n",
      "Epoch  2454  G loss  0.08173871\n",
      "Epoch  2455  G loss  0.06136632\n",
      "Epoch  2456  G loss  0.082438804\n",
      "Epoch  2457  G loss  0.03142076\n",
      "Epoch  2458  G loss  0.14949638\n",
      "Epoch  2459  G loss  0.019678941\n",
      "Epoch  2460  G loss  0.05390354\n",
      "Epoch  2461  G loss  0.047349587\n",
      "Epoch  2462  G loss  0.021127319\n",
      "Epoch  2463  G loss  0.029778471\n",
      "Epoch  2464  G loss  0.025955344\n",
      "Epoch  2465  G loss  0.030882113\n",
      "Epoch  2466  G loss  0.024872487\n",
      "Epoch  2467  G loss  0.041026827\n",
      "Epoch  2468  G loss  0.111761175\n",
      "Epoch  2469  G loss  0.14959796\n",
      "Epoch  2470  G loss  0.022981837\n",
      "Epoch  2471  G loss  0.045180574\n",
      "Epoch  2472  G loss  0.015671564\n",
      "Epoch  2473  G loss  0.03958423\n",
      "Epoch  2474  G loss  0.036841173\n",
      "Epoch  2475  G loss  0.024703277\n",
      "Epoch  2476  G loss  0.024892509\n",
      "Epoch  2477  G loss  0.053053673\n",
      "Epoch  2478  G loss  0.061819624\n",
      "Epoch  2479  G loss  0.055770528\n",
      "Epoch  2480  G loss  0.02590181\n",
      "Epoch  2481  G loss  0.047364373\n",
      "Epoch  2482  G loss  0.023394749\n",
      "Epoch  2483  G loss  0.032790896\n",
      "Epoch  2484  G loss  0.03134552\n",
      "Epoch  2485  G loss  0.02294966\n",
      "Epoch  2486  G loss  0.023181425\n",
      "Epoch  2487  G loss  0.05260256\n",
      "Epoch  2488  G loss  0.06264673\n",
      "Epoch  2489  G loss  0.025755458\n",
      "Epoch  2490  G loss  0.016730944\n",
      "Epoch  2491  G loss  0.021696687\n",
      "Epoch  2492  G loss  0.068439186\n",
      "Epoch  2493  G loss  0.030499125\n",
      "Epoch  2494  G loss  0.2601591\n",
      "Epoch  2495  G loss  0.06403944\n",
      "Epoch  2496  G loss  0.025383743\n",
      "Epoch  2497  G loss  0.060452227\n",
      "Epoch  2498  G loss  0.02475092\n",
      "Epoch  2499  G loss  0.046380743\n",
      "Epoch  2500  G loss  0.04000453\n",
      "Epoch  2501  G loss  0.03677753\n",
      "Epoch  2502  G loss  0.055108\n",
      "Epoch  2503  G loss  0.07116573\n",
      "Epoch  2504  G loss  0.0220645\n",
      "Epoch  2505  G loss  0.05876921\n",
      "Epoch  2506  G loss  0.035044756\n",
      "Epoch  2507  G loss  0.025524577\n",
      "Epoch  2508  G loss  0.023736134\n",
      "Epoch  2509  G loss  0.04023342\n",
      "Epoch  2510  G loss  0.06785988\n",
      "Epoch  2511  G loss  0.027286284\n",
      "Epoch  2512  G loss  0.041157987\n",
      "Epoch  2513  G loss  0.01806505\n",
      "Epoch  2514  G loss  0.07819275\n",
      "Epoch  2515  G loss  0.11245545\n",
      "Epoch  2516  G loss  0.11972785\n",
      "Epoch  2517  G loss  0.026850207\n",
      "Epoch  2518  G loss  0.023272695\n",
      "Epoch  2519  G loss  0.055477764\n",
      "Epoch  2520  G loss  0.03528552\n",
      "Epoch  2521  G loss  0.039715003\n",
      "Epoch  2522  G loss  0.018957539\n",
      "Epoch  2523  G loss  0.40040207\n",
      "Epoch  2524  G loss  0.02935146\n",
      "Epoch  2525  G loss  0.08425809\n",
      "Epoch  2526  G loss  0.03951961\n",
      "Epoch  2527  G loss  0.023374597\n",
      "Epoch  2528  G loss  0.019124754\n",
      "Epoch  2529  G loss  0.12893087\n",
      "Epoch  2530  G loss  0.06877933\n",
      "Epoch  2531  G loss  0.12217042\n",
      "Epoch  2532  G loss  0.023776522\n",
      "Epoch  2533  G loss  0.030046513\n",
      "Epoch  2534  G loss  0.043044474\n",
      "Epoch  2535  G loss  0.14283933\n",
      "Epoch  2536  G loss  0.03070972\n",
      "Epoch  2537  G loss  0.08582815\n",
      "Epoch  2538  G loss  0.04058035\n",
      "Epoch  2539  G loss  0.045337886\n",
      "Epoch  2540  G loss  0.03838128\n",
      "Epoch  2541  G loss  0.027022451\n",
      "Epoch  2542  G loss  0.13161339\n",
      "Epoch  2543  G loss  0.054557186\n",
      "Epoch  2544  G loss  0.044455905\n",
      "Epoch  2545  G loss  0.0772462\n",
      "Epoch  2546  G loss  0.021435156\n",
      "Epoch  2547  G loss  0.036779124\n",
      "Epoch  2548  G loss  0.016516775\n",
      "Epoch  2549  G loss  0.031978816\n",
      "Epoch  2550  G loss  0.07127993\n",
      "Epoch  2551  G loss  0.03934268\n",
      "Epoch  2552  G loss  0.036461856\n",
      "Epoch  2553  G loss  0.12915383\n",
      "Epoch  2554  G loss  0.018981116\n",
      "Epoch  2555  G loss  0.07207871\n",
      "Epoch  2556  G loss  0.0334537\n",
      "Epoch  2557  G loss  0.050662518\n",
      "Epoch  2558  G loss  0.031643543\n",
      "Epoch  2559  G loss  0.03904433\n",
      "Epoch  2560  G loss  0.04556727\n",
      "Epoch  2561  G loss  0.0980239\n",
      "Epoch  2562  G loss  0.061906543\n",
      "Epoch  2563  G loss  0.04355068\n",
      "Epoch  2564  G loss  0.04470591\n",
      "Epoch  2565  G loss  0.038605813\n",
      "Epoch  2566  G loss  0.13726611\n",
      "Epoch  2567  G loss  0.020170689\n",
      "Epoch  2568  G loss  0.097627886\n",
      "Epoch  2569  G loss  0.050349455\n",
      "Epoch  2570  G loss  0.077489644\n",
      "Epoch  2571  G loss  0.058171153\n",
      "Epoch  2572  G loss  0.020401558\n",
      "Epoch  2573  G loss  0.03472685\n",
      "Epoch  2574  G loss  0.050055336\n",
      "Epoch  2575  G loss  0.04095614\n",
      "Epoch  2576  G loss  0.035269815\n",
      "Epoch  2577  G loss  0.06908945\n",
      "Epoch  2578  G loss  0.057952743\n",
      "Epoch  2579  G loss  0.03979573\n",
      "Epoch  2580  G loss  0.07317152\n",
      "Epoch  2581  G loss  0.019483225\n",
      "Epoch  2582  G loss  0.04135772\n",
      "Epoch  2583  G loss  0.14202948\n",
      "Epoch  2584  G loss  0.119823836\n",
      "Epoch  2585  G loss  0.026604772\n",
      "Epoch  2586  G loss  0.039407406\n",
      "Epoch  2587  G loss  0.07037978\n",
      "Epoch  2588  G loss  0.06600729\n",
      "Epoch  2589  G loss  0.02604635\n",
      "Epoch  2590  G loss  0.03472721\n",
      "Epoch  2591  G loss  0.03860124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  2592  G loss  0.03349259\n",
      "Epoch  2593  G loss  0.08412558\n",
      "Epoch  2594  G loss  0.05136776\n",
      "Epoch  2595  G loss  0.023365429\n",
      "Epoch  2596  G loss  0.022638379\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-c715169f7cb8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcgan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCGAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_autoencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_generator_autoencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-35-535c30887aee>\u001b[0m in \u001b[0;36mtrain_generator_autoencoder\u001b[0;34m(self, epochs, batch_size, sample_interval)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmyGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m             \u001b[0mg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m             \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" G loss \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msample_interval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cgan = CGAN()\n",
    "cgan.build_autoencoder()\n",
    "cgan.train_generator_autoencoder(100000, 12, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:12: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "  if sys.path[0] == '':\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:13: DeprecationWarning: `imsave` is deprecated!\n",
      "`imsave` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imwrite`` instead.\n",
      "  del sys.path[0]\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:14: DeprecationWarning: `imsave` is deprecated!\n",
      "`imsave` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imwrite`` instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    x = myGenerator(1)\n",
    "    xtest, ytest = next(x)\n",
    "    pred = cgan.generator.predict(xtest)\n",
    "    pred = pred*127.5 + 127.5\n",
    "    pred = pred.astype(int)\n",
    "    #plt.imshow(pred[0])\n",
    "    #plt.show()\n",
    "    ytest = ytest*127.5+127.5\n",
    "    ytest = ytest.astype(int)\n",
    "    #plt.imshow(ytest[0])\n",
    "    #plt.show()\n",
    "    imsave(path+'test/frame_pred'+str(i)+'.png', pred[0])\n",
    "    imsave(path+'test/frame_real'+str(i)+'.png', ytest[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy \n",
    "import math\n",
    "import cv2\n",
    "\n",
    "def psnr(img1, img2):\n",
    "    mse = numpy.mean( (img1 - img2) ** 2 )\n",
    "    if mse == 0:\n",
    "        return 100\n",
    "    PIXEL_MAX = 255.0\n",
    "    return 20 * math.log10(PIXEL_MAX / math.sqrt(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PSNR: 31.612699630864867\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "for i in range(1000):\n",
    "    img1 = cv2.imread(path+'test/frame_real'+str(i)+'.png')\n",
    "    img2 = cv2.imread(path+'test/frame_pred'+str(i)+'.png')\n",
    "    d = psnr(img1, img2)\n",
    "    data.append(d)\n",
    "print('PSNR:', np.mean(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
