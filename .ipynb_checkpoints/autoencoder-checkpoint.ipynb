{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 2,
=======
   "execution_count": 1,
>>>>>>> superresolution
=======
   "execution_count": 1,
>>>>>>> autoencoder
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, multiply, GlobalAveragePooling2D, MaxPooling2D\n",
    "from keras.layers import BatchNormalization, Activation, Embedding, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU, PReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from scipy.misc import imread\n",
    "from scipy.misc import imresize\n",
    "from scipy.misc import imsave\n",
    "import glob\n",
    "import random\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import applications\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import to_categorical\n",
    "#from keras.applications.resnet50 import preprocess_input\n",
    "\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "#from keras.applications.vgg19 import preprocess_input as preprocess_vgg\n",
    "from keras.applications.inception_v3 import preprocess_input as inception_preprocess\n",
    "\n",
    "#from keras.applications.inception_resnet_v2 import preprocess_input\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import keras\n",
    "import copy\n",
    "import cv2\n",
<<<<<<< HEAD
    "\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "\n",
    "import numpy as np \n",
    "import os\n",
    "import numpy as np\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras import backend as keras"
=======
    "import os"
>>>>>>> ganvanilla
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 3,
=======
   "execution_count": 2,
>>>>>>> superresolution
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../data/'\n",
=======
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../data/sources/nparrs_384/'\n",
>>>>>>> autoencoder
    "savepath = '../data/'\n",
    "images = glob.glob(path+'new_data/*.npy')"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 95,
=======
   "execution_count": 4,
>>>>>>> ganvanilla
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_sizes = np.loadtxt(path+'tile_sizes.txt', dtype='int')\n",
    "images_sampled = {}\n",
    "for tile in tile_sizes:\n",
    "    if tile[2] > 200000:\n",
    "        for i in range(30):\n",
    "            images_sampled.setdefault(tile[0]*30+i, []).append(tile[1])"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 11,
=======
   "execution_count": 15,
>>>>>>> autoencoder
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "1615\n",
      "1039\n",
      "992\n",
      "995\n",
      "1042\n",
      "1087\n",
      "1242\n",
      "1038\n",
      "1203\n",
      "1329\n",
      "xtrain shape: (10, 384, 384, 3)\n",
      "ytrain shape: (10, 384, 384, 3)\n"
=======
      "xtrain shape: (90, 384, 384, 3)\n",
      "ytrain shape: (90, 384, 384, 3)\n"
>>>>>>> autoencoder
     ]
    }
   ],
=======
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
>>>>>>> superresolution
   "source": [
    "def myGenerator(batch_size):\n",
    "    while True:\n",
    "        #index_list = random.sample(range(1, totalImages), batch_size)\n",
    "        index_list = random.sample(images_sampled.keys(), batch_size)\n",
    "        alldata_x = []\n",
    "        alldata_y = []\n",
    "        \n",
    "        for i in index_list:\n",
    "            print (i)\n",
    "            frame = path+'sources/new_data/frame'+str(i)+'.npy'\n",
    "            frame = np.load(frame)\n",
<<<<<<< HEAD
<<<<<<< HEAD
    "            tile_index = np.random.randint(0, 199)\n",
    "            #print(i, tile_index, frame.shape)\n",
<<<<<<< HEAD
    "            alldata_x.append(tile_index*totalImages+i)\n",
=======
    "            #tile_index = np.random.randint(0, 199)\n",
    "            #print(i, tile_index, frame.shape, images_sampled[i])\n",
    "            #alldata_x.append(tile_index*totalImages+i)\n",
>>>>>>> ganvanilla
=======
    "            \n",
    "            temp  = imresize(frame[tile_index], (48, 48))\n",
    "            temp  = imresize(temp, (192, 192))\n",
    "            \n",
    "            alldata_x.append(temp)\n",
>>>>>>> superresolution
    "            alldata_y.append(frame[tile_index])\n",
    "        \n",
    "        alldata_x = np.array(alldata_x)\n",
    "        alldata_y = np.array(alldata_y)\n",
<<<<<<< HEAD
    "        #alldata_y = alldata_y/255.0\n",
    "        alldata_y = (alldata_y.astype(np.float32) - 127.5) / 127.5\n",
    "        yield alldata_y, alldata_y\n",
<<<<<<< HEAD
=======
    "        \n",
    "        alldata_y = (alldata_y.astype(np.float32) - 127.5) / 127.5\n",
    "        alldata_x = alldata_x.astype(np.float32)/255.0\n",
    "        \n",
    "        yield alldata_x, alldata_y\n",
    "\n",
>>>>>>> superresolution
    "# x = myGenerator(10)\n",
    "# xtrain, ytrain = next(x)\n",
    "# print('xtrain shape:',xtrain.shape)\n",
    "# print('ytrain shape:',ytrain.shape)"
=======
    "            \n",
    "            tile_index = np.random.randint(0, 199)\n",
    "            temp       = tile_index*totalImages+i\n",
    "            b          = frame[tile_index][:, :, :]\n",
    "            \n",
    "            alldata_y.append(b)\n",
    "            temp       = imresize(b, (12, 12))\n",
    "            tempa      = imresize(temp, (192, 192))\n",
    "            \n",
    "            alldata_x.append(tempa)\n",
    "        \n",
    "        alldata_x = np.array(alldata_x)\n",
    "        alldata_y = np.array(alldata_y)\n",
    "        \n",
    "        #alldata_y = (alldata_y.astype(np.float32) - 127.5) / 127.5\n",
    "        #alldata_x = (alldata_x.astype(np.float32) - 127.5) / 127.5\n",
    "        yield alldata_x, alldata_y\n",
    "\n",
    "x = myGenerator(10)\n",
    "xtrain, ytrain = next(x)\n",
    "print('xtrain shape:',xtrain.shape)\n",
    "print('ytrain shape:',ytrain.shape)"
>>>>>>> autoencoder
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 12,
=======
   "execution_count": 5,
>>>>>>> autoencoder
   "metadata": {},
   "outputs": [],
   "source": [
    "def myGenerator_new(batch_size):\n",
    "    while True:\n",
    "        index_list = random.sample(range(1, totalImages), batch_size)\n",
    "        alldata_x = []\n",
    "        alldata_y = []\n",
    "        cls = []\n",
    "        for i in index_list:\n",
    "            frame = images[i]\n",
    "            frame = np.load(frame)\n",
    "            tdx = np.random.randint(0, 199)\n",
    "            choices = None\n",
    "            classes = {}\n",
    "            if tdx < 20:\n",
    "                if tdx == 0:\n",
    "                    choices = [1, 20, 21]\n",
    "                    classes = {1:5, 20:7, 21:8}\n",
    "                elif tdx == 19:\n",
    "                    choices = [18, 38, 39]\n",
    "                    classes = {18:3, 38:6, 39:7}\n",
    "                else:\n",
    "                    choices = [tdx-1, tdx+1, tdx+19, tdx+20, tdx+21]\n",
    "                    classes = {tdx-1:3, tdx+1:5, tdx+19:6, tdx+20:7, tdx+21:8}\n",
    "            elif tdx >= 179:\n",
    "                if tdx == 199:\n",
    "                    choices = [178, 179, 198]\n",
    "                    classes = {178:0, 179:1, 198:3}\n",
    "                elif tdx == 179:\n",
    "                    choices = [158, 159, 160, 178, 180, 198, 199]\n",
    "                    classes = {158:0, 159:1, 160:2, 178:3, 180:5, 198:6, 199:7}\n",
    "                elif tdx == 180:\n",
    "                    choices = [160, 161, 181]\n",
    "                    classes = {160:0, 161:1, 181:5}\n",
    "                else:\n",
    "                    choices = [tdx-21, tdx-20, tdx-19, tdx-1, tdx+1]\n",
    "                    classes = {tdx-21:0, tdx-20:1, tdx-19:2, tdx-1:3, tdx+1:5}\n",
    "            else:\n",
    "                choices = [tdx-21, tdx-20, tdx-19, tdx-1, tdx+1, tdx+19, tdx+20, tdx+21]\n",
    "                classes = {tdx-21:0, tdx-20:1, tdx-19:2, tdx-1:3, tdx+1:5, tdx+19:6, tdx+20:7, tdx+21:8}\n",
    "            choice = random.choice(choices)\n",
    "            #print(i, tdx, choice, choices)\n",
    "            cls.append(classes[choice])\n",
    "            #print(i, tdx, choice, cls, choices)\n",
    "            alldata_x.append(frame[tdx])\n",
    "            alldata_y.append(frame[choice])\n",
    "        alldata_x = np.array(alldata_x)\n",
    "        alldata_y = np.array(alldata_y)\n",
    "        alldata_y = (alldata_y.astype(np.float32) - 127.5) / 127.5\n",
<<<<<<< HEAD
    "        yield alldata_x, alldata_y, np.array(cls)\n",
    "#x = myGenerator_new(1000)\n",
    "#xtrain, ytrain, cls = next(x)\n",
    "#print('xtrain shape:',xtrain.shape)\n",
    "#print('ytrain shape:',ytrain.shape)"
=======
    "x = myGenerator(10)\n",
    "xtrain, ytrain = next(x)\n",
    "print('xtrain shape:',xtrain.shape)\n",
    "print('ytrain shape:',ytrain.shape)"
>>>>>>> ganvanilla
=======
   "execution_count": 47,
=======
    "        yield alldata_x, alldata_y, cls\n",
    "# x = myGenerator_new(2)\n",
    "# xtrain, ytrain, cls = next(x)\n",
    "# print('xtrain shape:',xtrain.shape)\n",
    "# print('ytrain shape:',ytrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
>>>>>>> autoencoder
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: `imsave` is deprecated!\n",
      "`imsave` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imwrite`` instead.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "imsave(savepath+\"images/_realtest.jpg\", ytrain[4])"
>>>>>>> superresolution
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 30,
=======
   "execution_count": 5,
>>>>>>> ganvanilla
=======
   "execution_count": 4,
>>>>>>> superresolution
=======
   "execution_count": 7,
>>>>>>> autoencoder
   "metadata": {},
   "outputs": [],
   "source": [
    "class CGAN():\n",
    "    def __init__(self):\n",
    "        # Input shape\n",
    "        self.img_rows = 384\n",
    "        self.img_cols = 384\n",
    "        self.channels = 3\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.optimizer = Adam(0.0002, 0.5)\n",
<<<<<<< HEAD
    "        \n",
    "        \n",
    "        self.num_classes     = 9\n",
    "        self.latent_dim      = 200\n",
    "        self.embedding_layer = Embedding(self.num_classes, self.latent_dim)\n",
    "        \n",
=======
    "        self.latent_dim = 300\n",
>>>>>>> ganvanilla
    "        \n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss=['mse'],\n",
<<<<<<< HEAD
    "           optimizer=self.optimizer,\n",
    "           metrics=['accuracy'])\n",
=======
    "            optimizer=self.optimizer,\n",
    "            metrics=['accuracy'])\n",
>>>>>>> ganvanilla
    "        \n",
    "        #print(self.discriminator.summary())\n",
    "        \n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "        \n",
    "        print(self.generator.summary())\n",
    "        \n",
<<<<<<< HEAD
<<<<<<< HEAD
    "        noise     = Input(shape=(192, 192, 3))\n",
    "        out_image = Input(shape=(192, 192, 3))\n",
    "        label     = Input(shape=(1,))\n",
    "        \n",
    "        img       = self.generator([noise, label])\n",
=======
    "        noise = Input(shape=(self.latent_dim, ))\n",
=======
    "        noise = Input(shape=(384, 384, 3))\n",
>>>>>>> autoencoder
    "        img = self.generator(noise)\n",
>>>>>>> ganvanilla
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The discriminator takes generated image as input and determines validity\n",
    "        # and the label of that image\n",
<<<<<<< HEAD
    "        valid = self.discriminator([img, label])\n",
    "        \n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains generator to fool discriminator\n",
    "        self.combined = Model([noise, label], [img, valid])\n",
    "        self.combined.compile(loss=['mse', 'binary_crossentropy'],\n",
    "           loss_weights=[0.99, 0.01],\n",
    "           optimizer=self.optimizer)\n",
    "\n",
    "    def build_generator(self):\n",
<<<<<<< HEAD
    "        label      = Input(shape=(1,), dtype='int32')\n",
    "        input_size = (192,192,3)\n",
    "        inputs     = Input(input_size)\n",
    "        \n",
    "        \n",
    "        label_embedding  = Flatten()(self.embedding_layer(label))\n",
    "        label_dense      = Dense(81)(label_embedding)\n",
    "        embedding_vector = Reshape((9, 9, 1))(label_dense)\n",
    "        \n",
    "        \n",
    "        conv1 = Conv2D(64, 3, padding = 'same', kernel_initializer = 'he_normal')(inputs)\n",
    "        conv1 = PReLU()(conv1)\n",
    "        conv1 = BatchNormalization(momentum=0.8)(conv1)\n",
    "\n",
    "\n",
    "\n",
    "        conv2 = Conv2D(64, 3, padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
    "        conv2 = PReLU()(conv2)\n",
    "        conv2 = BatchNormalization(momentum=0.8)(conv2)\n",
    "\n",
    "        conv3 = Conv2D(64, 3, padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
    "        conv3 = PReLU()(conv3)\n",
=======
    "        input_size = (384,384,3)\n",
    "        inputs = Input(input_size)\n",
    "\n",
    "        conv1_a = Conv2D(16, 3, padding = 'same', kernel_initializer = 'he_normal')(inputs)\n",
    "        conv1_a = LeakyReLU(alpha=0.2)(conv1_a)\n",
    "        conv1_a = BatchNormalization(momentum=0.8)(conv1_a)\n",
    "\n",
    "        conv1_a = Conv2D(16, 3, padding = 'valid', kernel_initializer = 'he_normal')(conv1_a)\n",
    "        conv1_a = LeakyReLU(alpha=0.2)(conv1_a)\n",
    "        conv1_a = BatchNormalization(momentum=0.8)(conv1_a)\n",
    "\n",
    "        pool1_a = Conv2D(16, 3, subsample=(2, 2), kernel_initializer = 'he_normal')(conv1_a)\n",
    "        pool1_a = LeakyReLU(alpha=0.2)(pool1_a)\n",
    "        pool1_a = BatchNormalization(momentum=0.8)(pool1_a)\n",
    "        pool1_a = ZeroPadding2D(padding=(1, 1))(pool1_a)\n",
    "\n",
    "\n",
    "\n",
    "        conv1 = Conv2D(32, 3, padding = 'same', kernel_initializer = 'he_normal')(pool1_a)\n",
    "        conv1 = LeakyReLU(alpha=0.2)(conv1)\n",
    "        conv1 = BatchNormalization(momentum=0.8)(conv1)\n",
    "\n",
    "        conv1 = Conv2D(32, 3, padding = 'valid', kernel_initializer = 'he_normal')(conv1)\n",
    "        conv1 = LeakyReLU(alpha=0.2)(conv1)\n",
    "        conv1 = BatchNormalization(momentum=0.8)(conv1)\n",
    "\n",
    "        pool1 = Conv2D(32, 3, subsample=(2, 2), kernel_initializer = 'he_normal')(conv1)\n",
    "        pool1 = LeakyReLU(alpha=0.2)(pool1)\n",
    "        pool1 = BatchNormalization(momentum=0.8)(pool1)\n",
    "\n",
    "\n",
    "\n",
    "        conv2 = Conv2D(64, 3, padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
    "        conv2 = LeakyReLU(alpha=0.2)(conv2)\n",
    "        conv2 = BatchNormalization(momentum=0.8)(conv2)\n",
    "\n",
    "        conv2 = Conv2D(64, 3, padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
    "        conv2 = LeakyReLU(alpha=0.2)(conv2)\n",
    "        conv2 = BatchNormalization(momentum=0.8)(conv2)\n",
    "\n",
    "        pool2 = Conv2D(64, 3, subsample=(2, 2), kernel_initializer = 'he_normal')(conv2)\n",
    "        pool2 = LeakyReLU(alpha=0.2)(pool2)\n",
    "        pool2 = BatchNormalization(momentum=0.8)(pool2)\n",
    "\n",
    "\n",
    "\n",
    "        conv2_a = Conv2D(64, 3, padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
    "        conv2_a = LeakyReLU(alpha=0.2)(conv2_a)\n",
    "        conv2_a = BatchNormalization(momentum=0.8)(conv2_a)\n",
    "\n",
    "        conv2_a = Conv2D(64, 3, padding = 'same', kernel_initializer = 'he_normal')(conv2_a)\n",
    "        conv2_a = LeakyReLU(alpha=0.2)(conv2_a)\n",
    "        conv2_a = BatchNormalization(momentum=0.8)(conv2_a)\n",
    "\n",
    "        pool3   = Conv2D(64, 3, subsample=(2, 2), kernel_initializer = 'he_normal')(conv2_a)\n",
    "        pool3   = LeakyReLU(alpha=0.2)(pool3)\n",
    "        pool3   = BatchNormalization(momentum=0.8)(pool3)\n",
    "\n",
    "\n",
    "\n",
    "        conv3 = Conv2D(64, 3, padding = 'same', kernel_initializer = 'he_normal')(pool3)\n",
    "        conv3 = LeakyReLU(alpha=0.2)(conv3)\n",
    "        conv3 = BatchNormalization(momentum=0.8)(conv3)\n",
    "\n",
    "        conv3 = Conv2D(1, 3, padding = 'valid', kernel_initializer = 'he_normal')(conv3)\n",
    "        conv3 = LeakyReLU(alpha=0.2)(conv3)\n",
>>>>>>> autoencoder
    "        conv3 = BatchNormalization(momentum=0.8)(conv3)\n",
    "\n",
    "        concat1 = add([conv1, conv3])\n",
    "\n",
<<<<<<< HEAD
    "        conv3_pool = Conv2D(1, 3, subsample=(2, 2), kernel_initializer = 'he_normal')(conv3)\n",
    "        conv3_pool = LeakyReLU(alpha=0.2)(conv3_pool)\n",
    "        conv3_pool = BatchNormalization(momentum=0.8)(conv3_pool)\n",
    "        \n",
    "        # Multiplying with the conditional vector\n",
    "        conv3_pool =  multiply([conv3_pool, embedding_vector])\n",
    "        \n",
    "        \n",
    "        # Decoder\n",
    "        conv3_unpool = UpSampling2D(size = (2,2))(conv3_pool)\n",
    "        conv3_unpool = ZeroPadding2D(padding=(1, 1))(conv3_unpool)\n",
    "\n",
    "        conv3_unpool = Conv2D(1, 3, padding = 'same', kernel_initializer = 'he_normal')(conv3_unpool)\n",
    "        conv3_unpool = LeakyReLU(alpha=0.2)(conv3_unpool)\n",
    "        conv3_unpool = BatchNormalization(momentum=0.8)(conv3_unpool)\n",
<<<<<<< HEAD
    "        \n",
=======
    "\n",
    "\n",
>>>>>>> autoencoder
    "        up1    = UpSampling2D(size = (2,2))(conv3_unpool)\n",
    "        #up1    = UpSampling2D(size = (2,2))(conv3)\n",
    "        up1    = ZeroPadding2D(padding=(3, 3))(up1)\n",
=======
    "\n",
>>>>>>> superresolution
    "\n",
<<<<<<< HEAD
    "        conv4 = Conv2D(64, 3, padding = 'same', kernel_initializer = 'he_normal')(concat1)\n",
    "        conv4 = PReLU()(conv4)\n",
    "        conv4 = BatchNormalization(momentum=0.8)(conv4)\n",
    "\n",
    "        conv5 = Conv2D(64, 3, padding = 'same', kernel_initializer = 'he_normal')(conv4)\n",
    "        conv5 = PReLU()(conv5)\n",
    "        conv5 = BatchNormalization(momentum=0.8)(conv5)\n",
    "\n",
    "        concat2 = add([conv5, concat1])\n",
=======
    "        conv4 = Conv2D(64, 3, padding = 'same', kernel_initializer = 'he_normal')(up1)\n",
    "        conv4 = LeakyReLU(alpha=0.2)(conv4)\n",
    "        conv4 = BatchNormalization(momentum=0.8)(conv4)\n",
    "\n",
    "        merge1 = concatenate([conv4, conv2_a], axis = 3)\n",
    "\n",
    "        conv5 = Conv2D(64, 3, padding = 'same', kernel_initializer = 'he_normal')(merge1)\n",
    "        conv5 = LeakyReLU(alpha=0.2)(conv5)\n",
    "        conv5 = BatchNormalization(momentum=0.8)(conv5)\n",
    "\n",
    "        conv5 = Conv2D(64, 3, padding = 'same', kernel_initializer = 'he_normal')(conv5)\n",
    "        conv5 = LeakyReLU(alpha=0.2)(conv5)\n",
    "        conv5 = BatchNormalization(momentum=0.8)(conv5)\n",
>>>>>>> autoencoder
    "\n",
    "\n",
    "\n",
<<<<<<< HEAD
    "        conv6 = Conv2D(64, 3, padding = 'same', kernel_initializer = 'he_normal')(concat2)\n",
    "        conv6 = PReLU()(conv6)\n",
    "        conv6 = BatchNormalization(momentum=0.8)(conv6)\n",
    "\n",
    "        conv7 = Conv2D(64, 3, padding = 'same', kernel_initializer = 'he_normal')(conv6)\n",
    "        conv7 = PReLU()(conv7)\n",
    "        conv7 = BatchNormalization(momentum=0.8)(conv7)\n",
    "\n",
    "        concat3 = add([conv7, concat2])\n",
    "\n",
=======
    "        conv6 = Conv2D(64, 3, padding = 'same', kernel_initializer = 'he_normal')(up2)\n",
    "        conv6 = LeakyReLU(alpha=0.2)(conv6)\n",
    "        conv6 = BatchNormalization(momentum=0.8)(conv6)\n",
    "\n",
    "        merge2 = concatenate([conv2, conv6], axis = 3)\n",
    "\n",
    "        conv7 = Conv2D(64, 3, padding = 'same', kernel_initializer = 'he_normal')(merge2)\n",
    "        conv7 = LeakyReLU(alpha=0.2)(conv7)\n",
    "        conv7 = BatchNormalization(momentum=0.8)(conv7)\n",
    "\n",
    "        conv8 = Conv2D(64, 3, padding = 'same', kernel_initializer = 'he_normal')(conv7)\n",
    "        conv8 = LeakyReLU(alpha=0.2)(conv8)\n",
    "        conv8 = BatchNormalization(momentum=0.8)(conv8)\n",
>>>>>>> autoencoder
    "\n",
    "\n",
    "        conv8 = Conv2D(64, 3, padding = 'same', kernel_initializer = 'he_normal')(concat3)\n",
    "        conv8 = PReLU()(conv8)\n",
    "        conv8 = BatchNormalization(momentum=0.8)(conv8)\n",
    "\n",
<<<<<<< HEAD
    "        conv9 = Conv2D(64, 3, padding = 'same', kernel_initializer = 'he_normal')(conv8)\n",
    "        conv9 = PReLU()(conv9)\n",
=======
    "        conv9 = Conv2D(32, 3, padding = 'same', kernel_initializer = 'he_normal')(merge3)\n",
    "        conv9 = LeakyReLU(alpha=0.2)(conv9)\n",
>>>>>>> autoencoder
    "        conv9 = BatchNormalization(momentum=0.8)(conv9)\n",
    "        \n",
    "\n",
<<<<<<< HEAD
    "        conv10 = Conv2D(64, 3, padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "        conv10 = PReLU()(conv10)\n",
    "        conv10 = BatchNormalization(momentum=0.8)(conv10)\n",
    "\n",
    "        concat4 = add([conv10, conv1])\n",
    "        \n",
    "        conv11 = Conv2D(128, 3, padding = 'same', kernel_initializer = 'he_normal')(concat4)\n",
    "        conv11 = PReLU()(conv11)\n",
    "        conv11 = BatchNormalization(momentum=0.8)(conv11)\n",
    "        \n",
    "        conv12 = Conv2D(128, 3, padding = 'same', kernel_initializer = 'he_normal')(conv11)\n",
    "        conv12 = PReLU()(conv12)\n",
    "        conv12 = BatchNormalization(momentum=0.8)(conv12)\n",
    "\n",
    "\n",
    "        out = Conv2D(3, 3, padding = 'same', kernel_initializer = 'he_normal')(conv12)\n",
    "        out = PReLU()(out)\n",
    "        out = BatchNormalization(momentum=0.8)(out)\n",
    "\n",
<<<<<<< HEAD
    "        model  = Model([inputs, label], conv11)\n",
=======
    "        valid = self.discriminator(img)\n",
    "        \n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains generator to fool discriminator\n",
    "        self.combined = Model(noise, valid)\n",
    "        self.combined.compile(loss=['mse'],\n",
    "            optimizer=self.optimizer)\n",
    "\n",
    "    def build_generator(self):\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(64 * 48 * 48, input_dim=300))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Reshape((48, 48, 64)))\n",
    "\n",
    "        model.add(Conv2D(512, kernel_size=3, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "        model.add(Conv2D(512, kernel_size=3, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(256, kernel_size=3, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        \n",
    "        model.add(Conv2D(256, kernel_size=3, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        \n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(256, kernel_size=3, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "        model.add(Conv2D(128, kernel_size=3, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(64, kernel_size=3, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "        model.add(Conv2D(64, kernel_size=3, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "        model.add(Conv2D(32, kernel_size=3, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Activation(\"relu\"))\n",
    "    \n",
    "        model.add(Conv2D(32, kernel_size=3, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        \n",
    "        model.add(Conv2D(3, kernel_size=3, padding=\"same\"))\n",
    "        model.add(Activation(\"tanh\"))\n",
    "\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        img   = model(noise)\n",
>>>>>>> ganvanilla
=======
    "\n",
    "        model = Model(input = inputs, output = out)\n",
>>>>>>> superresolution
=======
    "        conv10 = Conv2D(32, 3, padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "        conv10 = LeakyReLU(alpha=0.2)(conv10)\n",
    "        conv10 = BatchNormalization(momentum=0.8)(conv10)\n",
    "\n",
    "\n",
    "        up4    = UpSampling2D(size = (2,2))(conv10)\n",
    "\n",
    "        conv11 = Conv2D(16, 3, padding = 'valid', kernel_initializer = 'he_normal')(up4)\n",
    "        conv11 = LeakyReLU(alpha=0.2)(conv11)\n",
    "        conv11 = BatchNormalization(momentum=0.8)(conv11)\n",
    "\n",
    "        merge4 = concatenate([conv1_a, conv11], axis = 3)\n",
    "        merge4 = ZeroPadding2D(padding=(1, 1))(merge4)\n",
    "\n",
    "        conv12 = Conv2D(16, 3, padding = 'same', kernel_initializer = 'he_normal')(merge4)\n",
    "        conv12 = LeakyReLU(alpha=0.2)(conv12)\n",
    "        conv12 = BatchNormalization(momentum=0.8)(conv12)\n",
    "\n",
    "        conv13 = Conv2D(16, 3, padding = 'same', kernel_initializer = 'he_normal')(conv12)\n",
    "        conv13 = LeakyReLU(alpha=0.2)(conv13)\n",
    "        conv13 = BatchNormalization(momentum=0.8)(conv13)\n",
    "\n",
    "\n",
    "        conv14 = Conv2D(3, 1, padding = 'same', kernel_initializer = 'he_normal', activation='tanh')(conv13)\n",
    "        \n",
    "        model  = Model(input = inputs, output = conv14)\n",
>>>>>>> autoencoder
    "        return model\n",
    "    \n",
    "    def build_autoencoder(self):\n",
    "        self.generator.compile(loss=['mse'], optimizer=self.optimizer)\n",
    "    \n",
    "    def train_generator_autoencoder(self, epochs, batch_size=128, sample_interval=10):\n",
    "        for epoch in range(epochs):\n",
    "            X_train, y_train = next(myGenerator(batch_size))\n",
    "            g_loss = self.generator.train_on_batch(X_train, y_train)\n",
    "            print (\"Epoch \", epoch, \" G loss \", g_loss)\n",
    "            if epoch % sample_interval == 0:\n",
    "                self.sample_images(epoch)\n",
    "                self.generator.save_weights(savepath+'weights/generator_weights_'+str(epoch)+'.h5')\n",
    "            \n",
    "    def build_discriminator(self):\n",
<<<<<<< HEAD
<<<<<<< HEAD
=======
    "        img   = Input(shape=(384, 384, 3))\n",
>>>>>>> ganvanilla
=======
    "        img   = Input(shape=(384, 384, 3))\n",
    "        \n",
>>>>>>> autoencoder
    "        model = Sequential()\n",
    "        model.add(Conv2D(64, (3, 3), input_shape=(384, 384, 3)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(64,  (3, 3),  strides=(2, 2)))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Conv2D(64, (3, 3)))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Conv2D(64, (6, 6),  strides=(2, 2)))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Conv2D(64, (3, 3),  strides=(2, 2)))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Conv2D(64, (3, 3),  strides=(2, 2)))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Flatten())\n",
<<<<<<< HEAD
    "\n",
    "        model.add(Dense(200))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "        img   = Input(shape=(192, 192, 3))\n",
    "        label = Input(shape=(1,), dtype='int32')\n",
    "        embedding_layer = Embedding(self.num_classes, self.latent_dim)\n",
    "\n",
    "        label_embedding  = Flatten()(self.embedding_layer(label))\n",
    "\n",
    "        dense200    = model(img)\n",
    "        conditioned = multiply([dense200, label_embedding])\n",
    "\n",
    "        temp     = Dense(32)(conditioned)\n",
    "        tempa    = LeakyReLU(alpha=0.2)(temp)\n",
    "        validity = Dense(1, activation='sigmoid')(tempa)\n",
    "\n",
    "        model3 = Model([img, label], validity)\n",
    "        \n",
=======
    "        model.add(Dense(100))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(32))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(1))\n",
    "        \n",
    "        output    = model(img)\n",
    "        model3    = Model(img, output)\n",
>>>>>>> ganvanilla
    "        return model3\n",
    "\n",
    "    def train(self, epochs, batch_size=128, sample_interval=50):\n",
    "        random.seed(10)\n",
    "        \n",
    "        # Load the dataset\n",
    "        for epoch in range(epochs):\n",
    "            X_train, Y_train, cls = next(myGenerator_new(batch_size))\n",
    "            \n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "            \n",
    "            # Adversarial ground truths\n",
    "            valid = np.ones((batch_size, 1))\n",
    "            fake  = np.zeros((batch_size, 1))\n",
    "            \n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "            \n",
    "            # Generate a half batch of new images\n",
<<<<<<< HEAD
    "            gen_imgs = self.generator.predict([X_train, cls])\n",
=======
    "            gen_imgs = self.generator.predict(noise)\n",
>>>>>>> ganvanilla
    "\n",
    "            # Train the discriminator\n",
    "            d_loss_real = self.discriminator.train_on_batch([Y_train, cls], valid)\n",
    "            d_loss_fake = self.discriminator.train_on_batch([gen_imgs, cls], fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "            d_loss_real= np.array(d_loss_real)\n",
    "            d_loss_fake= np.array(d_loss_fake)\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            # Train the generator\n",
<<<<<<< HEAD
    "            g_loss = self.combined.train_on_batch([X_train, cls], [Y_train, valid])\n",
    "            \n",
    "            #print(d_loss.dtype)\n",
    "            #print(d_loss_real.dtype)\n",
    "            #print(d_loss_fake.dtype)\n",
    "            #print(g_loss.dtype)\n",
    "\n",
    "            # Plot the progress\n",
    "            #print (\"%d [D loss: %f, mean_acc: %.2f%% real_acc: %.2f%% fake_acc: %.2f%%] [G loss: %f, mse: %f]\" % (epoch, d_loss[0], 100*d_loss[1], 100*d_loss_real[1], 100*d_loss_fake[1], g_loss[0], g_loss[1]))\n",
=======
    "            g_loss = self.combined.train_on_batch(noise, valid)\n",
    "\n",
    "            # Plot the progress\n",
    "            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
>>>>>>> ganvanilla
    "\n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % sample_interval == 0:\n",
    "                self.sample_images_new(epoch)\n",
    "                self.generator.save_weights(savepath+'weights/generator_weights_'+str(epoch)+'.h5')\n",
    "                self.discriminator.save_weights(savepath+'weights/discriminator_weights_'+str(epoch)+'.h5')\n",
    "    \n",
    "    \n",
    "    def sample_images(self, epoch):\n",
    "        r, c             = 1, 10\n",
    "        noise            = np.random.normal(0, 1, (5, self.latent_dim))\n",
    "        gen_imgs         = self.generator.predict(noise)\n",
    "        \n",
    "        # Rescale images 0 - 1\n",
    "        temp     = (0.5 * gen_imgs + 0.5)*255\n",
    "        gen_imgs = temp.astype(int)\n",
    "        y_train = (0.5 * y_train + 0.5)*255\n",
    "        y_train = y_train.astype(int)\n",
    "        X_train = X_train*255\n",
    "        X_train = X_train.astype(int)\n",
    "        \n",
<<<<<<< HEAD
    "        combined = np.array([gen_imgs[0], gen_imgs[1], gen_imgs[2], gen_imgs[3], gen_imgs[4], gen_imgs[5], gen_imgs[6], gen_imgs[7], gen_imgs[8], gen_imgs[9]])\n",
    "        combined = np.hstack(combined.reshape(10, 384,384, 3))\n",
    "        imsave(savepath+\"images/\"+str(epoch)+\".png\", combined)\n",
    "        \n",
    "        combined = np.array([y_train[0], y_train[1], y_train[2], y_train[3], y_train[4], y_train[5], y_train[6], y_train[7], y_train[8], y_train[9]])\n",
    "        combined = np.hstack(combined.reshape(10, 192,192, 3))\n",
    "        imsave(savepath+\"images/\"+str(epoch)+\"_real.png\", combined)\n",
    "        \n",
    "        combined = np.array([X_train[0], X_train[1], X_train[2], X_train[3], X_train[4], X_train[5], X_train[6], X_train[7], X_train[8], X_train[9]])\n",
<<<<<<< HEAD
    "        combined = np.hstack(combined.reshape(10, 192,192, 3))\n",
<<<<<<< HEAD
    "        imsave(savepath+\"images/\"+str(epoch)+\"_real.png\", combined)\n",
    "    \n",
    "    \n",
    "    def sample_images_new(self, epoch):\n",
    "        r, c                  = 1, 5\n",
    "        X_train, Y_train, cls = next(myGenerator_new(5))\n",
    "        gen_imgs              = self.generator.predict([X_train, cls])\n",
    "        \n",
    "        # Rescale images 0 - 1\n",
    "        temp     = (0.5 * gen_imgs + 0.5)*255\n",
    "        gen_imgs = temp.astype(int)\n",
    "        \n",
    "        combined = np.array([gen_imgs[0], gen_imgs[1], gen_imgs[2], gen_imgs[3], gen_imgs[4]])\n",
    "        combined = np.hstack(combined.reshape(5, 192,192, 3))\n",
    "        imsave(savepath+\"images/\"+str(epoch)+\".png\", combined)\n",
    "        \n",
    "        combined = np.array([Y_train[0], Y_train[1], Y_train[2], Y_train[3], Y_train[4]])\n",
    "        combined = np.hstack(combined.reshape(5, 192,192, 3))\n",
=======
    "        combined = np.hstack(combined.reshape(10, 384,384, 3))\n",
>>>>>>> autoencoder
    "        imsave(savepath+\"images/\"+str(epoch)+\"_real.png\", combined)"
=======
    "        imsave(savepath+\"images/\"+str(epoch)+\"_lowres.png\", combined)"
>>>>>>> superresolution
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#cgan = CGAN()\n",
    "#cgan.build_autoencoder()\n",
    "#cgan.train_generator_autoencoder(1000000, 8, 100)"
=======
    "        combined = np.array([gen_imgs[0], gen_imgs[1], gen_imgs[2], gen_imgs[3], gen_imgs[4]])\n",
    "        combined = np.hstack(combined.reshape(5, 384,384, 3))\n",
    "        imsave(savepath+\"images/\"+str(epoch)+\".png\", combined)\n",
    "        \n",
    "#         combined = np.array([Y_train[0], Y_train[1], Y_train[2], Y_train[3], Y_train[4]])\n",
    "#         combined = np.hstack(combined.reshape(5, 384,384, 3))\n",
    "#         imsave(savepath+\"images/\"+str(epoch)+\"_real.png\", combined)"
>>>>>>> ganvanilla
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 31,
=======
>>>>>>> superresolution
=======
   "execution_count": null,
>>>>>>> autoencoder
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# cgan = CGAN()\n",
    "# cgan.build_autoencoder()\n",
    "# cgan.train_generator_autoencoder(1000000, 10, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cgan.train(10000, batch_size=10, sample_interval=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
<<<<<<< HEAD
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:66: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, 3, strides=(2, 2), kernel_initializer=\"he_normal\")`\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:81: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, 3, strides=(2, 2), kernel_initializer=\"he_normal\")`\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:95: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, 3, strides=(2, 2), kernel_initializer=\"he_normal\")`\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:110: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(1, 3, strides=(2, 2), kernel_initializer=\"he_normal\")`\n"
=======
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:116: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"ba..., inputs=Tensor(\"in...)`\n"
>>>>>>> superresolution
=======
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:52: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, 3, kernel_initializer=\"he_normal\", strides=(2, 2))`\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:67: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, 3, kernel_initializer=\"he_normal\", strides=(2, 2))`\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:81: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, 3, kernel_initializer=\"he_normal\", strides=(2, 2))`\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:95: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, 3, kernel_initializer=\"he_normal\", strides=(2, 2))`\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:110: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(1, 3, kernel_initializer=\"he_normal\", strides=(2, 2))`\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:196: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"co..., inputs=Tensor(\"in...)`\n"
>>>>>>> autoencoder
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
<<<<<<< HEAD
<<<<<<< HEAD
      "input_70 (InputLayer)           (None, 192, 192, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_303 (Conv2D)             (None, 192, 192, 64) 1792        input_70[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_316 (LeakyReLU)     (None, 192, 192, 64) 0           conv2d_303[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_280 (BatchN (None, 192, 192, 64) 256         leaky_re_lu_316[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_304 (Conv2D)             (None, 190, 190, 64) 36928       batch_normalization_280[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_317 (LeakyReLU)     (None, 190, 190, 64) 0           conv2d_304[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_281 (BatchN (None, 190, 190, 64) 256         leaky_re_lu_317[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_305 (Conv2D)             (None, 94, 94, 64)   36928       batch_normalization_281[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_318 (LeakyReLU)     (None, 94, 94, 64)   0           conv2d_305[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_282 (BatchN (None, 94, 94, 64)   256         leaky_re_lu_318[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_306 (Conv2D)             (None, 94, 94, 128)  73856       batch_normalization_282[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_319 (LeakyReLU)     (None, 94, 94, 128)  0           conv2d_306[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_283 (BatchN (None, 94, 94, 128)  512         leaky_re_lu_319[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_307 (Conv2D)             (None, 94, 94, 128)  147584      batch_normalization_283[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_320 (LeakyReLU)     (None, 94, 94, 128)  0           conv2d_307[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_284 (BatchN (None, 94, 94, 128)  512         leaky_re_lu_320[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_308 (Conv2D)             (None, 46, 46, 128)  147584      batch_normalization_284[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_321 (LeakyReLU)     (None, 46, 46, 128)  0           conv2d_308[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_285 (BatchN (None, 46, 46, 128)  512         leaky_re_lu_321[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_309 (Conv2D)             (None, 46, 46, 256)  295168      batch_normalization_285[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_322 (LeakyReLU)     (None, 46, 46, 256)  0           conv2d_309[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_286 (BatchN (None, 46, 46, 256)  1024        leaky_re_lu_322[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_310 (Conv2D)             (None, 46, 46, 256)  590080      batch_normalization_286[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_323 (LeakyReLU)     (None, 46, 46, 256)  0           conv2d_310[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_287 (BatchN (None, 46, 46, 256)  1024        leaky_re_lu_323[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_311 (Conv2D)             (None, 22, 22, 256)  590080      batch_normalization_287[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_324 (LeakyReLU)     (None, 22, 22, 256)  0           conv2d_311[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_288 (BatchN (None, 22, 22, 256)  1024        leaky_re_lu_324[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_312 (Conv2D)             (None, 22, 22, 256)  590080      batch_normalization_288[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_325 (LeakyReLU)     (None, 22, 22, 256)  0           conv2d_312[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_289 (BatchN (None, 22, 22, 256)  1024        leaky_re_lu_325[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_313 (Conv2D)             (None, 20, 20, 1)    2305        batch_normalization_289[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_326 (LeakyReLU)     (None, 20, 20, 1)    0           conv2d_313[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "input_69 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_290 (BatchN (None, 20, 20, 1)    4           leaky_re_lu_326[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_23 (Embedding)        (None, 1, 200)       1800        input_69[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_314 (Conv2D)             (None, 9, 9, 1)      10          batch_normalization_290[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_36 (Flatten)            (None, 200)          0           embedding_23[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_327 (LeakyReLU)     (None, 9, 9, 1)      0           conv2d_314[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_48 (Dense)                (None, 81)           16281       flatten_36[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_291 (BatchN (None, 9, 9, 1)      4           leaky_re_lu_327[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "reshape_12 (Reshape)            (None, 9, 9, 1)      0           dense_48[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "multiply_24 (Multiply)          (None, 9, 9, 1)      0           batch_normalization_291[0][0]    \n",
      "                                                                 reshape_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_45 (UpSampling2D) (None, 18, 18, 1)    0           multiply_24[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_56 (ZeroPadding2 (None, 20, 20, 1)    0           up_sampling2d_45[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_315 (Conv2D)             (None, 20, 20, 1)    10          zero_padding2d_56[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_328 (LeakyReLU)     (None, 20, 20, 1)    0           conv2d_315[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_292 (BatchN (None, 20, 20, 1)    4           leaky_re_lu_328[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_46 (UpSampling2D) (None, 40, 40, 1)    0           batch_normalization_292[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_57 (ZeroPadding2 (None, 46, 46, 1)    0           up_sampling2d_46[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_316 (Conv2D)             (None, 46, 46, 256)  2560        zero_padding2d_57[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_329 (LeakyReLU)     (None, 46, 46, 256)  0           conv2d_316[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_293 (BatchN (None, 46, 46, 256)  1024        leaky_re_lu_329[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_34 (Concatenate)    (None, 46, 46, 512)  0           batch_normalization_293[0][0]    \n",
      "                                                                 batch_normalization_287[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_317 (Conv2D)             (None, 46, 46, 256)  1179904     concatenate_34[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_330 (LeakyReLU)     (None, 46, 46, 256)  0           conv2d_317[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_294 (BatchN (None, 46, 46, 256)  1024        leaky_re_lu_330[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_318 (Conv2D)             (None, 46, 46, 256)  590080      batch_normalization_294[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_331 (LeakyReLU)     (None, 46, 46, 256)  0           conv2d_318[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_295 (BatchN (None, 46, 46, 256)  1024        leaky_re_lu_331[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_47 (UpSampling2D) (None, 92, 92, 256)  0           batch_normalization_295[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_58 (ZeroPadding2 (None, 94, 94, 256)  0           up_sampling2d_47[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_319 (Conv2D)             (None, 94, 94, 128)  295040      zero_padding2d_58[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_332 (LeakyReLU)     (None, 94, 94, 128)  0           conv2d_319[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_296 (BatchN (None, 94, 94, 128)  512         leaky_re_lu_332[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_35 (Concatenate)    (None, 94, 94, 256)  0           batch_normalization_284[0][0]    \n",
      "                                                                 batch_normalization_296[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_320 (Conv2D)             (None, 94, 94, 128)  295040      concatenate_35[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_333 (LeakyReLU)     (None, 94, 94, 128)  0           conv2d_320[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_297 (BatchN (None, 94, 94, 128)  512         leaky_re_lu_333[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_321 (Conv2D)             (None, 94, 94, 128)  147584      batch_normalization_297[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_334 (LeakyReLU)     (None, 94, 94, 128)  0           conv2d_321[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_298 (BatchN (None, 94, 94, 128)  512         leaky_re_lu_334[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_48 (UpSampling2D) (None, 188, 188, 128 0           batch_normalization_298[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_59 (ZeroPadding2 (None, 190, 190, 128 0           up_sampling2d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_36 (Concatenate)    (None, 190, 190, 192 0           batch_normalization_281[0][0]    \n",
      "                                                                 zero_padding2d_59[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_60 (ZeroPadding2 (None, 192, 192, 192 0           concatenate_36[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_322 (Conv2D)             (None, 192, 192, 64) 110656      zero_padding2d_60[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_335 (LeakyReLU)     (None, 192, 192, 64) 0           conv2d_322[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_299 (BatchN (None, 192, 192, 64) 256         leaky_re_lu_335[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_323 (Conv2D)             (None, 192, 192, 64) 36928       batch_normalization_299[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_336 (LeakyReLU)     (None, 192, 192, 64) 0           conv2d_323[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_300 (BatchN (None, 192, 192, 64) 256         leaky_re_lu_336[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_324 (Conv2D)             (None, 192, 192, 3)  195         batch_normalization_300[0][0]    \n",
      "==================================================================================================\n",
      "Total params: 5,200,005\n",
      "Trainable params: 5,194,239\n",
      "Non-trainable params: 5,766\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:312: DeprecationWarning: `imsave` is deprecated!\n",
      "`imsave` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imwrite`` instead.\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:316: DeprecationWarning: `imsave` is deprecated!\n",
      "`imsave` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imwrite`` instead.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-906b9b8bd43f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcgan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCGAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-30-528a10f21c14>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epochs, batch_size, sample_interval)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m             \u001b[0;31m# Train the generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m             \u001b[0mg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcombined\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m             \u001b[0;31m#print(d_loss.dtype)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cgan = CGAN()\n",
    "cgan.train(10000, batch_size=10, sample_interval=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
=======
      "input_1 (InputLayer)            (None, 192, 192, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 192, 192, 64) 1792        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_1 (PReLU)               (None, 192, 192, 64) 2359296     conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 192, 192, 64) 256         p_re_lu_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 192, 192, 64) 36928       batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_2 (PReLU)               (None, 192, 192, 64) 2359296     conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 192, 192, 64) 256         p_re_lu_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 192, 192, 64) 36928       batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_3 (PReLU)               (None, 192, 192, 64) 2359296     conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 192, 192, 64) 256         p_re_lu_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 192, 192, 64) 0           batch_normalization_1[0][0]      \n",
      "                                                                 batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 192, 192, 64) 36928       add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_4 (PReLU)               (None, 192, 192, 64) 2359296     conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 192, 192, 64) 256         p_re_lu_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 192, 192, 64) 36928       batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_5 (PReLU)               (None, 192, 192, 64) 2359296     conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 192, 192, 64) 256         p_re_lu_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 192, 192, 64) 0           batch_normalization_5[0][0]      \n",
      "                                                                 add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 192, 192, 64) 36928       add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_6 (PReLU)               (None, 192, 192, 64) 2359296     conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 192, 192, 64) 256         p_re_lu_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 192, 192, 64) 36928       batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_7 (PReLU)               (None, 192, 192, 64) 2359296     conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 192, 192, 64) 256         p_re_lu_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 192, 192, 64) 0           batch_normalization_7[0][0]      \n",
      "                                                                 add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 192, 192, 64) 36928       add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_8 (PReLU)               (None, 192, 192, 64) 2359296     conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 192, 192, 64) 256         p_re_lu_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 192, 192, 64) 36928       batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_9 (PReLU)               (None, 192, 192, 64) 2359296     conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 192, 192, 64) 256         p_re_lu_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 192, 192, 64) 36928       batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_10 (PReLU)              (None, 192, 192, 64) 2359296     conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 192, 192, 64) 256         p_re_lu_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 192, 192, 64) 0           batch_normalization_10[0][0]     \n",
      "                                                                 batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 192, 192, 128 73856       add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_11 (PReLU)              (None, 192, 192, 128 4718592     conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 192, 192, 128 512         p_re_lu_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 192, 192, 128 147584      batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_12 (PReLU)              (None, 192, 192, 128 4718592     conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 192, 192, 128 512         p_re_lu_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 192, 192, 3)  3459        batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_13 (PReLU)              (None, 192, 192, 3)  110592      conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 192, 192, 3)  12          p_re_lu_13[0][0]                 \n",
>>>>>>> superresolution
      "==================================================================================================\n",
      "Total params: 33,703,375\n",
      "Trainable params: 33,701,577\n",
      "Non-trainable params: 1,798\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:12: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "  if sys.path[0] == '':\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:13: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0  G loss  1.4612372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:222: DeprecationWarning: `imsave` is deprecated!\n",
      "`imsave` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imwrite`` instead.\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:226: DeprecationWarning: `imsave` is deprecated!\n",
      "`imsave` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imwrite`` instead.\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:230: DeprecationWarning: `imsave` is deprecated!\n",
      "`imsave` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imwrite`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1  G loss  0.9946966\n",
      "Epoch  2  G loss  0.7106959\n",
      "Epoch  3  G loss  0.63329256\n",
      "Epoch  4  G loss  0.82410705\n",
      "Epoch  5  G loss  0.58659136\n",
      "Epoch  6  G loss  0.8211112\n",
      "Epoch  7  G loss  0.43409774\n",
      "Epoch  8  G loss  0.3455085\n",
      "Epoch  9  G loss  0.5399548\n",
      "Epoch  10  G loss  0.41149268\n",
      "Epoch  11  G loss  0.43250355\n",
      "Epoch  12  G loss  0.29557806\n",
      "Epoch  13  G loss  0.27332786\n",
      "Epoch  14  G loss  0.30138847\n",
      "Epoch  15  G loss  0.34687984\n",
      "Epoch  16  G loss  0.6772423\n",
      "Epoch  17  G loss  0.25139463\n",
      "Epoch  18  G loss  0.34162366\n",
      "Epoch  19  G loss  0.27158308\n",
      "Epoch  20  G loss  0.34738556\n",
      "Epoch  21  G loss  0.36522055\n",
      "Epoch  22  G loss  0.38803515\n",
      "Epoch  23  G loss  0.56321615\n",
      "Epoch  24  G loss  0.3791135\n",
      "Epoch  25  G loss  0.30589032\n",
      "Epoch  26  G loss  0.41381755\n",
      "Epoch  27  G loss  0.354734\n",
      "Epoch  28  G loss  0.3578729\n",
      "Epoch  29  G loss  0.18066604\n",
      "Epoch  30  G loss  0.33352143\n",
      "Epoch  31  G loss  0.36567435\n",
      "Epoch  32  G loss  0.25437307\n",
      "Epoch  33  G loss  0.2535713\n",
      "Epoch  34  G loss  0.22628984\n",
      "Epoch  35  G loss  0.41926923\n",
      "Epoch  36  G loss  0.17645547\n",
      "Epoch  37  G loss  0.49723274\n",
      "Epoch  38  G loss  0.29740578\n",
      "Epoch  39  G loss  0.22056855\n",
      "Epoch  40  G loss  0.44245225\n",
      "Epoch  41  G loss  0.3445055\n",
      "Epoch  42  G loss  0.22147164\n",
      "Epoch  43  G loss  0.40598297\n",
      "Epoch  44  G loss  0.18283258\n",
      "Epoch  45  G loss  0.1548019\n",
      "Epoch  46  G loss  0.25227582\n",
      "Epoch  47  G loss  0.66606957\n",
      "Epoch  48  G loss  0.16898641\n",
      "Epoch  49  G loss  0.30583313\n",
      "Epoch  50  G loss  0.40102237\n",
      "Epoch  51  G loss  0.3470959\n",
      "Epoch  52  G loss  0.19949241\n",
      "Epoch  53  G loss  0.19760129\n",
      "Epoch  54  G loss  0.16302942\n",
      "Epoch  55  G loss  0.38871133\n",
      "Epoch  56  G loss  0.23516282\n",
      "Epoch  57  G loss  0.20509571\n",
      "Epoch  58  G loss  0.3153162\n",
      "Epoch  59  G loss  0.23110637\n",
      "Epoch  60  G loss  0.26029336\n",
      "Epoch  61  G loss  0.1679998\n",
      "Epoch  62  G loss  0.20534214\n",
      "Epoch  63  G loss  0.1791788\n",
      "Epoch  64  G loss  0.25735024\n",
      "Epoch  65  G loss  0.1771158\n",
      "Epoch  66  G loss  0.19728163\n",
      "Epoch  67  G loss  0.26042566\n",
      "Epoch  68  G loss  0.522552\n",
      "Epoch  69  G loss  0.59137714\n",
      "Epoch  70  G loss  0.22424757\n",
      "Epoch  71  G loss  0.23067136\n",
      "Epoch  72  G loss  0.3524701\n",
      "Epoch  73  G loss  0.20972441\n",
      "Epoch  74  G loss  0.22387818\n",
      "Epoch  75  G loss  0.16956347\n",
      "Epoch  76  G loss  0.17894915\n",
      "Epoch  77  G loss  0.13964722\n",
      "Epoch  78  G loss  0.19255306\n",
      "Epoch  79  G loss  0.20912445\n",
      "Epoch  80  G loss  0.51014936\n",
      "Epoch  81  G loss  0.1405347\n",
      "Epoch  82  G loss  0.1517435\n",
      "Epoch  83  G loss  0.19897869\n",
      "Epoch  84  G loss  0.26091045\n",
      "Epoch  85  G loss  0.24305037\n",
      "Epoch  86  G loss  0.21003976\n",
      "Epoch  87  G loss  0.16661651\n",
      "Epoch  88  G loss  0.29735398\n",
      "Epoch  89  G loss  0.19971296\n",
      "Epoch  90  G loss  0.37933725\n",
      "Epoch  91  G loss  0.3222711\n",
      "Epoch  92  G loss  0.8663571\n",
      "Epoch  93  G loss  0.2861226\n",
      "Epoch  94  G loss  0.14658743\n",
      "Epoch  95  G loss  0.16663608\n",
      "Epoch  96  G loss  0.31043017\n",
      "Epoch  97  G loss  0.21044947\n",
      "Epoch  98  G loss  0.1542624\n",
      "Epoch  99  G loss  0.19084829\n",
      "Epoch  100  G loss  0.20825294\n",
      "Epoch  101  G loss  0.2141312\n",
      "Epoch  102  G loss  0.19709614\n",
      "Epoch  103  G loss  0.26739997\n",
      "Epoch  104  G loss  0.18308458\n",
      "Epoch  105  G loss  0.3666464\n",
      "Epoch  106  G loss  0.335302\n",
      "Epoch  107  G loss  0.21000603\n",
      "Epoch  108  G loss  0.2492862\n",
      "Epoch  109  G loss  0.13424593\n",
      "Epoch  110  G loss  0.23356254\n",
      "Epoch  111  G loss  0.11147961\n",
      "Epoch  112  G loss  0.25877553\n",
      "Epoch  113  G loss  0.2736252\n",
      "Epoch  114  G loss  0.29536647\n",
      "Epoch  115  G loss  0.26029292\n",
      "Epoch  116  G loss  0.21309243\n",
      "Epoch  117  G loss  0.1647659\n",
      "Epoch  118  G loss  0.1726503\n",
      "Epoch  119  G loss  0.19818373\n",
      "Epoch  120  G loss  0.17797348\n",
      "Epoch  121  G loss  0.29307643\n",
      "Epoch  122  G loss  0.14053886\n",
      "Epoch  123  G loss  0.10135594\n",
      "Epoch  124  G loss  0.27839684\n",
      "Epoch  125  G loss  0.9617127\n",
      "Epoch  126  G loss  0.1675188\n",
      "Epoch  127  G loss  0.15530238\n",
      "Epoch  128  G loss  0.1922693\n",
      "Epoch  129  G loss  0.15293851\n",
      "Epoch  130  G loss  0.23871335\n",
      "Epoch  131  G loss  0.19469115\n",
      "Epoch  132  G loss  0.4745354\n",
      "Epoch  133  G loss  0.13935167\n",
      "Epoch  134  G loss  0.14331143\n",
      "Epoch  135  G loss  0.17844069\n",
      "Epoch  136  G loss  0.411202\n",
      "Epoch  137  G loss  0.16063452\n",
      "Epoch  138  G loss  0.17608868\n",
      "Epoch  139  G loss  0.18724912\n",
      "Epoch  140  G loss  0.20882519\n",
      "Epoch  141  G loss  0.16627862\n",
      "Epoch  142  G loss  0.34566996\n",
      "Epoch  143  G loss  0.13981144\n",
      "Epoch  144  G loss  0.16514546\n",
      "Epoch  145  G loss  0.16143161\n",
      "Epoch  146  G loss  0.2888302\n",
      "Epoch  147  G loss  0.13307956\n",
      "Epoch  148  G loss  0.33608818\n",
      "Epoch  149  G loss  0.41611338\n",
      "Epoch  150  G loss  0.25348496\n",
      "Epoch  151  G loss  0.19868876\n",
      "Epoch  152  G loss  0.14264676\n",
      "Epoch  153  G loss  0.30002818\n",
      "Epoch  154  G loss  0.19084448\n",
      "Epoch  155  G loss  0.1514242\n",
      "Epoch  156  G loss  0.15441585\n",
      "Epoch  157  G loss  0.23962677\n",
      "Epoch  158  G loss  0.1700693\n",
      "Epoch  159  G loss  0.31646508\n",
      "Epoch  160  G loss  0.14737605\n",
      "Epoch  161  G loss  0.24131763\n",
      "Epoch  162  G loss  0.14660278\n",
      "Epoch  163  G loss  0.16001798\n",
      "Epoch  164  G loss  0.27036643\n",
      "Epoch  165  G loss  0.18699086\n",
      "Epoch  166  G loss  0.15291083\n",
      "Epoch  167  G loss  0.3437056\n",
      "Epoch  168  G loss  0.20038794\n",
      "Epoch  169  G loss  0.15563834\n",
      "Epoch  170  G loss  0.2629449\n",
      "Epoch  171  G loss  0.24133518\n",
      "Epoch  172  G loss  0.2251285\n",
      "Epoch  173  G loss  0.13888785\n",
      "Epoch  174  G loss  0.17193212\n",
      "Epoch  175  G loss  0.16927546\n",
      "Epoch  176  G loss  0.20193857\n",
      "Epoch  177  G loss  0.29762006\n",
      "Epoch  178  G loss  0.33840054\n",
      "Epoch  179  G loss  0.27544388\n",
      "Epoch  180  G loss  0.16820818\n",
      "Epoch  181  G loss  0.18504381\n",
      "Epoch  182  G loss  0.11857046\n",
      "Epoch  183  G loss  0.34325916\n",
      "Epoch  184  G loss  0.17283005\n",
      "Epoch  185  G loss  0.13582836\n",
      "Epoch  186  G loss  0.189258\n",
      "Epoch  187  G loss  0.20791867\n",
      "Epoch  188  G loss  0.107197076\n",
      "Epoch  189  G loss  0.12690789\n",
      "Epoch  190  G loss  0.2504391\n",
      "Epoch  191  G loss  0.15756208\n",
      "Epoch  192  G loss  0.3259714\n",
      "Epoch  193  G loss  0.15417035\n",
      "Epoch  194  G loss  0.14763436\n",
      "Epoch  195  G loss  0.27390707\n",
      "Epoch  196  G loss  0.14915618\n",
      "Epoch  197  G loss  0.12525421\n",
      "Epoch  198  G loss  0.21531004\n",
      "Epoch  199  G loss  0.22438127\n",
      "Epoch  200  G loss  0.15432829\n",
      "Epoch  201  G loss  0.17676634\n",
      "Epoch  202  G loss  0.21824726\n",
      "Epoch  203  G loss  0.108052194\n",
      "Epoch  204  G loss  0.3106454\n",
      "Epoch  205  G loss  0.42152593\n",
      "Epoch  206  G loss  0.34081948\n",
      "Epoch  207  G loss  0.13232036\n",
      "Epoch  208  G loss  0.1512434\n",
      "Epoch  209  G loss  0.19627555\n",
      "Epoch  210  G loss  0.339823\n",
      "Epoch  211  G loss  0.16380224\n",
      "Epoch  212  G loss  0.38685885\n",
      "Epoch  213  G loss  0.40883428\n",
      "Epoch  214  G loss  0.25888222\n",
      "Epoch  215  G loss  0.1510479\n",
      "Epoch  216  G loss  0.25102535\n",
      "Epoch  217  G loss  0.29919404\n",
      "Epoch  218  G loss  0.1344886\n",
      "Epoch  219  G loss  0.18932478\n",
      "Epoch  220  G loss  0.3099107\n",
      "Epoch  221  G loss  0.4151867\n",
      "Epoch  222  G loss  0.19239962\n",
      "Epoch  223  G loss  0.21442513\n",
      "Epoch  224  G loss  0.22590953\n",
      "Epoch  225  G loss  0.12904955\n",
      "Epoch  226  G loss  0.2630206\n",
      "Epoch  227  G loss  0.34865904\n",
      "Epoch  228  G loss  0.12110365\n",
      "Epoch  229  G loss  0.1812019\n",
      "Epoch  230  G loss  0.20272596\n",
      "Epoch  231  G loss  0.20155337\n",
      "Epoch  232  G loss  0.2934696\n",
      "Epoch  233  G loss  0.3510072\n",
      "Epoch  234  G loss  0.1206158\n",
      "Epoch  235  G loss  0.20523024\n",
      "Epoch  236  G loss  0.16806012\n",
      "Epoch  237  G loss  0.19399786\n",
      "Epoch  238  G loss  0.22071593\n",
      "Epoch  239  G loss  0.23527679\n",
      "Epoch  240  G loss  0.15275085\n",
      "Epoch  241  G loss  0.102325566\n",
      "Epoch  242  G loss  0.123067535\n",
      "Epoch  243  G loss  0.2402228\n",
      "Epoch  244  G loss  0.20506755\n",
      "Epoch  245  G loss  0.1819758\n",
      "Epoch  246  G loss  0.20936903\n",
      "Epoch  247  G loss  0.17975056\n",
      "Epoch  248  G loss  0.2720908\n",
      "Epoch  249  G loss  0.20151499\n",
      "Epoch  250  G loss  0.16482833\n",
      "Epoch  251  G loss  0.120770484\n",
      "Epoch  252  G loss  0.19246149\n",
      "Epoch  253  G loss  0.1997582\n",
      "Epoch  254  G loss  0.3262007\n",
      "Epoch  255  G loss  0.21179259\n",
      "Epoch  256  G loss  0.1743711\n",
      "Epoch  257  G loss  0.2374895\n",
      "Epoch  258  G loss  0.12591435\n",
      "Epoch  259  G loss  0.31413883\n",
      "Epoch  260  G loss  0.41238296\n",
      "Epoch  261  G loss  0.24709162\n",
      "Epoch  262  G loss  0.15602261\n",
      "Epoch  263  G loss  0.16130865\n",
      "Epoch  264  G loss  0.113442585\n",
      "Epoch  265  G loss  0.15957387\n",
      "Epoch  266  G loss  0.14154759\n",
      "Epoch  267  G loss  0.1557256\n",
      "Epoch  268  G loss  0.28515798\n",
      "Epoch  269  G loss  0.17255627\n",
      "Epoch  270  G loss  0.104034714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  271  G loss  0.112664804\n",
      "Epoch  272  G loss  0.1045382\n",
      "Epoch  273  G loss  0.42036173\n",
      "Epoch  274  G loss  0.10717116\n",
      "Epoch  275  G loss  0.26551363\n",
      "Epoch  276  G loss  0.10389818\n",
      "Epoch  277  G loss  0.2099151\n",
      "Epoch  278  G loss  0.07818246\n",
      "Epoch  279  G loss  0.17217717\n",
      "Epoch  280  G loss  0.30160365\n",
      "Epoch  281  G loss  0.1470992\n",
      "Epoch  282  G loss  0.11504178\n",
      "Epoch  283  G loss  0.33167958\n",
      "Epoch  284  G loss  0.12424609\n",
      "Epoch  285  G loss  0.12451497\n",
      "Epoch  286  G loss  0.22968465\n",
      "Epoch  287  G loss  0.10292714\n",
      "Epoch  288  G loss  0.17511949\n",
      "Epoch  289  G loss  0.21774483\n",
      "Epoch  290  G loss  0.27770442\n",
      "Epoch  291  G loss  0.109720245\n",
      "Epoch  292  G loss  0.0932361\n",
      "Epoch  293  G loss  0.23058483\n",
      "Epoch  294  G loss  0.14909682\n",
      "Epoch  295  G loss  0.16049138\n",
      "Epoch  296  G loss  0.2541685\n",
      "Epoch  297  G loss  0.15916328\n",
      "Epoch  298  G loss  0.20880195\n",
      "Epoch  299  G loss  0.17519134\n",
      "Epoch  300  G loss  0.19158445\n",
      "Epoch  301  G loss  0.117087536\n",
      "Epoch  302  G loss  0.23480168\n",
      "Epoch  303  G loss  0.14139119\n",
      "Epoch  304  G loss  0.5230346\n",
      "Epoch  305  G loss  0.13013959\n",
      "Epoch  306  G loss  0.2813506\n",
      "Epoch  307  G loss  0.21771783\n",
      "Epoch  308  G loss  0.19084111\n",
      "Epoch  309  G loss  0.20307566\n",
      "Epoch  310  G loss  0.11801337\n",
      "Epoch  311  G loss  0.099771746\n",
      "Epoch  312  G loss  0.088633224\n",
      "Epoch  313  G loss  0.23236325\n",
      "Epoch  314  G loss  0.16701986\n",
      "Epoch  315  G loss  0.11211726\n",
      "Epoch  316  G loss  0.25300193\n",
      "Epoch  317  G loss  0.26004484\n",
      "Epoch  318  G loss  0.39565837\n",
      "Epoch  319  G loss  0.13175112\n",
      "Epoch  320  G loss  0.18588781\n",
      "Epoch  321  G loss  0.16586205\n",
      "Epoch  322  G loss  0.16551244\n",
      "Epoch  323  G loss  0.19401881\n",
      "Epoch  324  G loss  0.14384608\n",
      "Epoch  325  G loss  0.08377739\n",
      "Epoch  326  G loss  0.17330909\n",
      "Epoch  327  G loss  0.17350787\n",
      "Epoch  328  G loss  0.06366578\n",
      "Epoch  329  G loss  0.13476638\n",
      "Epoch  330  G loss  0.14808756\n",
      "Epoch  331  G loss  0.4578519\n",
      "Epoch  332  G loss  0.5030099\n",
      "Epoch  333  G loss  0.16891104\n",
      "Epoch  334  G loss  0.20244598\n",
      "Epoch  335  G loss  0.144519\n",
      "Epoch  336  G loss  0.3528592\n",
      "Epoch  337  G loss  0.19565365\n",
      "Epoch  338  G loss  0.14943773\n",
      "Epoch  339  G loss  0.12630959\n",
      "Epoch  340  G loss  0.1391499\n",
      "Epoch  341  G loss  0.12691016\n",
      "Epoch  342  G loss  0.1449123\n",
      "Epoch  343  G loss  0.19944382\n",
      "Epoch  344  G loss  0.21751669\n",
      "Epoch  345  G loss  0.12768704\n",
      "Epoch  346  G loss  0.17218614\n",
      "Epoch  347  G loss  0.1356947\n",
      "Epoch  348  G loss  0.121296175\n",
      "Epoch  349  G loss  0.0971428\n",
      "Epoch  350  G loss  0.13312265\n",
      "Epoch  351  G loss  0.22326893\n",
      "Epoch  352  G loss  0.13218963\n",
      "Epoch  353  G loss  0.1316805\n",
      "Epoch  354  G loss  0.16757083\n",
      "Epoch  355  G loss  0.12124296\n",
      "Epoch  356  G loss  0.14791927\n",
      "Epoch  357  G loss  0.20748326\n",
      "Epoch  358  G loss  0.045735735\n",
      "Epoch  359  G loss  0.11884644\n",
      "Epoch  360  G loss  0.17902511\n",
      "Epoch  361  G loss  0.0812378\n",
      "Epoch  362  G loss  0.3305873\n",
      "Epoch  363  G loss  0.20058343\n",
      "Epoch  364  G loss  0.17278007\n",
      "Epoch  365  G loss  0.21490571\n",
      "Epoch  366  G loss  0.27649942\n",
      "Epoch  367  G loss  0.1657628\n",
      "Epoch  368  G loss  0.44635183\n",
      "Epoch  369  G loss  0.14928919\n",
      "Epoch  370  G loss  0.13841152\n",
      "Epoch  371  G loss  0.22228248\n",
      "Epoch  372  G loss  0.12907678\n",
      "Epoch  373  G loss  0.21180499\n",
      "Epoch  374  G loss  0.15995331\n",
      "Epoch  375  G loss  0.10930169\n",
      "Epoch  376  G loss  0.601686\n",
      "Epoch  377  G loss  0.11035772\n",
      "Epoch  378  G loss  0.4001621\n",
      "Epoch  379  G loss  0.12641422\n",
      "Epoch  380  G loss  0.15663174\n",
      "Epoch  381  G loss  0.2116754\n",
      "Epoch  382  G loss  0.10154347\n",
      "Epoch  383  G loss  0.12429637\n",
      "Epoch  384  G loss  0.1501258\n",
      "Epoch  385  G loss  0.14949693\n",
      "Epoch  386  G loss  0.121505246\n",
      "Epoch  387  G loss  0.118805066\n",
      "Epoch  388  G loss  0.120202795\n",
      "Epoch  389  G loss  0.14136215\n",
      "Epoch  390  G loss  0.21039598\n",
      "Epoch  391  G loss  0.11885177\n",
      "Epoch  392  G loss  0.2298527\n",
      "Epoch  393  G loss  0.1260393\n",
      "Epoch  394  G loss  0.14675531\n",
      "Epoch  395  G loss  0.34361175\n",
      "Epoch  396  G loss  0.099482566\n",
      "Epoch  397  G loss  0.1380588\n",
      "Epoch  398  G loss  0.162657\n",
      "Epoch  399  G loss  0.095693335\n",
      "Epoch  400  G loss  0.11490619\n",
      "Epoch  401  G loss  0.13773055\n",
      "Epoch  402  G loss  0.22718889\n",
      "Epoch  403  G loss  0.15869328\n",
      "Epoch  404  G loss  0.13708362\n",
      "Epoch  405  G loss  0.14016294\n",
      "Epoch  406  G loss  0.17907399\n",
      "Epoch  407  G loss  0.35942864\n",
      "Epoch  408  G loss  0.14918658\n",
      "Epoch  409  G loss  0.15023655\n",
      "Epoch  410  G loss  0.19118395\n",
      "Epoch  411  G loss  0.094029605\n",
      "Epoch  412  G loss  0.09949332\n",
      "Epoch  413  G loss  0.4233085\n",
      "Epoch  414  G loss  0.108986884\n",
      "Epoch  415  G loss  0.15828021\n",
      "Epoch  416  G loss  0.16807324\n",
      "Epoch  417  G loss  0.1798909\n",
      "Epoch  418  G loss  0.072336555\n",
      "Epoch  419  G loss  0.14468011\n",
      "Epoch  420  G loss  0.1639857\n",
      "Epoch  421  G loss  0.22907335\n",
      "Epoch  422  G loss  0.13638757\n",
      "Epoch  423  G loss  0.14898467\n",
      "Epoch  424  G loss  0.13935703\n",
      "Epoch  425  G loss  0.1737315\n",
      "Epoch  426  G loss  0.12597004\n",
      "Epoch  427  G loss  0.18889257\n",
      "Epoch  428  G loss  0.08580172\n",
      "Epoch  429  G loss  0.2922465\n",
      "Epoch  430  G loss  0.23970233\n",
      "Epoch  431  G loss  0.20838721\n",
      "Epoch  432  G loss  0.08708417\n",
      "Epoch  433  G loss  0.15938607\n",
      "Epoch  434  G loss  0.14854069\n",
      "Epoch  435  G loss  0.14505146\n",
      "Epoch  436  G loss  0.15450665\n",
      "Epoch  437  G loss  0.06623201\n",
      "Epoch  438  G loss  0.14532228\n",
      "Epoch  439  G loss  0.1619632\n",
      "Epoch  440  G loss  0.14933354\n",
      "Epoch  441  G loss  0.2626108\n",
      "Epoch  442  G loss  0.104276866\n",
      "Epoch  443  G loss  0.08272984\n",
      "Epoch  444  G loss  0.29702505\n",
      "Epoch  445  G loss  0.15674874\n",
      "Epoch  446  G loss  0.095905274\n",
      "Epoch  447  G loss  0.12492577\n",
      "Epoch  448  G loss  0.15249094\n",
      "Epoch  449  G loss  0.10871504\n",
      "Epoch  450  G loss  0.11947286\n",
      "Epoch  451  G loss  0.3717592\n",
      "Epoch  452  G loss  0.093945995\n",
      "Epoch  453  G loss  0.25904977\n",
      "Epoch  454  G loss  0.07728038\n",
      "Epoch  455  G loss  0.16144341\n",
      "Epoch  456  G loss  0.08095418\n",
      "Epoch  457  G loss  0.16119969\n",
      "Epoch  458  G loss  0.18944538\n",
      "Epoch  459  G loss  0.6366202\n",
      "Epoch  460  G loss  0.059600595\n",
      "Epoch  461  G loss  0.14666614\n",
      "Epoch  462  G loss  0.16701141\n",
      "Epoch  463  G loss  0.20819478\n",
      "Epoch  464  G loss  0.15902478\n",
      "Epoch  465  G loss  0.14054\n",
      "Epoch  466  G loss  0.06700955\n",
      "Epoch  467  G loss  0.10260099\n",
      "Epoch  468  G loss  0.1371556\n",
      "Epoch  469  G loss  0.2608463\n",
      "Epoch  470  G loss  0.06920758\n",
      "Epoch  471  G loss  0.24569805\n",
      "Epoch  472  G loss  0.090957366\n",
      "Epoch  473  G loss  0.15283024\n",
      "Epoch  474  G loss  0.10557065\n",
      "Epoch  475  G loss  0.16107705\n",
      "Epoch  476  G loss  0.10135686\n",
      "Epoch  477  G loss  0.11906324\n",
      "Epoch  478  G loss  0.14570388\n",
      "Epoch  479  G loss  0.12189722\n",
      "Epoch  480  G loss  0.11951423\n",
      "Epoch  481  G loss  0.12793817\n",
      "Epoch  482  G loss  0.1447784\n",
      "Epoch  483  G loss  0.12787274\n",
      "Epoch  484  G loss  0.085582785\n",
      "Epoch  485  G loss  0.2052904\n",
      "Epoch  486  G loss  0.12932542\n",
      "Epoch  487  G loss  0.14933357\n",
      "Epoch  488  G loss  0.11645898\n",
      "Epoch  489  G loss  0.19472104\n",
      "Epoch  490  G loss  0.15820664\n",
      "Epoch  491  G loss  0.2944838\n",
      "Epoch  492  G loss  0.084321365\n",
      "Epoch  493  G loss  0.16021252\n",
      "Epoch  494  G loss  0.12897137\n",
      "Epoch  495  G loss  0.07846889\n",
      "Epoch  496  G loss  0.09643601\n",
      "Epoch  497  G loss  0.12950514\n",
      "Epoch  498  G loss  0.13440295\n",
      "Epoch  499  G loss  0.16390057\n",
      "Epoch  500  G loss  0.0874435\n",
      "Epoch  501  G loss  0.094889924\n",
      "Epoch  502  G loss  0.12647721\n",
      "Epoch  503  G loss  0.098308474\n",
      "Epoch  504  G loss  0.12512752\n",
      "Epoch  505  G loss  0.12745711\n",
      "Epoch  506  G loss  0.16573991\n",
      "Epoch  507  G loss  0.10867466\n",
      "Epoch  508  G loss  0.6859703\n",
      "Epoch  509  G loss  0.12644655\n",
      "Epoch  510  G loss  0.08801657\n",
      "Epoch  511  G loss  0.3689353\n",
      "Epoch  512  G loss  0.38338268\n",
      "Epoch  513  G loss  0.120673195\n",
      "Epoch  514  G loss  0.17682421\n",
      "Epoch  515  G loss  0.42890686\n",
      "Epoch  516  G loss  0.07763716\n",
      "Epoch  517  G loss  0.16702755\n",
      "Epoch  518  G loss  0.2740842\n",
      "Epoch  519  G loss  0.16102076\n",
      "Epoch  520  G loss  0.15110222\n",
      "Epoch  521  G loss  0.18030931\n",
      "Epoch  522  G loss  0.48014742\n",
      "Epoch  523  G loss  0.09937185\n",
      "Epoch  524  G loss  0.06270385\n",
      "Epoch  525  G loss  0.21975948\n",
      "Epoch  526  G loss  0.09782192\n",
      "Epoch  527  G loss  0.21074903\n",
      "Epoch  528  G loss  0.122999504\n",
      "Epoch  529  G loss  0.21983382\n",
      "Epoch  530  G loss  0.15346378\n",
      "Epoch  531  G loss  0.17730111\n",
      "Epoch  532  G loss  0.18357278\n",
      "Epoch  533  G loss  0.24917437\n",
      "Epoch  534  G loss  0.1191432\n",
      "Epoch  535  G loss  0.07226114\n",
      "Epoch  536  G loss  0.25910506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  537  G loss  0.27475244\n",
      "Epoch  538  G loss  0.11113588\n",
      "Epoch  539  G loss  0.19864325\n",
      "Epoch  540  G loss  0.11346568\n",
      "Epoch  541  G loss  0.17962483\n",
      "Epoch  542  G loss  0.26054913\n",
      "Epoch  543  G loss  0.22293706\n",
      "Epoch  544  G loss  0.12793352\n",
      "Epoch  545  G loss  0.10031846\n",
      "Epoch  546  G loss  0.30390105\n",
      "Epoch  547  G loss  0.13533613\n",
      "Epoch  548  G loss  0.16664666\n",
      "Epoch  549  G loss  0.18342027\n",
      "Epoch  550  G loss  0.08709702\n",
      "Epoch  551  G loss  0.095877856\n",
      "Epoch  552  G loss  0.32879424\n",
      "Epoch  553  G loss  0.1269588\n",
      "Epoch  554  G loss  0.08970074\n",
      "Epoch  555  G loss  0.16876061\n",
      "Epoch  556  G loss  0.11184595\n",
      "Epoch  557  G loss  0.17259908\n",
      "Epoch  558  G loss  0.5708494\n",
      "Epoch  559  G loss  0.2099165\n",
      "Epoch  560  G loss  0.12742642\n",
      "Epoch  561  G loss  0.07201323\n",
      "Epoch  562  G loss  0.08735267\n",
      "Epoch  563  G loss  0.1023989\n",
      "Epoch  564  G loss  0.15868297\n"
=======
      "input_1 (InputLayer)            (None, 384, 384, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 384, 384, 16) 448         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 384, 384, 16) 0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 384, 384, 16) 64          leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 382, 382, 16) 2320        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 382, 382, 16) 0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 382, 382, 16) 64          leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 190, 190, 16) 2320        batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 190, 190, 16) 0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 190, 190, 16) 64          leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPadding2D (None, 192, 192, 16) 0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 192, 192, 32) 4640        zero_padding2d_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 192, 192, 32) 0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 192, 192, 32) 128         leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 190, 190, 32) 9248        batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, 190, 190, 32) 0           conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 190, 190, 32) 128         leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 94, 94, 32)   9248        batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)       (None, 94, 94, 32)   0           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 94, 94, 32)   128         leaky_re_lu_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 94, 94, 64)   18496       batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)       (None, 94, 94, 64)   0           conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 94, 94, 64)   256         leaky_re_lu_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 94, 94, 64)   36928       batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)       (None, 94, 94, 64)   0           conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 94, 94, 64)   256         leaky_re_lu_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 46, 46, 64)   36928       batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)       (None, 46, 46, 64)   0           conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 46, 46, 64)   256         leaky_re_lu_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 46, 46, 64)   36928       batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)      (None, 46, 46, 64)   0           conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 46, 46, 64)   256         leaky_re_lu_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 46, 46, 64)   36928       batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)      (None, 46, 46, 64)   0           conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 46, 46, 64)   256         leaky_re_lu_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 22, 22, 64)   36928       batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)      (None, 22, 22, 64)   0           conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 22, 22, 64)   256         leaky_re_lu_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 22, 22, 64)   36928       batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)      (None, 22, 22, 64)   0           conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 22, 22, 64)   256         leaky_re_lu_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 20, 20, 1)    577         batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)      (None, 20, 20, 1)    0           conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 20, 20, 1)    4           leaky_re_lu_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 9, 9, 1)      10          batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)      (None, 9, 9, 1)      0           conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 9, 9, 1)      4           leaky_re_lu_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2D)  (None, 18, 18, 1)    0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_2 (ZeroPadding2D (None, 20, 20, 1)    0           up_sampling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 20, 20, 1)    10          zero_padding2d_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_16 (LeakyReLU)      (None, 20, 20, 1)    0           conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 20, 20, 1)    4           leaky_re_lu_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2D)  (None, 40, 40, 1)    0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_3 (ZeroPadding2D (None, 46, 46, 1)    0           up_sampling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 46, 46, 64)   640         zero_padding2d_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_17 (LeakyReLU)      (None, 46, 46, 64)   0           conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 46, 46, 64)   256         leaky_re_lu_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 46, 46, 128)  0           batch_normalization_17[0][0]     \n",
      "                                                                 batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 46, 46, 64)   73792       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_18 (LeakyReLU)      (None, 46, 46, 64)   0           conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 46, 46, 64)   256         leaky_re_lu_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 46, 46, 64)   36928       batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_19 (LeakyReLU)      (None, 46, 46, 64)   0           conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 46, 46, 64)   256         leaky_re_lu_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2D)  (None, 92, 92, 64)   0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_4 (ZeroPadding2D (None, 94, 94, 64)   0           up_sampling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 94, 94, 64)   36928       zero_padding2d_4[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_20 (LeakyReLU)      (None, 94, 94, 64)   0           conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 94, 94, 64)   256         leaky_re_lu_20[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 94, 94, 128)  0           batch_normalization_8[0][0]      \n",
      "                                                                 batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 94, 94, 64)   73792       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_21 (LeakyReLU)      (None, 94, 94, 64)   0           conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 94, 94, 64)   256         leaky_re_lu_21[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 94, 94, 64)   36928       batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_22 (LeakyReLU)      (None, 94, 94, 64)   0           conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 94, 94, 64)   256         leaky_re_lu_22[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_4 (UpSampling2D)  (None, 188, 188, 64) 0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_5 (ZeroPadding2D (None, 190, 190, 64) 0           up_sampling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 190, 190, 96) 0           batch_normalization_5[0][0]      \n",
      "                                                                 zero_padding2d_5[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_6 (ZeroPadding2D (None, 192, 192, 96) 0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 192, 192, 32) 27680       zero_padding2d_6[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_23 (LeakyReLU)      (None, 192, 192, 32) 0           conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 192, 192, 32) 128         leaky_re_lu_23[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 192, 192, 32) 9248        batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_24 (LeakyReLU)      (None, 192, 192, 32) 0           conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 192, 192, 32) 128         leaky_re_lu_24[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_5 (UpSampling2D)  (None, 384, 384, 32) 0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 382, 382, 16) 4624        up_sampling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_25 (LeakyReLU)      (None, 382, 382, 16) 0           conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 382, 382, 16) 64          leaky_re_lu_25[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 382, 382, 32) 0           batch_normalization_2[0][0]      \n",
      "                                                                 batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_7 (ZeroPadding2D (None, 384, 384, 32) 0           concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 384, 384, 16) 4624        zero_padding2d_7[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_26 (LeakyReLU)      (None, 384, 384, 16) 0           conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 384, 384, 16) 64          leaky_re_lu_26[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 384, 384, 16) 2320        batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_27 (LeakyReLU)      (None, 384, 384, 16) 0           conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 384, 384, 16) 64          leaky_re_lu_27[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 384, 384, 3)  51          batch_normalization_27[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 580,804\n",
      "Trainable params: 578,622\n",
      "Non-trainable params: 2,182\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "cgan2 = CGAN()\n",
    "#cgan2.generator.load_weights('../data/weights/generator_weights_3000.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../data/sources/'\n",
    "def myGenerator(batch_size):\n",
    "    global tdx\n",
    "    while True:\n",
    "        index_list = random.sample(range(1, totalImages), batch_size)\n",
    "        alldata_y = []\n",
    "        for i in range(30, 120):#index_list:\n",
    "            frame = np.load(path+'nparrs_384/frame'+str(i)+'.npy')\n",
    "            alldata_y.append(frame[30])\n",
    "        alldata_y = np.array(alldata_y)\n",
    "        alldata_y = (alldata_y.astype(np.float32) - 127.5) / 127.5\n",
    "        yield alldata_y, alldata_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:15: DeprecationWarning: `imsave` is deprecated!\n",
      "`imsave` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imwrite`` instead.\n",
      "  from ipykernel import kernelapp as app\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:19: DeprecationWarning: `imsave` is deprecated!\n",
      "`imsave` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imwrite`` instead.\n"
>>>>>>> autoencoder
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "cgan = CGAN()\n",
    "cgan.build_autoencoder()\n",
    "cgan.train_generator_autoencoder(100000, 8, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cgan.train(10000, batch_size=10, sample_interval=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cgan2 = CGAN()\n",
    "cgan2.generator.load_weights('../data/weights/generator_weights_3000.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "for i in range(100):\n",
    "    x = myGenerator(1)\n",
    "    xtest, ytest = next(x)\n",
    "    #print('xtrain shape:',xtest.shape)\n",
    "    #print('ytrain shape:',ytest.shape)\n",
    "    pred = cgan.generator.predict(xtest)\n",
    "    pred = pred*127.5 + 127.5\n",
    "    pred = pred.astype(int)\n",
    "    print(pred.dtype)\n",
    "    plt.imshow(pred[0])\n",
    "    plt.show()\n",
    "    ytest = ytest*127.5+127.5\n",
    "    ytest = ytest.astype(int)\n",
    "    plt.imshow(ytest[0])\n",
    "    plt.show()\n",
    "    #break\n",
    "    #time.sleep(1)"
=======
    "savepath = '../data/sources/'\n",
    "for i in range(1):\n",
    "    x = myGenerator(90)\n",
    "    X_train, ytest = next(x)\n",
    "\n",
    "    gen_imgs         = cgan.generator.predict(X_train)\n",
    "\n",
    "    # Rescale images 0 - 1\n",
    "    temp     = (0.5 * gen_imgs + 0.5)*255\n",
    "    X_train = (0.5 * X_train + 0.5)*255\n",
    "    gen_imgs = temp.astype(int)\n",
    "    \n",
    "    idx = 0\n",
    "    for j in gen_imgs:\n",
    "        imsave(savepath+'prediction/tile_segment_image'+str(idx)+'.png', j)\n",
    "        idx += 1\n",
    "    idx = 0\n",
    "    for j in X_train:\n",
    "        imsave(savepath+'real/tile_segment_image_real'+str(idx)+'.png', j)\n",
    "        idx += 1"
>>>>>>> autoencoder
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = cgan.generator.predict(xtest)\n",
    "print(pred[0])"
=======
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#cgan = CGAN()\n",
    "#cgan.build_discriminator()\n",
    "#cgan.build_generator()\n",
    "cgan.train(100000, 6, 100)\n",
    "#cgan.build_autoencoder()\n",
    "#cgan.train_generator_autoencoder(100000, 8, 100)"
>>>>>>> ganvanilla
=======
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.467476844787598\n",
      "1.785005807876587\n",
      "1.7195923328399658\n",
      "1.7406260967254639\n",
      "1.7078578472137451\n",
      "1.7259161472320557\n",
      "1.7722742557525635\n",
      "1.7466368675231934\n",
      "1.686011552810669\n",
      "1.7431378364562988\n",
      "2.009453558921814\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "a = []\n",
    "for i in range(10):\n",
    "    stime = time.time()\n",
    "    pred = cgan2.generator.predict(xtrain)\n",
    "    etime = time.time()\n",
    "    tp = etime-stime\n",
    "    a.append(tp)\n",
    "    print(tp)\n",
    "print(np.mean(a))"
>>>>>>> autoencoder
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
