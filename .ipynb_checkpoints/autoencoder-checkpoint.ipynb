{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, multiply, GlobalAveragePooling2D, MaxPooling2D\n",
    "from keras.layers import BatchNormalization, Activation, Embedding, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from scipy.misc import imread\n",
    "from scipy.misc import imresize\n",
    "from scipy.misc import imsave\n",
    "import glob\n",
    "import random\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import applications\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import to_categorical\n",
    "#from keras.applications.resnet50 import preprocess_input\n",
    "\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "#from keras.applications.vgg19 import preprocess_input as preprocess_vgg\n",
    "from keras.applications.inception_v3 import preprocess_input as inception_preprocess\n",
    "\n",
    "#from keras.applications.inception_resnet_v2 import preprocess_input\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import keras\n",
    "import copy\n",
    "import cv2\n",
    "\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "\n",
    "import numpy as np \n",
    "import os\n",
    "import numpy as np\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras import backend as keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../data/sources/nparrs_384/'\n",
    "savepath = '../data/'\n",
    "images = glob.glob(path+'*.npy')\n",
    "totalImages = 1800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myGenerator(batch_size):\n",
    "    while True:\n",
    "        index_list = random.sample(range(1, totalImages), batch_size)\n",
    "        alldata_x = []\n",
    "        alldata_y = []\n",
    "        for i in index_list:\n",
    "            frame = images[i]\n",
    "            frame = np.load(frame)\n",
    "            tile_index = np.random.randint(0, 49)\n",
    "            #print(i, tile_index, frame.shape)\n",
    "            alldata_x.append(tile_index*totalImages+i)\n",
    "            alldata_y.append(frame[tile_index])\n",
    "        alldata_x = np.array(alldata_x)\n",
    "        alldata_y = np.array(alldata_y)\n",
    "        alldata_y = (alldata_y.astype(np.float32) - 127.5) / 127.5\n",
    "        yield alldata_y, alldata_y\n",
    "#x = myGenerator(10)\n",
    "#xtrain, ytrain = next(x)\n",
    "#print('xtrain shape:',xtrain.shape)\n",
    "#print('ytrain shape:',ytrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77 137 138 5 [116, 117, 118, 136, 138, 156, 157, 158]\n",
      "952 29 50 8 [8, 9, 10, 28, 30, 48, 49, 50]\n",
      "xtrain shape: (2, 192, 192, 3)\n",
      "ytrain shape: (2, 192, 192, 3)\n"
     ]
    }
   ],
   "source": [
    "def myGenerator_new(batch_size):\n",
    "    while True:\n",
    "        index_list = random.sample(range(1, totalImages), batch_size)\n",
    "        alldata_x = []\n",
    "        alldata_y = []\n",
    "        for i in index_list:\n",
    "            frame = images[i]\n",
    "            frame = np.load(frame)\n",
    "            tdx = np.random.randint(0, 199)\n",
    "            choices = None\n",
    "            classes = {}\n",
    "            if tdx < 20:\n",
    "                if tdx == 0:\n",
    "                    choices = [1, 20, 21]\n",
    "                    classes = {1:5, 20:7, 21:8}\n",
    "                elif tdx == 19:\n",
    "                    choices = [18, 38, 39]\n",
    "                    classes = {18:3, 38:6, 29:7}\n",
    "                else:\n",
    "                    choices = [tdx-1, tdx+1, tdx+19, tdx+20, tdx+21]\n",
    "                    classes = {tdx-1:3, tdx+1:5, tdx+19:6, tdx+20:7, tdx+21:8}\n",
    "            elif tdx > 179:\n",
    "                if tdx == 199:\n",
    "                    choices = [178, 179, 198]\n",
    "                    classes = {178:0, 179:1, 198:3}\n",
    "                elif tdx == 180:\n",
    "                    choices = [160, 161, 181]\n",
    "                    classes = {160:0, 161:1, 181:5}\n",
    "                else:\n",
    "                    choices = [tdx-21, tdx-20, tdx-19, tdx-1, tdx+1]\n",
    "                    classes = {tdx-21:0, tdx-20:1, tdx-19:2, tdx-1:3, tdx+1:5}\n",
    "            else:\n",
    "                choices = [tdx-21, tdx-20, tdx-19, tdx-1, tdx+1, tdx+19, tdx+20, tdx+21]\n",
    "                classes = {tdx-21:0, tdx-20:1, tdx-19:2, tdx-1:3, tdx+1:5, tdx+19:6, tdx+20:7, tdx+21:8}\n",
    "            choice = random.choice(choices)\n",
    "            cls = classes[choice]\n",
    "            print(i, tdx, choice, cls, choices)\n",
    "            alldata_x.append(frame[tdx])\n",
    "            alldata_y.append(frame[choice])\n",
    "        alldata_x = np.array(alldata_x)\n",
    "        alldata_y = np.array(alldata_y)\n",
    "        alldata_y = (alldata_y.astype(np.float32) - 127.5) / 127.5\n",
    "        yield alldata_x, alldata_y, cls\n",
    "x = myGenerator_new(2)\n",
    "xtrain, ytrain, cls = next(x)\n",
    "print('xtrain shape:',xtrain.shape)\n",
    "print('ytrain shape:',ytrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 2}\n"
     ]
    }
   ],
   "source": [
    "tdx = 1\n",
    "cls = {}\n",
    "cls = {tdx:2}\n",
    "print(cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CGAN():\n",
    "    def __init__(self):\n",
    "        # Input shape\n",
    "        self.img_rows = 384\n",
    "        self.img_cols = 384\n",
    "        self.channels = 3\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        #self.discriminator = self.build_discriminator()\n",
    "        #self.discriminator.compile(loss=['mse'],\n",
    "        #    optimizer=optimizer,\n",
    "        #    metrics=['accuracy'])\n",
    "        \n",
    "        #print(self.discriminator.summary())\n",
    "        \n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "        \n",
    "        print(self.generator.summary())\n",
    "        \n",
    "        noise = Input(shape=(384, 384, 3))\n",
    "        img = self.generator(noise)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        #self.discriminator.trainable = False\n",
    "\n",
    "        # The discriminator takes generated image as input and determines validity\n",
    "        # and the label of that image\n",
    "        #valid = self.discriminator(img)\n",
    "        \n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains generator to fool discriminator\n",
    "        #self.combined = Model(noise, [img, valid])\n",
    "        #self.combined.compile(loss=['mse', 'mse'],\n",
    "        #    loss_weights=[0.9, 0.1],\n",
    "        #    optimizer=optimizer)\n",
    "\n",
    "    def build_generator(self):\n",
    "        input_size = (384,384,3)\n",
    "        inputs = Input(input_size)\n",
    "\n",
    "        conv1_a = Conv2D(16, 3, padding = 'same', kernel_initializer = 'he_normal')(inputs)\n",
    "        conv1_a = LeakyReLU(alpha=0.2)(conv1_a)\n",
    "        conv1_a = BatchNormalization(momentum=0.8)(conv1_a)\n",
    "\n",
    "        conv1_a = Conv2D(16, 3, padding = 'valid', kernel_initializer = 'he_normal')(conv1_a)\n",
    "        conv1_a = LeakyReLU(alpha=0.2)(conv1_a)\n",
    "        conv1_a = BatchNormalization(momentum=0.8)(conv1_a)\n",
    "\n",
    "        pool1_a = Conv2D(16, 3, subsample=(2, 2), kernel_initializer = 'he_normal')(conv1_a)\n",
    "        pool1_a = LeakyReLU(alpha=0.2)(pool1_a)\n",
    "        pool1_a = BatchNormalization(momentum=0.8)(pool1_a)\n",
    "        pool1_a = ZeroPadding2D(padding=(1, 1))(pool1_a)\n",
    "\n",
    "\n",
    "\n",
    "        conv1 = Conv2D(32, 3, padding = 'same', kernel_initializer = 'he_normal')(pool1_a)\n",
    "        conv1 = LeakyReLU(alpha=0.2)(conv1)\n",
    "        conv1 = BatchNormalization(momentum=0.8)(conv1)\n",
    "\n",
    "        conv1 = Conv2D(32, 3, padding = 'valid', kernel_initializer = 'he_normal')(conv1)\n",
    "        conv1 = LeakyReLU(alpha=0.2)(conv1)\n",
    "        conv1 = BatchNormalization(momentum=0.8)(conv1)\n",
    "\n",
    "        pool1 = Conv2D(32, 3, subsample=(2, 2), kernel_initializer = 'he_normal')(conv1)\n",
    "        pool1 = LeakyReLU(alpha=0.2)(pool1)\n",
    "        pool1 = BatchNormalization(momentum=0.8)(pool1)\n",
    "\n",
    "\n",
    "\n",
    "        conv2 = Conv2D(64, 3, padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
    "        conv2 = LeakyReLU(alpha=0.2)(conv2)\n",
    "        conv2 = BatchNormalization(momentum=0.8)(conv2)\n",
    "\n",
    "        conv2 = Conv2D(64, 3, padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
    "        conv2 = LeakyReLU(alpha=0.2)(conv2)\n",
    "        conv2 = BatchNormalization(momentum=0.8)(conv2)\n",
    "\n",
    "        pool2 = Conv2D(64, 3, subsample=(2, 2), kernel_initializer = 'he_normal')(conv2)\n",
    "        pool2 = LeakyReLU(alpha=0.2)(pool2)\n",
    "        pool2 = BatchNormalization(momentum=0.8)(pool2)\n",
    "\n",
    "\n",
    "\n",
    "        conv2_a = Conv2D(64, 3, padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
    "        conv2_a = LeakyReLU(alpha=0.2)(conv2_a)\n",
    "        conv2_a = BatchNormalization(momentum=0.8)(conv2_a)\n",
    "\n",
    "        conv2_a = Conv2D(64, 3, padding = 'same', kernel_initializer = 'he_normal')(conv2_a)\n",
    "        conv2_a = LeakyReLU(alpha=0.2)(conv2_a)\n",
    "        conv2_a = BatchNormalization(momentum=0.8)(conv2_a)\n",
    "\n",
    "        pool3   = Conv2D(64, 3, subsample=(2, 2), kernel_initializer = 'he_normal')(conv2_a)\n",
    "        pool3   = LeakyReLU(alpha=0.2)(pool3)\n",
    "        pool3   = BatchNormalization(momentum=0.8)(pool3)\n",
    "\n",
    "\n",
    "\n",
    "        conv3 = Conv2D(64, 3, padding = 'same', kernel_initializer = 'he_normal')(pool3)\n",
    "        conv3 = LeakyReLU(alpha=0.2)(conv3)\n",
    "        conv3 = BatchNormalization(momentum=0.8)(conv3)\n",
    "\n",
    "        conv3 = Conv2D(1, 3, padding = 'valid', kernel_initializer = 'he_normal')(conv3)\n",
    "        conv3 = LeakyReLU(alpha=0.2)(conv3)\n",
    "        conv3 = BatchNormalization(momentum=0.8)(conv3)\n",
    "\n",
    "\n",
    "        conv3_pool = Conv2D(1, 3, subsample=(2, 2), kernel_initializer = 'he_normal')(conv3)\n",
    "        conv3_pool = LeakyReLU(alpha=0.2)(conv3_pool)\n",
    "        conv3_pool = BatchNormalization(momentum=0.8)(conv3_pool)\n",
    "\n",
    "        # #Decoder\n",
    "\n",
    "        conv3_unpool = UpSampling2D(size = (2,2))(conv3_pool)\n",
    "        conv3_unpool = ZeroPadding2D(padding=(1, 1))(conv3_unpool)\n",
    "\n",
    "        conv3_unpool = Conv2D(1, 3, padding = 'same', kernel_initializer = 'he_normal')(conv3_unpool)\n",
    "        conv3_unpool = LeakyReLU(alpha=0.2)(conv3_unpool)\n",
    "        conv3_unpool = BatchNormalization(momentum=0.8)(conv3_unpool)\n",
    "\n",
    "\n",
    "        up1    = UpSampling2D(size = (2,2))(conv3_unpool)\n",
    "        #up1    = UpSampling2D(size = (2,2))(conv3)\n",
    "        up1    = ZeroPadding2D(padding=(3, 3))(up1)\n",
    "\n",
    "\n",
    "        conv4 = Conv2D(64, 3, padding = 'same', kernel_initializer = 'he_normal')(up1)\n",
    "        conv4 = LeakyReLU(alpha=0.2)(conv4)\n",
    "        conv4 = BatchNormalization(momentum=0.8)(conv4)\n",
    "\n",
    "        merge1 = concatenate([conv4, conv2_a], axis = 3)\n",
    "\n",
    "        conv5 = Conv2D(64, 3, padding = 'same', kernel_initializer = 'he_normal')(merge1)\n",
    "        conv5 = LeakyReLU(alpha=0.2)(conv5)\n",
    "        conv5 = BatchNormalization(momentum=0.8)(conv5)\n",
    "\n",
    "        conv5 = Conv2D(64, 3, padding = 'same', kernel_initializer = 'he_normal')(conv5)\n",
    "        conv5 = LeakyReLU(alpha=0.2)(conv5)\n",
    "        conv5 = BatchNormalization(momentum=0.8)(conv5)\n",
    "\n",
    "\n",
    "        up2    = UpSampling2D(size = (2,2))(conv5)\n",
    "        up2    = ZeroPadding2D(padding=(1, 1))(up2)\n",
    "\n",
    "        conv6 = Conv2D(64, 3, padding = 'same', kernel_initializer = 'he_normal')(up2)\n",
    "        conv6 = LeakyReLU(alpha=0.2)(conv6)\n",
    "        conv6 = BatchNormalization(momentum=0.8)(conv6)\n",
    "\n",
    "        merge2 = concatenate([conv2, conv6], axis = 3)\n",
    "\n",
    "        conv7 = Conv2D(64, 3, padding = 'same', kernel_initializer = 'he_normal')(merge2)\n",
    "        conv7 = LeakyReLU(alpha=0.2)(conv7)\n",
    "        conv7 = BatchNormalization(momentum=0.8)(conv7)\n",
    "\n",
    "        conv8 = Conv2D(64, 3, padding = 'same', kernel_initializer = 'he_normal')(conv7)\n",
    "        conv8 = LeakyReLU(alpha=0.2)(conv8)\n",
    "        conv8 = BatchNormalization(momentum=0.8)(conv8)\n",
    "\n",
    "        up3    = UpSampling2D(size = (2,2))(conv8)\n",
    "        up3    = ZeroPadding2D(padding=(1, 1))(up3)\n",
    "\n",
    "        merge3 = concatenate([conv1, up3], axis = 3)\n",
    "        merge3 = ZeroPadding2D(padding=(1, 1))(merge3)\n",
    "\n",
    "        conv9 = Conv2D(32, 3, padding = 'same', kernel_initializer = 'he_normal')(merge3)\n",
    "        conv9 = LeakyReLU(alpha=0.2)(conv9)\n",
    "        conv9 = BatchNormalization(momentum=0.8)(conv9)\n",
    "\n",
    "        conv10 = Conv2D(32, 3, padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "        conv10 = LeakyReLU(alpha=0.2)(conv10)\n",
    "        conv10 = BatchNormalization(momentum=0.8)(conv10)\n",
    "\n",
    "\n",
    "        up4    = UpSampling2D(size = (2,2))(conv10)\n",
    "\n",
    "        conv11 = Conv2D(16, 3, padding = 'valid', kernel_initializer = 'he_normal')(up4)\n",
    "        conv11 = LeakyReLU(alpha=0.2)(conv11)\n",
    "        conv11 = BatchNormalization(momentum=0.8)(conv11)\n",
    "\n",
    "        merge4 = concatenate([conv1_a, conv11], axis = 3)\n",
    "        merge4 = ZeroPadding2D(padding=(1, 1))(merge4)\n",
    "\n",
    "        conv12 = Conv2D(16, 3, padding = 'same', kernel_initializer = 'he_normal')(merge4)\n",
    "        conv12 = LeakyReLU(alpha=0.2)(conv12)\n",
    "        conv12 = BatchNormalization(momentum=0.8)(conv12)\n",
    "\n",
    "        conv13 = Conv2D(16, 3, padding = 'same', kernel_initializer = 'he_normal')(conv12)\n",
    "        conv13 = LeakyReLU(alpha=0.2)(conv13)\n",
    "        conv13 = BatchNormalization(momentum=0.8)(conv13)\n",
    "\n",
    "\n",
    "        conv14 = Conv2D(3, 1, padding = 'same', kernel_initializer = 'he_normal', activation='tanh')(conv13)\n",
    "        \n",
    "        model  = Model(input = inputs, output = conv14)\n",
    "        return model\n",
    "    \n",
    "    def build_autoencoder(self):\n",
    "        self.generator.compile(loss=['mse'],optimizer=self.optimizer)\n",
    "    \n",
    "    def train_generator_autoencoder(self, epochs, batch_size=128, sample_interval=10):\n",
    "        for epoch in range(epochs):\n",
    "            X_train, y_train = next(myGenerator(batch_size))\n",
    "            g_loss = self.generator.train_on_batch(X_train, X_train)\n",
    "            print (\"Epoch \", epoch, \" G loss \", g_loss)\n",
    "            if epoch % sample_interval == 0:\n",
    "                self.sample_images(epoch)\n",
    "                self.generator.save_weights(savepath+'weights/generator_weights_'+str(epoch)+'.h5')\n",
    "            \n",
    "    def build_discriminator(self):\n",
    "        img   = Input(shape=(384, 384, 3))\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(64, (3, 3), input_shape=(192, 192, 3)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        #model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(64,  (3, 3),  strides=(2, 2)))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        #model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(64, (3, 3)))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        #model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(64, (6, 6),  strides=(2, 2)))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        #model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(64, (3, 3),  strides=(2, 2)))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Flatten())\n",
    "        \n",
    "        model.add(Dense(100))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        \n",
    "        model.add(Dense(32))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(1))\n",
    "        \n",
    "        output    = model(img)\n",
    "        \n",
    "        model3 = Model(img, output)\n",
    "        return model3\n",
    "\n",
    "    def train(self, epochs, batch_size=128, sample_interval=50):\n",
    "        random.seed(10)\n",
    "        \n",
    "        # Load the dataset\n",
    "        for epoch in range(epochs):\n",
    "            X_train, y_train = next(myGenerator(batch_size))\n",
    "            \n",
    "            # Adversarial ground truths\n",
    "            valid = np.ones((batch_size, 1))\n",
    "            fake  = np.zeros((batch_size, 1))\n",
    "            \n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "            \n",
    "            # Generate a half batch of new images\n",
    "            gen_imgs = self.generator.predict(X_train)\n",
    "\n",
    "            # Train the discriminator\n",
    "            d_loss_real = self.discriminator.train_on_batch(X_train, valid)\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            # Train the generator\n",
    "            g_loss = self.combined.train_on_batch(X_train, [X_train, valid])\n",
    "\n",
    "            # Plot the progress\n",
    "            print (\"%d [D loss: %f, mean_acc: %.2f%% real_acc: %.2f%% fake_acc: %.2f%%] [G loss: %f, mse: %f]\" % (epoch, d_loss[0], 100*d_loss[1], 100*d_loss_real[1], 100*d_loss_fake[1], g_loss[0], g_loss[1]))\n",
    "\n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % sample_interval == 0:\n",
    "                self.sample_images(epoch)\n",
    "                self.generator.save_weights(savepath+'weights/generator_weights_'+str(epoch)+'.h5')\n",
    "                self.discriminator.save_weights(savepath+'weights/discriminator_weights_'+str(epoch)+'.h5')\n",
    "\n",
    "    def sample_images(self, epoch):\n",
    "        r, c             = 1, 10\n",
    "        X_train, y_train = next(myGenerator(10))\n",
    "        gen_imgs         = self.generator.predict(X_train)\n",
    "        \n",
    "        # Rescale images 0 - 1\n",
    "        temp     = (0.5 * gen_imgs + 0.5)*255\n",
    "        gen_imgs = temp.astype(int)\n",
    "        \n",
    "        combined = np.array([gen_imgs[0], gen_imgs[1], gen_imgs[2], gen_imgs[3], gen_imgs[4], gen_imgs[5], gen_imgs[6], gen_imgs[7], gen_imgs[8], gen_imgs[9]])\n",
    "        combined = np.hstack(combined.reshape(10, 384,384, 3))\n",
    "        imsave(savepath+\"images/\"+str(epoch)+\".png\", combined)\n",
    "        \n",
    "        combined = np.array([X_train[0], X_train[1], X_train[2], X_train[3], X_train[4], X_train[5], X_train[6], X_train[7], X_train[8], X_train[9]])\n",
    "        combined = np.hstack(combined.reshape(10, 384,384, 3))\n",
    "        imsave(savepath+\"images/\"+str(epoch)+\"_real.png\", combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cgan = CGAN()\n",
    "cgan.build_autoencoder()\n",
    "cgan.train_generator_autoencoder(1000000, 10, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cgan.train(10000, batch_size=10, sample_interval=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cgan2 = CGAN()\n",
    "cgan2.generator.load_weights('../data/weights/generator_weights_3000.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../data/sources/'\n",
    "def myGenerator(batch_size):\n",
    "    global tdx\n",
    "    while True:\n",
    "        index_list = random.sample(range(1, totalImages), batch_size)\n",
    "        alldata_y = []\n",
    "        for i in range(30, 120):#index_list:\n",
    "            frame = np.load(path+'nparrs_384/frame'+str(i)+'.npy')\n",
    "            alldata_y.append(frame[30])\n",
    "        alldata_y = np.array(alldata_y)\n",
    "        alldata_y = (alldata_y.astype(np.float32) - 127.5) / 127.5\n",
    "        yield alldata_y, alldata_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:15: DeprecationWarning: `imsave` is deprecated!\n",
      "`imsave` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imwrite`` instead.\n",
      "  from ipykernel import kernelapp as app\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:19: DeprecationWarning: `imsave` is deprecated!\n",
      "`imsave` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imwrite`` instead.\n"
     ]
    }
   ],
   "source": [
    "savepath = '../data/sources/'\n",
    "for i in range(1):\n",
    "    x = myGenerator(90)\n",
    "    X_train, ytest = next(x)\n",
    "\n",
    "    gen_imgs         = cgan.generator.predict(X_train)\n",
    "\n",
    "    # Rescale images 0 - 1\n",
    "    temp     = (0.5 * gen_imgs + 0.5)*255\n",
    "    X_train = (0.5 * X_train + 0.5)*255\n",
    "    gen_imgs = temp.astype(int)\n",
    "    \n",
    "    idx = 0\n",
    "    for j in gen_imgs:\n",
    "        imsave(savepath+'prediction/tile_segment_image'+str(idx)+'.png', j)\n",
    "        idx += 1\n",
    "    idx = 0\n",
    "    for j in X_train:\n",
    "        imsave(savepath+'real/tile_segment_image_real'+str(idx)+'.png', j)\n",
    "        idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = cgan.generator.predict(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
