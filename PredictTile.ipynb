{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, multiply, GlobalAveragePooling2D\n",
    "from keras.layers import BatchNormalization, Activation, Embedding, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from scipy.misc import imread\n",
    "from scipy.misc import imresize\n",
    "from scipy.misc import imsave\n",
    "import glob\n",
    "import random\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import applications\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import to_categorical\n",
    "#from keras.applications.resnet50 import preprocess_input\n",
    "\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "#from keras.applications.vgg19 import preprocess_input as preprocess_vgg\n",
    "from keras.applications.inception_v3 import preprocess_input as inception_preprocess\n",
    "\n",
    "#from keras.applications.inception_resnet_v2 import preprocess_input\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import keras\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../data/nparrs/'\n",
    "savepath = '../data/'\n",
    "images = glob.glob(path+'*.npy')\n",
    "totalImages = 1800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myGenerator(batch_size):\n",
    "    while True:\n",
    "        index_list = random.sample(range(1, totalImages), batch_size)\n",
    "        alldata_x = []\n",
    "        alldata_y = []\n",
    "        for i in index_list:\n",
    "            frame = images[i]\n",
    "            frame = np.load(frame)\n",
    "            tile_index = np.random.randint(0, 199)\n",
    "            #print(i, tile_index, frame.shape)\n",
    "            alldata_x.append(tile_index*totalImages+i)\n",
    "            alldata_y.append(frame[tile_index])\n",
    "        alldata_x = np.array(alldata_x)\n",
    "        #alldata_x = np.rollaxis(alldata_x, 1, 5)  \n",
    "        #alldata_x = alldata_x.reshape((32, 30, 240, 480, 3))\n",
    "        #alldata_x = np.swapaxes(alldata_x, 1, 4)\n",
    "        alldata_y = np.array(alldata_y)\n",
    "        alldata_y = (alldata_y.astype(np.float32) - 127.5) / 127.5\n",
    "        yield alldata_x, alldata_y\n",
    "#x = myGenerator()\n",
    "#xtrain, ytrain = next(x)\n",
    "#print('xtrain shape:',xtrain.shape)\n",
    "#print('ytrain shape:',ytrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CGAN():\n",
    "    def __init__(self):\n",
    "        # Input shape\n",
    "        self.img_rows = 192\n",
    "        self.img_cols = 192\n",
    "        self.channels = 3\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.num_classes = 360000\n",
    "        self.latent_dim  = 200\n",
    "        self.embedding_layer = Embedding(self.num_classes, self.latent_dim)\n",
    "        \n",
    "        self.test_tiles = np.array([np.random.randint(0, 1800*200, 10)])\n",
    "        \n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss=['mse'],\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # The generator takes noise and the target label as input\n",
    "        # and generates the corresponding digit of that label\n",
    "        noise = Input(shape=(1,))\n",
    "        #label = Input(shape=(1,))\n",
    "        img = self.generator(noise)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The discriminator takes generated image as input and determines validity\n",
    "        # and the label of that image\n",
    "        valid = self.discriminator([img, noise])\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains generator to fool discriminator\n",
    "        self.combined = Model(noise, valid)\n",
    "        self.combined.compile(loss=['mse'],\n",
    "            optimizer=optimizer)\n",
    "\n",
    "    def build_generator(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(192 * 192 * 3, activation='relu', input_dim=self.latent_dim))\n",
    "        model.add(Reshape((192, 192, 3)))\n",
    "        model.add(Conv2D(256, kernel_size=3, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(Conv2D(128, kernel_size=3, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(Conv2D(64, kernel_size=3, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(Conv2D(3, kernel_size=3, padding=\"same\"))\n",
    "        model.add(Activation(\"tanh\"))\n",
    "\n",
    "        label           = Input(shape=(1,), dtype='int32')\n",
    "        label_embedding = Flatten()(self.embedding_layer(label))\n",
    "        img             = model(label_embedding)\n",
    "\n",
    "        model2 =  Model(label, img)\n",
    "        print(model2.summary())\n",
    "        return model2\n",
    "\n",
    "    def build_discriminator(self):\n",
    "        img   = Input(shape=self.img_shape)\n",
    "        label = Input(shape=(1,), dtype='int32')\n",
    "        label_embedding = Flatten()(self.embedding_layer(label))\n",
    "        \n",
    "        label_dense = Dense(100)(label_embedding)\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(64, (3, 3), input_shape=(192, 192, 3)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        #model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(64,  (3, 3),  strides=(2, 2)))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        #model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(64, (3, 3)))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        #model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(64, (6, 6),  strides=(2, 2)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        #model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(64, (3, 3),  strides=(2, 2)))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        #model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(128, (3, 3)))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Conv2D(128, (3, 3),  strides=(2, 2)))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(100))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        #model.add(Dropout(0.25))\n",
    "        #model.add(Dense(512))\n",
    "        #model.add(LeakyReLU(alpha=0.2))\n",
    "        \n",
    "        dense100    = model(img)\n",
    "        conditioned = multiply([dense100, label_dense])\n",
    "        #dense64     = Dense(128)(conditioned)\n",
    "        #dense64l    = LeakyReLU(alpha=0.2)(dense64)\n",
    "        dense32     = Dense(32)(conditioned)\n",
    "        dense32l    = LeakyReLU(alpha=0.2)(dense32)\n",
    "        \n",
    "        #For ls gan\n",
    "        validity = Dense(1)(dense32l)\n",
    "        #validity = Dense(1, activation='sigmoid')(dense32l)\n",
    "        \n",
    "        model3 = Model([img, label], validity) \n",
    "        print(model3.summary())\n",
    "        return model3\n",
    "\n",
    "    def train(self, epochs, batch_size=128, sample_interval=50):\n",
    "        random.seed(10)\n",
    "        \n",
    "        # Load the dataset\n",
    "        for epoch in range(epochs):\n",
    "            X_train, y_train = next(myGenerator(batch_size))\n",
    "            \n",
    "            # Adversarial ground truths\n",
    "            valid = np.ones((batch_size, 1))\n",
    "            fake  = np.zeros((batch_size, 1))\n",
    "            \n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random half batch of images\n",
    "            #idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "            noise, imgs = X_train, y_train\n",
    "\n",
    "            # Sample noise as generator input\n",
    "            # noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "\n",
    "            # Generate a half batch of new images\n",
    "            gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "            # Train the discriminator\n",
    "            d_loss_real = self.discriminator.train_on_batch([imgs, noise], valid)\n",
    "            d_loss_fake = self.discriminator.train_on_batch([gen_imgs, noise], fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            # Condition on labels\n",
    "            #sampled_labels = np.random.randint(0, 7, batch_size).reshape(-1, 1)\n",
    "\n",
    "            # Train the generator\n",
    "            g_loss = self.combined.train_on_batch(noise, valid)\n",
    "\n",
    "            # Plot the progress\n",
    "            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "\n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % sample_interval == 0:\n",
    "                #self.sample_images(epoch)\n",
    "                self.generator.save_weights(savepath+'weights/generator_weights_'+str(epoch)+'.h5')\n",
    "                self.discriminator.save_weights(savepath+'weights/discriminator_weights_'+str(epoch)+'.h5')\n",
    "\n",
    "    def sample_images(self, epoch):\n",
    "        r, c = 1, 7\n",
    "        noise          = self.test_tiles\n",
    "\n",
    "        gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "        temp = (0.5 * gen_imgs + 0.5)*255\n",
    "        gen_imgs = temp.astype(int)\n",
    "        #print(gen_imgs[0].shape)\n",
    "        combined = np.array([gen_imgs[0], gen_imgs[1], gen_imgs[2], gen_imgs[3], gen_imgs[4], gen_imgs[5], gen_imgs[6]])\n",
    "        #print(combined.shape)\n",
    "        combined = np.hstack(combined.reshape(7,192,192, 3))\n",
    "        imsave(savepath+\"images/\"+str(epoch)+\".png\", combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 1, 200)       72000000    input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, 192, 192, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 200)          0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "sequential_1 (Sequential)       (None, 100)          1520228     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 100)          20100       flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 100)          0           sequential_1[1][0]               \n",
      "                                                                 dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 32)           3232        multiply_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)       (None, 32)           0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1)            33          leaky_re_lu_9[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 73,543,593\n",
      "Trainable params: 73,542,697\n",
      "Non-trainable params: 896\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 1, 200)            72000000  \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "sequential_2 (Sequential)    (None, 192, 192, 3)       22608515  \n",
      "=================================================================\n",
      "Total params: 94,608,515\n",
      "Trainable params: 94,607,619\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "cgan = CGAN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 0.426035, acc.: 50.00%] [G loss: 0.736480]\n",
      "1 [D loss: 0.454116, acc.: 50.00%] [G loss: 0.682353]\n",
      "2 [D loss: 0.439750, acc.: 50.00%] [G loss: 0.646172]\n",
      "3 [D loss: 0.398924, acc.: 50.00%] [G loss: 0.687595]\n",
      "4 [D loss: 0.369145, acc.: 55.00%] [G loss: 0.516780]\n",
      "5 [D loss: 0.396341, acc.: 50.00%] [G loss: 0.557979]\n",
      "6 [D loss: 0.416612, acc.: 50.00%] [G loss: 0.593036]\n",
      "7 [D loss: 0.371029, acc.: 50.00%] [G loss: 0.527829]\n",
      "8 [D loss: 0.339106, acc.: 50.00%] [G loss: 0.557505]\n",
      "9 [D loss: 0.341632, acc.: 50.00%] [G loss: 0.445864]\n",
      "10 [D loss: 0.354653, acc.: 55.00%] [G loss: 0.489229]\n",
      "11 [D loss: 0.328518, acc.: 50.00%] [G loss: 0.376544]\n",
      "12 [D loss: 0.307437, acc.: 55.00%] [G loss: 0.390481]\n",
      "13 [D loss: 0.377286, acc.: 35.00%] [G loss: 0.317583]\n",
      "14 [D loss: 0.366653, acc.: 45.00%] [G loss: 0.395787]\n",
      "15 [D loss: 0.340335, acc.: 50.00%] [G loss: 0.377513]\n",
      "16 [D loss: 0.347185, acc.: 50.00%] [G loss: 0.351297]\n",
      "17 [D loss: 0.268869, acc.: 65.00%] [G loss: 0.343631]\n",
      "18 [D loss: 0.266360, acc.: 60.00%] [G loss: 0.279069]\n",
      "19 [D loss: 0.244698, acc.: 65.00%] [G loss: 0.314177]\n",
      "20 [D loss: 0.216879, acc.: 65.00%] [G loss: 0.318932]\n",
      "21 [D loss: 0.162026, acc.: 80.00%] [G loss: 0.204814]\n",
      "22 [D loss: 0.244008, acc.: 70.00%] [G loss: 0.269500]\n",
      "23 [D loss: 0.238105, acc.: 70.00%] [G loss: 0.315233]\n",
      "24 [D loss: 0.241393, acc.: 65.00%] [G loss: 0.257237]\n",
      "25 [D loss: 0.228723, acc.: 70.00%] [G loss: 0.195152]\n",
      "26 [D loss: 0.109374, acc.: 80.00%] [G loss: 0.310022]\n",
      "27 [D loss: 0.121476, acc.: 95.00%] [G loss: 0.306256]\n",
      "28 [D loss: 0.213832, acc.: 70.00%] [G loss: 0.479351]\n",
      "29 [D loss: 0.277300, acc.: 60.00%] [G loss: 0.381490]\n",
      "30 [D loss: 0.249261, acc.: 75.00%] [G loss: 0.420622]\n",
      "31 [D loss: 0.186501, acc.: 75.00%] [G loss: 0.391669]\n",
      "32 [D loss: 0.151668, acc.: 80.00%] [G loss: 0.266301]\n",
      "33 [D loss: 0.216198, acc.: 70.00%] [G loss: 0.301355]\n",
      "34 [D loss: 0.231804, acc.: 65.00%] [G loss: 0.247159]\n",
      "35 [D loss: 0.255199, acc.: 75.00%] [G loss: 0.250797]\n",
      "36 [D loss: 0.227942, acc.: 65.00%] [G loss: 0.284799]\n",
      "37 [D loss: 0.252778, acc.: 55.00%] [G loss: 0.322865]\n",
      "38 [D loss: 0.174633, acc.: 80.00%] [G loss: 0.336536]\n",
      "39 [D loss: 0.205486, acc.: 60.00%] [G loss: 0.248198]\n",
      "40 [D loss: 0.133054, acc.: 85.00%] [G loss: 0.208832]\n",
      "41 [D loss: 0.142329, acc.: 90.00%] [G loss: 0.276454]\n",
      "42 [D loss: 0.141306, acc.: 85.00%] [G loss: 0.228593]\n",
      "43 [D loss: 0.109162, acc.: 90.00%] [G loss: 0.313009]\n",
      "44 [D loss: 0.159511, acc.: 85.00%] [G loss: 0.257508]\n",
      "45 [D loss: 0.102776, acc.: 85.00%] [G loss: 0.345761]\n",
      "46 [D loss: 0.234497, acc.: 65.00%] [G loss: 0.388825]\n",
      "47 [D loss: 0.148880, acc.: 75.00%] [G loss: 0.420947]\n",
      "48 [D loss: 0.136223, acc.: 85.00%] [G loss: 0.380019]\n",
      "49 [D loss: 0.216413, acc.: 80.00%] [G loss: 0.405477]\n",
      "50 [D loss: 0.203711, acc.: 85.00%] [G loss: 0.346402]\n",
      "51 [D loss: 0.102996, acc.: 90.00%] [G loss: 0.440004]\n",
      "52 [D loss: 0.138586, acc.: 85.00%] [G loss: 0.414003]\n",
      "53 [D loss: 0.240720, acc.: 75.00%] [G loss: 0.420387]\n",
      "54 [D loss: 0.119014, acc.: 85.00%] [G loss: 0.457258]\n",
      "55 [D loss: 0.119036, acc.: 85.00%] [G loss: 0.467757]\n",
      "56 [D loss: 0.157310, acc.: 80.00%] [G loss: 0.426183]\n",
      "57 [D loss: 0.103102, acc.: 85.00%] [G loss: 0.357241]\n",
      "58 [D loss: 0.151240, acc.: 80.00%] [G loss: 0.462354]\n",
      "59 [D loss: 0.213472, acc.: 75.00%] [G loss: 0.367672]\n",
      "60 [D loss: 0.087799, acc.: 90.00%] [G loss: 0.382372]\n",
      "61 [D loss: 0.069241, acc.: 90.00%] [G loss: 0.389757]\n",
      "62 [D loss: 0.101214, acc.: 90.00%] [G loss: 0.347681]\n",
      "63 [D loss: 0.144524, acc.: 80.00%] [G loss: 0.374259]\n",
      "64 [D loss: 0.090846, acc.: 85.00%] [G loss: 0.368560]\n",
      "65 [D loss: 0.079606, acc.: 90.00%] [G loss: 0.247914]\n",
      "66 [D loss: 0.100953, acc.: 100.00%] [G loss: 0.491142]\n",
      "67 [D loss: 0.095625, acc.: 90.00%] [G loss: 0.335804]\n",
      "68 [D loss: 0.127147, acc.: 90.00%] [G loss: 0.341330]\n",
      "69 [D loss: 0.147937, acc.: 80.00%] [G loss: 0.294151]\n",
      "70 [D loss: 0.111634, acc.: 85.00%] [G loss: 0.280973]\n",
      "71 [D loss: 0.061158, acc.: 95.00%] [G loss: 0.465412]\n",
      "72 [D loss: 0.109975, acc.: 85.00%] [G loss: 0.704719]\n",
      "73 [D loss: 0.142873, acc.: 70.00%] [G loss: 0.716719]\n",
      "74 [D loss: 0.292528, acc.: 65.00%] [G loss: 0.663920]\n",
      "75 [D loss: 0.191411, acc.: 75.00%] [G loss: 0.552515]\n",
      "76 [D loss: 0.184600, acc.: 75.00%] [G loss: 0.665080]\n",
      "77 [D loss: 0.280264, acc.: 60.00%] [G loss: 0.638913]\n",
      "78 [D loss: 0.243865, acc.: 55.00%] [G loss: 0.731272]\n",
      "79 [D loss: 0.156855, acc.: 70.00%] [G loss: 0.709131]\n",
      "80 [D loss: 0.215790, acc.: 65.00%] [G loss: 0.650815]\n",
      "81 [D loss: 0.173260, acc.: 70.00%] [G loss: 0.672106]\n",
      "82 [D loss: 0.145304, acc.: 85.00%] [G loss: 0.620497]\n",
      "83 [D loss: 0.204443, acc.: 60.00%] [G loss: 0.659016]\n",
      "84 [D loss: 0.219620, acc.: 70.00%] [G loss: 0.479468]\n",
      "85 [D loss: 0.207937, acc.: 70.00%] [G loss: 0.317610]\n",
      "86 [D loss: 0.154127, acc.: 75.00%] [G loss: 0.255034]\n",
      "87 [D loss: 0.100287, acc.: 90.00%] [G loss: 0.303239]\n",
      "88 [D loss: 0.130728, acc.: 80.00%] [G loss: 0.272927]\n",
      "89 [D loss: 0.074407, acc.: 95.00%] [G loss: 0.369766]\n",
      "90 [D loss: 0.069524, acc.: 95.00%] [G loss: 0.479133]\n",
      "91 [D loss: 0.121353, acc.: 80.00%] [G loss: 0.627456]\n",
      "92 [D loss: 0.152549, acc.: 75.00%] [G loss: 0.457066]\n",
      "93 [D loss: 0.158959, acc.: 75.00%] [G loss: 0.622080]\n",
      "94 [D loss: 0.222551, acc.: 65.00%] [G loss: 0.412354]\n",
      "95 [D loss: 0.148023, acc.: 80.00%] [G loss: 0.645228]\n",
      "96 [D loss: 0.138102, acc.: 85.00%] [G loss: 0.704992]\n",
      "97 [D loss: 0.104697, acc.: 85.00%] [G loss: 0.784779]\n",
      "98 [D loss: 0.054458, acc.: 95.00%] [G loss: 0.757383]\n",
      "99 [D loss: 0.102153, acc.: 90.00%] [G loss: 0.764068]\n",
      "100 [D loss: 0.063597, acc.: 95.00%] [G loss: 0.571202]\n",
      "101 [D loss: 0.080340, acc.: 90.00%] [G loss: 0.610138]\n",
      "102 [D loss: 0.264435, acc.: 65.00%] [G loss: 0.635740]\n",
      "103 [D loss: 0.078047, acc.: 100.00%] [G loss: 0.812323]\n",
      "104 [D loss: 0.222967, acc.: 60.00%] [G loss: 0.658842]\n",
      "105 [D loss: 0.149541, acc.: 80.00%] [G loss: 0.541489]\n",
      "106 [D loss: 0.067235, acc.: 90.00%] [G loss: 0.320754]\n",
      "107 [D loss: 0.105616, acc.: 90.00%] [G loss: 0.292213]\n",
      "108 [D loss: 0.078225, acc.: 90.00%] [G loss: 0.416820]\n",
      "109 [D loss: 0.087046, acc.: 85.00%] [G loss: 0.258291]\n",
      "110 [D loss: 0.048590, acc.: 95.00%] [G loss: 0.438722]\n",
      "111 [D loss: 0.157613, acc.: 75.00%] [G loss: 0.787127]\n",
      "112 [D loss: 0.055504, acc.: 95.00%] [G loss: 0.791775]\n",
      "113 [D loss: 0.029170, acc.: 100.00%] [G loss: 0.814164]\n",
      "114 [D loss: 0.050196, acc.: 95.00%] [G loss: 0.760766]\n",
      "115 [D loss: 0.051628, acc.: 95.00%] [G loss: 0.775893]\n",
      "116 [D loss: 0.041964, acc.: 95.00%] [G loss: 0.760479]\n",
      "117 [D loss: 0.070000, acc.: 95.00%] [G loss: 0.750035]\n",
      "118 [D loss: 0.060310, acc.: 90.00%] [G loss: 0.761060]\n",
      "119 [D loss: 0.045104, acc.: 95.00%] [G loss: 0.807845]\n",
      "120 [D loss: 0.056971, acc.: 100.00%] [G loss: 0.708732]\n",
      "121 [D loss: 0.089203, acc.: 95.00%] [G loss: 0.754764]\n",
      "122 [D loss: 0.067505, acc.: 90.00%] [G loss: 0.710005]\n",
      "123 [D loss: 0.046145, acc.: 100.00%] [G loss: 0.727640]\n",
      "124 [D loss: 0.051541, acc.: 100.00%] [G loss: 0.790812]\n",
      "125 [D loss: 0.096277, acc.: 90.00%] [G loss: 0.789665]\n",
      "126 [D loss: 0.080642, acc.: 80.00%] [G loss: 0.794830]\n",
      "127 [D loss: 0.069243, acc.: 90.00%] [G loss: 0.898087]\n",
      "128 [D loss: 0.074417, acc.: 90.00%] [G loss: 0.888074]\n",
      "129 [D loss: 0.079178, acc.: 90.00%] [G loss: 0.892524]\n",
      "130 [D loss: 0.057983, acc.: 90.00%] [G loss: 0.797618]\n",
      "131 [D loss: 0.080929, acc.: 90.00%] [G loss: 0.907784]\n",
      "132 [D loss: 0.059809, acc.: 95.00%] [G loss: 0.693399]\n",
      "133 [D loss: 0.125722, acc.: 80.00%] [G loss: 0.812332]\n",
      "134 [D loss: 0.278804, acc.: 60.00%] [G loss: 0.734887]\n",
      "135 [D loss: 0.053598, acc.: 95.00%] [G loss: 0.500227]\n",
      "136 [D loss: 0.178098, acc.: 70.00%] [G loss: 0.867740]\n",
      "137 [D loss: 0.244305, acc.: 65.00%] [G loss: 0.814405]\n",
      "138 [D loss: 0.171304, acc.: 75.00%] [G loss: 0.631413]\n",
      "139 [D loss: 0.144515, acc.: 80.00%] [G loss: 0.684390]\n",
      "140 [D loss: 0.157154, acc.: 85.00%] [G loss: 0.666850]\n",
      "141 [D loss: 0.243993, acc.: 70.00%] [G loss: 0.803356]\n",
      "142 [D loss: 0.047777, acc.: 95.00%] [G loss: 0.796610]\n",
      "143 [D loss: 0.062800, acc.: 100.00%] [G loss: 0.808283]\n",
      "144 [D loss: 0.041172, acc.: 95.00%] [G loss: 0.819264]\n",
      "145 [D loss: 0.070908, acc.: 95.00%] [G loss: 0.761121]\n",
      "146 [D loss: 0.098724, acc.: 85.00%] [G loss: 0.778816]\n",
      "147 [D loss: 0.139333, acc.: 70.00%] [G loss: 0.910258]\n",
      "148 [D loss: 0.134978, acc.: 75.00%] [G loss: 0.833601]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149 [D loss: 0.119631, acc.: 80.00%] [G loss: 0.816122]\n",
      "150 [D loss: 0.120417, acc.: 90.00%] [G loss: 0.805658]\n",
      "151 [D loss: 0.155687, acc.: 75.00%] [G loss: 0.567500]\n",
      "152 [D loss: 0.164161, acc.: 75.00%] [G loss: 0.757974]\n",
      "153 [D loss: 0.183110, acc.: 80.00%] [G loss: 0.976950]\n",
      "154 [D loss: 0.050877, acc.: 90.00%] [G loss: 0.909090]\n",
      "155 [D loss: 0.091996, acc.: 90.00%] [G loss: 0.843868]\n",
      "156 [D loss: 0.118960, acc.: 85.00%] [G loss: 0.883803]\n",
      "157 [D loss: 0.134374, acc.: 80.00%] [G loss: 0.849309]\n",
      "158 [D loss: 0.085463, acc.: 90.00%] [G loss: 0.787465]\n",
      "159 [D loss: 0.063984, acc.: 95.00%] [G loss: 0.783770]\n",
      "160 [D loss: 0.054994, acc.: 100.00%] [G loss: 0.764833]\n",
      "161 [D loss: 0.120183, acc.: 90.00%] [G loss: 0.773226]\n",
      "162 [D loss: 0.083108, acc.: 90.00%] [G loss: 0.731245]\n",
      "163 [D loss: 0.053622, acc.: 100.00%] [G loss: 0.789911]\n",
      "164 [D loss: 0.093464, acc.: 95.00%] [G loss: 0.834433]\n",
      "165 [D loss: 0.186985, acc.: 75.00%] [G loss: 0.744236]\n",
      "166 [D loss: 0.087084, acc.: 90.00%] [G loss: 0.806349]\n",
      "167 [D loss: 0.042905, acc.: 95.00%] [G loss: 0.832251]\n",
      "168 [D loss: 0.056968, acc.: 95.00%] [G loss: 0.804649]\n",
      "169 [D loss: 0.052824, acc.: 95.00%] [G loss: 0.803179]\n",
      "170 [D loss: 0.085806, acc.: 85.00%] [G loss: 0.799915]\n",
      "171 [D loss: 0.042047, acc.: 100.00%] [G loss: 0.705823]\n",
      "172 [D loss: 0.111865, acc.: 80.00%] [G loss: 0.834531]\n",
      "173 [D loss: 0.160843, acc.: 70.00%] [G loss: 0.855236]\n",
      "174 [D loss: 0.028487, acc.: 100.00%] [G loss: 0.890190]\n",
      "175 [D loss: 0.175445, acc.: 85.00%] [G loss: 0.859449]\n",
      "176 [D loss: 0.154472, acc.: 85.00%] [G loss: 0.848148]\n",
      "177 [D loss: 0.116028, acc.: 85.00%] [G loss: 0.855710]\n",
      "178 [D loss: 0.235739, acc.: 65.00%] [G loss: 0.706691]\n",
      "179 [D loss: 0.111823, acc.: 85.00%] [G loss: 0.742268]\n",
      "180 [D loss: 0.090066, acc.: 95.00%] [G loss: 0.678122]\n",
      "181 [D loss: 0.084815, acc.: 95.00%] [G loss: 0.581267]\n",
      "182 [D loss: 0.093844, acc.: 90.00%] [G loss: 0.824493]\n",
      "183 [D loss: 0.056499, acc.: 90.00%] [G loss: 0.563171]\n",
      "184 [D loss: 0.025980, acc.: 100.00%] [G loss: 0.806243]\n",
      "185 [D loss: 0.022326, acc.: 100.00%] [G loss: 0.902469]\n",
      "186 [D loss: 0.060654, acc.: 95.00%] [G loss: 0.664291]\n",
      "187 [D loss: 0.061497, acc.: 90.00%] [G loss: 0.738772]\n",
      "188 [D loss: 0.161630, acc.: 90.00%] [G loss: 0.824305]\n",
      "189 [D loss: 0.021784, acc.: 100.00%] [G loss: 0.854843]\n",
      "190 [D loss: 0.040248, acc.: 95.00%] [G loss: 0.863040]\n"
     ]
    }
   ],
   "source": [
    "cgan.train(1000000, batch_size=10, sample_interval=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
